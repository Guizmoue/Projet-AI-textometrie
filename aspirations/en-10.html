



<!DOCTYPE html>
<html lang="en">
<head>
    <script>
        dataLayer = [];
    </script>
    <script>
        dataLayer.push({ 'app_id': '1'});
    </script>
    <!-- Google Tag Manager -->
    <script>
        (function (w, d, s, l, i) {
            w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' });
            var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true;
            j.src = 'https://tag-manager.frontiersin.org/gtm.js?id=' + i + dl + '&gtm_auth=u-n-7MEC83-lilAhi0fqhQ&gtm_preview=env-1&gtm_cookies_win=x';
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-5SXX286');
    </script>
    <!-- End Google Tag Manager -->
    <script src="https://unpkg.com/vue@2.6.14/dist/vue.min.js"></script>


        <link data-n-head="ssr" rel="canonical" href="https://www.frontiersin.org/articles/10.3389/frai.2021.622364">


    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" /><script type="text/javascript">window.NREUM||(NREUM={});NREUM.info = {"beacon":"bam.nr-data.net","errorBeacon":"bam.nr-data.net","licenseKey":"598a124f17","applicationID":"3007887","transactionName":"MQcDMkECCkNSW0YMWghNLDBwTCVCR1FRCVAlDQ8SQQwIXFZKHSNACg41A0sXJkl3d3s=","queueTime":0,"applicationTime":305,"agent":"","atts":""}</script><script type="text/javascript">(window.NREUM||(NREUM={})).init={ajax:{deny_list:["bam.nr-data.net"]}};(window.NREUM||(NREUM={})).loader_config={xpid:"VgUHUl5WGwAAVFZaDwY=",licenseKey:"598a124f17",applicationID:"3007887"};;/*! For license information please see nr-loader-full-1.248.0.min.js.LICENSE.txt */
(()=>{var e,t,r={234:(e,t,r)=>{"use strict";r.d(t,{P_:()=>g,Mt:()=>v,C5:()=>s,DL:()=>A,OP:()=>S,lF:()=>R,Yu:()=>w,Dg:()=>m,CX:()=>c,GE:()=>x,sU:()=>j});var n=r(8632),i=r(9567);const a={beacon:n.ce.beacon,errorBeacon:n.ce.errorBeacon,licenseKey:void 0,applicationID:void 0,sa:void 0,queueTime:void 0,applicationTime:void 0,ttGuid:void 0,user:void 0,account:void 0,product:void 0,extra:void 0,jsAttributes:{},userAttributes:void 0,atts:void 0,transactionName:void 0,tNamePlain:void 0},o={};function s(e){if(!e)throw new Error("All info objects require an agent identifier!");if(!o[e])throw new Error("Info for ".concat(e," was never set"));return o[e]}function c(e,t){if(!e)throw new Error("All info objects require an agent identifier!");o[e]=(0,i.D)(t,a);const r=(0,n.ek)(e);r&&(r.info=o[e])}const d=e=>{if(!e||"string"!=typeof e)return!1;try{document.createDocumentFragment().querySelector(e)}catch{return!1}return!0};var u=r(7056),l=r(50);const f=()=>{const e={mask_selector:"*",block_selector:"[data-nr-block]",mask_input_options:{color:!1,date:!1,"datetime-local":!1,email:!1,month:!1,number:!1,range:!1,search:!1,tel:!1,text:!1,time:!1,url:!1,week:!1,textarea:!1,select:!1,password:!0}};return{feature_flags:[],proxy:{assets:void 0,beacon:void 0},privacy:{cookies_enabled:!0},ajax:{deny_list:void 0,block_internal:!0,enabled:!0,harvestTimeSeconds:10,autoStart:!0},distributed_tracing:{enabled:void 0,exclude_newrelic_header:void 0,cors_use_newrelic_header:void 0,cors_use_tracecontext_headers:void 0,allowed_origins:void 0},session:{domain:void 0,expiresMs:u.oD,inactiveMs:u.Hb},ssl:void 0,obfuscate:void 0,jserrors:{enabled:!0,harvestTimeSeconds:10,autoStart:!0},metrics:{enabled:!0,autoStart:!0},page_action:{enabled:!0,harvestTimeSeconds:30,autoStart:!0},page_view_event:{enabled:!0,autoStart:!0},page_view_timing:{enabled:!0,harvestTimeSeconds:30,long_task:!1,autoStart:!0},session_trace:{enabled:!0,harvestTimeSeconds:10,autoStart:!0},harvest:{tooManyRequestsDelay:60},session_replay:{autoStart:!0,enabled:!1,harvestTimeSeconds:60,sampling_rate:50,error_sampling_rate:50,collect_fonts:!1,inline_images:!1,inline_stylesheet:!0,mask_all_inputs:!0,get mask_text_selector(){return e.mask_selector},set mask_text_selector(t){d(t)?e.mask_selector=t+",[data-nr-mask]":null===t?e.mask_selector=t:(0,l.Z)("An invalid session_replay.mask_selector was provided and will not be used",t)},get block_class(){return"nr-block"},get ignore_class(){return"nr-ignore"},get mask_text_class(){return"nr-mask"},get block_selector(){return e.block_selector},set block_selector(t){d(t)?e.block_selector+=",".concat(t):""!==t&&(0,l.Z)("An invalid session_replay.block_selector was provided and will not be used",t)},get mask_input_options(){return e.mask_input_options},set mask_input_options(t){t&&"object"==typeof t?e.mask_input_options={...t,password:!0}:(0,l.Z)("An invalid session_replay.mask_input_option was provided and will not be used",t)}},spa:{enabled:!0,harvestTimeSeconds:10,autoStart:!0}}},h={},p="All configuration objects require an agent identifier!";function g(e){if(!e)throw new Error(p);if(!h[e])throw new Error("Configuration for ".concat(e," was never set"));return h[e]}function m(e,t){if(!e)throw new Error(p);h[e]=(0,i.D)(t,f());const r=(0,n.ek)(e);r&&(r.init=h[e])}function v(e,t){if(!e)throw new Error(p);var r=g(e);if(r){for(var n=t.split("."),i=0;i<n.length-1;i++)if("object"!=typeof(r=r[n[i]]))return;r=r[n[n.length-1]]}return r}const b={accountID:void 0,trustKey:void 0,agentID:void 0,licenseKey:void 0,applicationID:void 0,xpid:void 0},y={};function A(e){if(!e)throw new Error("All loader-config objects require an agent identifier!");if(!y[e])throw new Error("LoaderConfig for ".concat(e," was never set"));return y[e]}function x(e,t){if(!e)throw new Error("All loader-config objects require an agent identifier!");y[e]=(0,i.D)(t,b);const r=(0,n.ek)(e);r&&(r.loader_config=y[e])}const w=(0,n.mF)().o;var _=r(385),E=r(6818);const T={buildEnv:E.Re,customTransaction:void 0,disabled:!1,distMethod:E.gF,isolatedBacklog:!1,loaderType:void 0,maxBytes:3e4,offset:Math.floor(_._A?.performance?.timeOrigin||_._A?.performance?.timing?.navigationStart||Date.now()),onerror:void 0,origin:""+_._A.location,ptid:void 0,releaseIds:{},session:void 0,xhrWrappable:"function"==typeof _._A.XMLHttpRequest?.prototype?.addEventListener,version:E.q4,denyList:void 0},D={};function S(e){if(!e)throw new Error("All runtime objects require an agent identifier!");if(!D[e])throw new Error("Runtime for ".concat(e," was never set"));return D[e]}function j(e,t){if(!e)throw new Error("All runtime objects require an agent identifier!");D[e]=(0,i.D)(t,T);const r=(0,n.ek)(e);r&&(r.runtime=D[e])}function R(e){return function(e){try{const t=s(e);return!!t.licenseKey&&!!t.errorBeacon&&!!t.applicationID}catch(e){return!1}}(e)}},9567:(e,t,r)=>{"use strict";r.d(t,{D:()=>i});var n=r(50);function i(e,t){try{if(!e||"object"!=typeof e)return(0,n.Z)("Setting a Configurable requires an object as input");if(!t||"object"!=typeof t)return(0,n.Z)("Setting a Configurable requires a model to set its initial properties");const r=Object.create(Object.getPrototypeOf(t),Object.getOwnPropertyDescriptors(t)),a=0===Object.keys(r).length?e:r;for(let o in a)if(void 0!==e[o])try{Array.isArray(e[o])&&Array.isArray(t[o])?r[o]=Array.from(new Set([...e[o],...t[o]])):"object"==typeof e[o]&&"object"==typeof t[o]?r[o]=i(e[o],t[o]):r[o]=e[o]}catch(e){(0,n.Z)("An error occurred while setting a property of a Configurable",e)}return r}catch(e){(0,n.Z)("An error occured while setting a Configurable",e)}}},6818:(e,t,r)=>{"use strict";r.d(t,{Re:()=>i,gF:()=>a,lF:()=>o,q4:()=>n});const n="1.248.0",i="PROD",a="CDN",o="2.0.0-alpha.11"},385:(e,t,r)=>{"use strict";r.d(t,{FN:()=>s,IF:()=>u,Nk:()=>f,Tt:()=>c,_A:()=>a,cv:()=>h,iS:()=>o,il:()=>n,ux:()=>d,v6:()=>i,w1:()=>l});const n="undefined"!=typeof window&&!!window.document,i="undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self.navigator instanceof WorkerNavigator||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis.navigator instanceof WorkerNavigator),a=n?window:"undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis),o=(a?.document?.readyState,Boolean("hidden"===a?.document?.visibilityState)),s=""+a?.location,c=/iPad|iPhone|iPod/.test(a.navigator?.userAgent),d=c&&"undefined"==typeof SharedWorker,u=(()=>{const e=a.navigator?.userAgent?.match(/Firefox[/\s](\d+\.\d+)/);return Array.isArray(e)&&e.length>=2?+e[1]:0})(),l=Boolean(n&&window.document.documentMode),f=!!a.navigator?.sendBeacon,h=Math.floor(a?.performance?.timeOrigin||a?.performance?.timing?.navigationStart||Date.now())},1117:(e,t,r)=>{"use strict";r.d(t,{w:()=>a});var n=r(50);const i={agentIdentifier:"",ee:void 0};class a{constructor(e){try{if("object"!=typeof e)return(0,n.Z)("shared context requires an object as input");this.sharedContext={},Object.assign(this.sharedContext,i),Object.entries(e).forEach((e=>{let[t,r]=e;Object.keys(i).includes(t)&&(this.sharedContext[t]=r)}))}catch(e){(0,n.Z)("An error occured while setting SharedContext",e)}}}},8e3:(e,t,r)=>{"use strict";r.d(t,{L:()=>u,R:()=>c});var n=r(8325),i=r(1284),a=r(4322),o=r(3325);const s={};function c(e,t){const r={staged:!1,priority:o.p[t]||0};d(e),s[e].get(t)||s[e].set(t,r)}function d(e){e&&(s[e]||(s[e]=new Map))}function u(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"",t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"feature";if(d(e),!e||!s[e].get(t))return o(t);s[e].get(t).staged=!0;const r=[...s[e]];function o(t){const r=e?n.ee.get(e):n.ee,o=a.X.handlers;if(r.backlog&&o){var s=r.backlog[t],c=o[t];if(c){for(var d=0;s&&d<s.length;++d)l(s[d],c);(0,i.D)(c,(function(e,t){(0,i.D)(t,(function(t,r){r[0].on(e,r[1])}))}))}delete o[t],r.backlog[t]=null,r.emit("drain-"+t,[])}}r.every((e=>{let[t,r]=e;return r.staged}))&&(r.sort(((e,t)=>e[1].priority-t[1].priority)),r.forEach((t=>{let[r]=t;s[e].delete(r),o(r)})))}function l(e,t){var r=e[1];(0,i.D)(t[r],(function(t,r){var n=e[0];if(r[0]===n){var i=r[1],a=e[3],o=e[2];i.apply(a,o)}}))}},8325:(e,t,r)=>{"use strict";r.d(t,{A:()=>c,ee:()=>d});var n=r(8632),i=r(2210),a=r(234);class o{constructor(e){this.contextId=e}}var s=r(3117);const c="nr@context:".concat(s.a),d=function e(t,r){var n={},s={},u={},f=!1;try{f=16===r.length&&(0,a.OP)(r).isolatedBacklog}catch(e){}var h={on:g,addEventListener:g,removeEventListener:function(e,t){var r=n[e];if(!r)return;for(var i=0;i<r.length;i++)r[i]===t&&r.splice(i,1)},emit:function(e,r,n,i,a){!1!==a&&(a=!0);if(d.aborted&&!i)return;t&&a&&t.emit(e,r,n);for(var o=p(n),c=m(e),u=c.length,l=0;l<u;l++)c[l].apply(o,r);var f=b()[s[e]];f&&f.push([h,e,r,o]);return o},get:v,listeners:m,context:p,buffer:function(e,t){const r=b();if(t=t||"feature",h.aborted)return;Object.entries(e||{}).forEach((e=>{let[n,i]=e;s[i]=t,t in r||(r[t]=[])}))},abort:l,aborted:!1,isBuffering:function(e){return!!b()[s[e]]},debugId:r,backlog:f?{}:t&&"object"==typeof t.backlog?t.backlog:{}};return h;function p(e){return e&&e instanceof o?e:e?(0,i.X)(e,c,(()=>new o(c))):new o(c)}function g(e,t){n[e]=m(e).concat(t)}function m(e){return n[e]||[]}function v(t){return u[t]=u[t]||e(h,t)}function b(){return h.backlog}}(void 0,"globalEE"),u=(0,n.fP)();function l(){d.aborted=!0,d.backlog={}}u.ee||(u.ee=d)},5546:(e,t,r)=>{"use strict";r.d(t,{E:()=>n,p:()=>i});var n=r(8325).ee.get("handle");function i(e,t,r,i,a){a?(a.buffer([e],i),a.emit(e,t,r)):(n.buffer([e],i),n.emit(e,t,r))}},4322:(e,t,r)=>{"use strict";r.d(t,{X:()=>a});var n=r(5546);a.on=o;var i=a.handlers={};function a(e,t,r,a){o(a||n.E,i,e,t,r)}function o(e,t,r,i,a){a||(a="feature"),e||(e=n.E);var o=t[a]=t[a]||{};(o[r]=o[r]||[]).push([e,i])}},3239:(e,t,r)=>{"use strict";r.d(t,{bP:()=>s,iz:()=>c,m$:()=>o});var n=r(385);let i=!1,a=!1;try{const e={get passive(){return i=!0,!1},get signal(){return a=!0,!1}};n._A.addEventListener("test",null,e),n._A.removeEventListener("test",null,e)}catch(e){}function o(e,t){return i||a?{capture:!!e,passive:i,signal:t}:!!e}function s(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3?arguments[3]:void 0;window.addEventListener(e,t,o(r,n))}function c(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3?arguments[3]:void 0;document.addEventListener(e,t,o(r,n))}},3117:(e,t,r)=>{"use strict";r.d(t,{a:()=>n});const n=(0,r(4402).Rl)()},4402:(e,t,r)=>{"use strict";r.d(t,{Ht:()=>d,M:()=>c,Rl:()=>o,ky:()=>s});var n=r(385);const i="xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";function a(e,t){return e?15&e[t]:16*Math.random()|0}function o(){const e=n._A?.crypto||n._A?.msCrypto;let t,r=0;return e&&e.getRandomValues&&(t=e.getRandomValues(new Uint8Array(31))),i.split("").map((e=>"x"===e?a(t,++r).toString(16):"y"===e?(3&a()|8).toString(16):e)).join("")}function s(e){const t=n._A?.crypto||n._A?.msCrypto;let r,i=0;t&&t.getRandomValues&&(r=t.getRandomValues(new Uint8Array(31)));const o=[];for(var s=0;s<e;s++)o.push(a(r,++i).toString(16));return o.join("")}function c(){return s(16)}function d(){return s(32)}},7056:(e,t,r)=>{"use strict";r.d(t,{Bq:()=>n,Hb:()=>a,oD:()=>i});const n="NRBA",i=144e5,a=18e5},7894:(e,t,r)=>{"use strict";function n(){return Math.round(performance.now())}r.d(t,{z:()=>n})},7243:(e,t,r)=>{"use strict";r.d(t,{e:()=>i});var n=r(385);function i(e){if(0===(e||"").indexOf("data:"))return{protocol:"data"};try{const t=new URL(e,location.href),r={port:t.port,hostname:t.hostname,pathname:t.pathname,search:t.search,protocol:t.protocol.slice(0,t.protocol.indexOf(":")),sameOrigin:t.protocol===n._A?.location?.protocol&&t.host===n._A?.location?.host};return r.port&&""!==r.port||("http:"===t.protocol&&(r.port="80"),"https:"===t.protocol&&(r.port="443")),r.pathname&&""!==r.pathname?r.pathname.startsWith("/")||(r.pathname="/".concat(r.pathname)):r.pathname="/",r}catch(e){return{}}}},50:(e,t,r)=>{"use strict";function n(e,t){"function"==typeof console.warn&&(console.warn("New Relic: ".concat(e)),t&&console.warn(t))}r.d(t,{Z:()=>n})},2825:(e,t,r)=>{"use strict";r.d(t,{N:()=>u,T:()=>l});var n=r(8325),i=r(5546),a=r(3325),o=r(385);const s="newrelic";const c={stn:[a.D.sessionTrace],err:[a.D.jserrors,a.D.metrics],ins:[a.D.pageAction],spa:[a.D.spa],sr:[a.D.sessionReplay,a.D.sessionTrace]},d=new Set;function u(e,t){const r=n.ee.get(t);e&&"object"==typeof e&&(d.has(t)||(Object.entries(e).forEach((e=>{let[t,n]=e;c[t]?c[t].forEach((e=>{n?(0,i.p)("feat-"+t,[],void 0,e,r):(0,i.p)("block-"+t,[],void 0,e,r),(0,i.p)("rumresp-"+t,[Boolean(n)],void 0,e,r)})):n&&(0,i.p)("feat-"+t,[],void 0,void 0,r),l[t]=Boolean(n)})),Object.keys(c).forEach((e=>{void 0===l[e]&&(c[e]?.forEach((t=>(0,i.p)("rumresp-"+e,[!1],void 0,t,r))),l[e]=!1)})),d.add(t),function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};try{o._A.dispatchEvent(new CustomEvent(s,{detail:e}))}catch(e){}}({loaded:!0})))}const l={}},2210:(e,t,r)=>{"use strict";r.d(t,{X:()=>i});var n=Object.prototype.hasOwnProperty;function i(e,t,r){if(n.call(e,t))return e[t];var i=r();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,t,{value:i,writable:!0,enumerable:!1}),i}catch(e){}return e[t]=i,i}},1284:(e,t,r)=>{"use strict";r.d(t,{D:()=>n});const n=(e,t)=>Object.entries(e||{}).map((e=>{let[r,n]=e;return t(r,n)}))},4351:(e,t,r)=>{"use strict";r.d(t,{P:()=>a});var n=r(8325);const i=()=>{const e=new WeakSet;return(t,r)=>{if("object"==typeof r&&null!==r){if(e.has(r))return;e.add(r)}return r}};function a(e){try{return JSON.stringify(e,i())}catch(e){try{n.ee.emit("internal-error",[e])}catch(e){}}}},3960:(e,t,r)=>{"use strict";r.d(t,{KB:()=>o,b2:()=>a});var n=r(3239);function i(){return"undefined"==typeof document||"complete"===document.readyState}function a(e,t){if(i())return e();(0,n.bP)("load",e,t)}function o(e){if(i())return e();(0,n.iz)("DOMContentLoaded",e)}},8632:(e,t,r)=>{"use strict";r.d(t,{EZ:()=>u,ce:()=>a,ek:()=>d,fP:()=>o,gG:()=>l,h5:()=>c,mF:()=>s});var n=r(7894),i=r(385);const a={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net"};function o(){return i._A.NREUM||(i._A.NREUM={}),void 0===i._A.newrelic&&(i._A.newrelic=i._A.NREUM),i._A.NREUM}function s(){let e=o();return e.o||(e.o={ST:i._A.setTimeout,SI:i._A.setImmediate,CT:i._A.clearTimeout,XHR:i._A.XMLHttpRequest,REQ:i._A.Request,EV:i._A.Event,PR:i._A.Promise,MO:i._A.MutationObserver,FETCH:i._A.fetch}),e}function c(e,t){let r=o();r.initializedAgents??={},t.initializedAt={ms:(0,n.z)(),date:new Date},r.initializedAgents[e]=t}function d(e){let t=o();return t.initializedAgents?.[e]}function u(e,t){o()[e]=t}function l(){return function(){let e=o();const t=e.info||{};e.info={beacon:a.beacon,errorBeacon:a.errorBeacon,...t}}(),function(){let e=o();const t=e.init||{};e.init={...t}}(),s(),function(){let e=o();const t=e.loader_config||{};e.loader_config={...t}}(),o()}},7956:(e,t,r)=>{"use strict";r.d(t,{N:()=>i});var n=r(3239);function i(e){let t=arguments.length>1&&void 0!==arguments[1]&&arguments[1],r=arguments.length>2?arguments[2]:void 0,i=arguments.length>3?arguments[3]:void 0;(0,n.iz)("visibilitychange",(function(){if(t)return void("hidden"===document.visibilityState&&e());e(document.visibilityState)}),r,i)}},1214:(e,t,r)=>{"use strict";r.d(t,{em:()=>b,u5:()=>S,QU:()=>C,Kf:()=>N});var n=r(8325),i=r(3117);const a="nr@original:".concat(i.a);var o=Object.prototype.hasOwnProperty,s=!1;function c(e,t){return e||(e=n.ee),r.inPlace=function(e,t,n,i,a){n||(n="");const o="-"===n.charAt(0);for(let s=0;s<t.length;s++){const c=t[s],d=e[c];u(d)||(e[c]=r(d,o?c+n:n,i,c,a))}},r.flag=a,r;function r(t,r,n,s,c){return u(t)?t:(r||(r=""),nrWrapper[a]=t,function(e,t,r){if(Object.defineProperty&&Object.keys)try{return Object.keys(e).forEach((function(r){Object.defineProperty(t,r,{get:function(){return e[r]},set:function(t){return e[r]=t,t}})})),t}catch(e){d([e],r)}for(var n in e)o.call(e,n)&&(t[n]=e[n])}(t,nrWrapper,e),nrWrapper);function nrWrapper(){var a,o,u,l;try{o=this,a=[...arguments],u="function"==typeof n?n(a,o):n||{}}catch(t){d([t,"",[a,o,s],u],e)}i(r+"start",[a,o,s],u,c);try{return l=t.apply(o,a)}catch(e){throw i(r+"err",[a,o,e],u,c),e}finally{i(r+"end",[a,o,l],u,c)}}}function i(r,n,i,a){if(!s||t){var o=s;s=!0;try{e.emit(r,n,i,t,a)}catch(t){d([t,r,n,i],e)}s=o}}}function d(e,t){t||(t=n.ee);try{t.emit("internal-error",e)}catch(e){}}function u(e){return!(e&&"function"==typeof e&&e.apply&&!e[a])}var l=r(2210),f=r(385);const h={},p=f._A.XMLHttpRequest,g="addEventListener",m="removeEventListener",v="nr@wrapped:".concat(n.A);function b(e){var t=function(e){return(e||n.ee).get("events")}(e);if(h[t.debugId]++)return t;h[t.debugId]=1;var r=c(t,!0);function i(e){r.inPlace(e,[g,m],"-",a)}function a(e,t){return e[1]}return"getPrototypeOf"in Object&&(f.il&&y(document,i),y(f._A,i),y(p.prototype,i)),t.on(g+"-start",(function(e,t){var n=e[1];if(null!==n&&("function"==typeof n||"object"==typeof n)){var i=(0,l.X)(n,v,(function(){var e={object:function(){if("function"!=typeof n.handleEvent)return;return n.handleEvent.apply(n,arguments)},function:n}[typeof n];return e?r(e,"fn-",null,e.name||"anonymous"):n}));this.wrapped=e[1]=i}})),t.on(m+"-start",(function(e){e[1]=this.wrapped||e[1]})),t}function y(e,t){let r=e;for(;"object"==typeof r&&!Object.prototype.hasOwnProperty.call(r,g);)r=Object.getPrototypeOf(r);for(var n=arguments.length,i=new Array(n>2?n-2:0),a=2;a<n;a++)i[a-2]=arguments[a];r&&t(r,...i)}var A="fetch-",x=A+"body-",w=["arrayBuffer","blob","json","text","formData"],_=f._A.Request,E=f._A.Response,T="prototype";const D={};function S(e){const t=function(e){return(e||n.ee).get("fetch")}(e);if(!(_&&E&&f._A.fetch))return t;if(D[t.debugId]++)return t;function r(e,r,i){var a=e[r];"function"==typeof a&&(e[r]=function(){var e,r=[...arguments],o={};t.emit(i+"before-start",[r],o),o[n.A]&&o[n.A].dt&&(e=o[n.A].dt);var s=a.apply(this,r);return t.emit(i+"start",[r,e],s),s.then((function(e){return t.emit(i+"end",[null,e],s),e}),(function(e){throw t.emit(i+"end",[e],s),e}))})}return D[t.debugId]=1,w.forEach((e=>{r(_[T],e,x),r(E[T],e,x)})),r(f._A,"fetch",A),t.on(A+"end",(function(e,r){var n=this;if(r){var i=r.headers.get("content-length");null!==i&&(n.rxSize=i),t.emit(A+"done",[null,r],n)}else t.emit(A+"done",[e],n)})),t}const j={},R=["pushState","replaceState"];function C(e){const t=function(e){return(e||n.ee).get("history")}(e);return!f.il||j[t.debugId]++||(j[t.debugId]=1,c(t).inPlace(window.history,R,"-")),t}var P=r(3239);var O=r(50);const k={},I=["open","send"];function N(e){var t=e||n.ee;const r=function(e){return(e||n.ee).get("xhr")}(t);if(k[r.debugId]++)return r;k[r.debugId]=1,b(t);var i=c(r),a=f._A.XMLHttpRequest,o=f._A.MutationObserver,s=f._A.Promise,d=f._A.setInterval,u="readystatechange",l=["onload","onerror","onabort","onloadstart","onloadend","onprogress","ontimeout"],h=[],p=f._A.XMLHttpRequest=function(e){const t=new a(e),n=r.context(t);try{r.emit("new-xhr",[t],n),t.addEventListener(u,(o=n,function(){var e=this;e.readyState>3&&!o.resolved&&(o.resolved=!0,r.emit("xhr-resolved",[],e)),i.inPlace(e,l,"fn-",x)}),(0,P.m$)(!1))}catch(e){(0,O.Z)("An error occurred while intercepting XHR",e);try{r.emit("internal-error",[e])}catch(e){}}var o;return t};function g(e,t){i.inPlace(t,["onreadystatechange"],"fn-",x)}if(function(e,t){for(var r in e)t[r]=e[r]}(a,p),p.prototype=a.prototype,i.inPlace(p.prototype,I,"-xhr-",x),r.on("send-xhr-start",(function(e,t){g(e,t),function(e){h.push(e),o&&(m?m.then(A):d?d(A):(v=-v,y.data=v))}(t)})),r.on("open-xhr-start",g),o){var m=s&&s.resolve();if(!d&&!s){var v=1,y=document.createTextNode(v);new o(A).observe(y,{characterData:!0})}}else t.on("fn-end",(function(e){e[0]&&e[0].type===u||A()}));function A(){for(var e=0;e<h.length;e++)g(0,h[e]);h.length&&(h=[])}function x(e,t){return t}return r}},7825:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(3325).D.ajax},6660:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(3325).D.jserrors},3081:(e,t,r)=>{"use strict";r.d(t,{gF:()=>a,mY:()=>i,t9:()=>n,vz:()=>s,xS:()=>o});const n=r(3325).D.metrics,i="sm",a="cm",o="storeSupportabilityMetrics",s="storeEventMetrics"},4649:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(3325).D.pageAction},7633:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(3325).D.pageViewEvent},9251:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(3325).D.pageViewTiming},7144:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(3325).D.sessionReplay},3614:(e,t,r)=>{"use strict";r.d(t,{BST_RESOURCE:()=>i,END:()=>s,FEATURE_NAME:()=>n,FN_END:()=>d,FN_START:()=>c,PUSH_STATE:()=>u,RESOURCE:()=>a,START:()=>o});const n=r(3325).D.sessionTrace,i="bstResource",a="resource",o="-start",s="-end",c="fn"+o,d="fn"+s,u="pushState"},5938:(e,t,r)=>{"use strict";r.d(t,{W:()=>i});var n=r(8325);class i{constructor(e,t,r){this.agentIdentifier=e,this.aggregator=t,this.ee=n.ee.get(e),this.featureName=r,this.blocked=!1}}},7530:(e,t,r)=>{"use strict";r.d(t,{j:()=>b});var n=r(3325),i=r(234),a=r(5546),o=r(8325),s=r(7894),c=r(8e3),d=r(3960),u=r(385),l=r(50),f=r(3081),h=r(8632);function p(){const e=(0,h.gG)();["setErrorHandler","finished","addToTrace","addRelease","addPageAction","setCurrentRouteName","setPageViewName","setCustomAttribute","interaction","noticeError","setUserId","setApplicationVersion","start","recordReplay","pauseReplay"].forEach((t=>{e[t]=function(){for(var r=arguments.length,n=new Array(r),i=0;i<r;i++)n[i]=arguments[i];return function(t){for(var r=arguments.length,n=new Array(r>1?r-1:0),i=1;i<r;i++)n[i-1]=arguments[i];let a=[];return Object.values(e.initializedAgents).forEach((e=>{e.exposed&&e.api[t]&&a.push(e.api[t](...n))})),a.length>1?a:a[0]}(t,...n)}}))}var g=r(2825);const m=e=>{const t=e.startsWith("http");e+="/",r.p=t?e:"https://"+e};let v=!1;function b(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},b=arguments.length>2?arguments[2]:void 0,y=arguments.length>3?arguments[3]:void 0,{init:A,info:x,loader_config:w,runtime:_={loaderType:b},exposed:E=!0}=t;const T=(0,h.gG)();x||(A=T.init,x=T.info,w=T.loader_config),(0,i.Dg)(e.agentIdentifier,A||{}),(0,i.GE)(e.agentIdentifier,w||{}),x.jsAttributes??={},u.v6&&(x.jsAttributes.isWorker=!0),(0,i.CX)(e.agentIdentifier,x);const D=(0,i.P_)(e.agentIdentifier),S=[x.beacon,x.errorBeacon];v||(D.proxy.assets&&(m(D.proxy.assets),S.push(D.proxy.assets)),D.proxy.beacon&&S.push(D.proxy.beacon),p(),(0,h.EZ)("activatedFeatures",g.T)),_.denyList=[...D.ajax.deny_list||[],...D.ajax.block_internal?S:[]],(0,i.sU)(e.agentIdentifier,_),void 0===e.api&&(e.api=function(e,t){t||(0,c.R)(e,"api");const h={};var p=o.ee.get(e),g=p.get("tracer"),m="api-",v=m+"ixn-";function b(t,r,n,a){const o=(0,i.C5)(e);return null===r?delete o.jsAttributes[t]:(0,i.CX)(e,{...o,jsAttributes:{...o.jsAttributes,[t]:r}}),x(m,n,!0,a||null===r?"session":void 0)(t,r)}function y(){}["setErrorHandler","finished","addToTrace","addRelease"].forEach((e=>{h[e]=x(m,e,!0,"api")})),h.addPageAction=x(m,"addPageAction",!0,n.D.pageAction),h.setCurrentRouteName=x(m,"routeName",!0,n.D.spa),h.setPageViewName=function(t,r){if("string"==typeof t)return"/"!==t.charAt(0)&&(t="/"+t),(0,i.OP)(e).customTransaction=(r||"http://custom.transaction")+t,x(m,"setPageViewName",!0)()},h.setCustomAttribute=function(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2];if("string"==typeof e){if(["string","number","boolean"].includes(typeof t)||null===t)return b(e,t,"setCustomAttribute",r);(0,l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string, number or boolean type, but a type of <".concat(typeof t,"> was provided."))}else(0,l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e,"> was provided."))},h.setUserId=function(e){if("string"==typeof e||null===e)return b("enduser.id",e,"setUserId",!0);(0,l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e,"> was provided."))},h.setApplicationVersion=function(e){if("string"==typeof e||null===e)return b("application.version",e,"setApplicationVersion",!1);(0,l.Z)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e,">."))},h.start=e=>{try{const t=e?"defined":"undefined";(0,a.p)(f.xS,["API/start/".concat(t,"/called")],void 0,n.D.metrics,p);const r=Object.values(n.D);if(void 0===e)e=r;else{if((e=Array.isArray(e)&&e.length?e:[e]).some((e=>!r.includes(e))))return(0,l.Z)("Invalid feature name supplied. Acceptable feature names are: ".concat(r));e.includes(n.D.pageViewEvent)||e.push(n.D.pageViewEvent)}e.forEach((e=>{p.emit("".concat(e,"-opt-in"))}))}catch(e){(0,l.Z)("An unexpected issue occurred",e)}},h.recordReplay=function(){(0,a.p)(f.xS,["API/recordReplay/called"],void 0,n.D.metrics,p),(0,a.p)("recordReplay",[],void 0,n.D.sessionReplay,p)},h.pauseReplay=function(){(0,a.p)(f.xS,["API/pauseReplay/called"],void 0,n.D.metrics,p),(0,a.p)("pauseReplay",[],void 0,n.D.sessionReplay,p)},h.interaction=function(){return(new y).get()};var A=y.prototype={createTracer:function(e,t){var r={},i=this,o="function"==typeof t;return(0,a.p)(f.xS,["API/createTracer/called"],void 0,n.D.metrics,p),(0,a.p)(v+"tracer",[(0,s.z)(),e,r],i,n.D.spa,p),function(){if(g.emit((o?"":"no-")+"fn-start",[(0,s.z)(),i,o],r),o)try{return t.apply(this,arguments)}catch(e){throw g.emit("fn-err",[arguments,this,e],r),e}finally{g.emit("fn-end",[(0,s.z)()],r)}}}};function x(e,t,r,i){return function(){return(0,a.p)(f.xS,["API/"+t+"/called"],void 0,n.D.metrics,p),i&&(0,a.p)(e+t,[(0,s.z)(),...arguments],r?null:this,i,p),r?void 0:this}}function w(){r.e(63).then(r.bind(r,7438)).then((t=>{let{setAPI:r}=t;r(e),(0,c.L)(e,"api")})).catch((()=>(0,l.Z)("Downloading runtime APIs failed...")))}return["actionText","setName","setAttribute","save","ignore","onEnd","getContext","end","get"].forEach((e=>{A[e]=x(v,e,void 0,n.D.spa)})),h.noticeError=function(e,t){"string"==typeof e&&(e=new Error(e)),(0,a.p)(f.xS,["API/noticeError/called"],void 0,n.D.metrics,p),(0,a.p)("err",[e,(0,s.z)(),!1,t],void 0,n.D.jserrors,p)},u.il?(0,d.b2)((()=>w()),!0):w(),h}(e.agentIdentifier,y)),void 0===e.exposed&&(e.exposed=E),v=!0}},1926:(e,t,r)=>{r.nc=(()=>{try{return document?.currentScript?.nonce}catch(e){}return""})()},3325:(e,t,r)=>{"use strict";r.d(t,{D:()=>n,p:()=>i});const n={ajax:"ajax",jserrors:"jserrors",metrics:"metrics",pageAction:"page_action",pageViewEvent:"page_view_event",pageViewTiming:"page_view_timing",sessionReplay:"session_replay",sessionTrace:"session_trace",spa:"spa"},i={[n.pageViewEvent]:1,[n.pageViewTiming]:2,[n.metrics]:3,[n.jserrors]:4,[n.ajax]:5,[n.sessionTrace]:6,[n.pageAction]:7,[n.spa]:8,[n.sessionReplay]:9}}},n={};function i(e){var t=n[e];if(void 0!==t)return t.exports;var a=n[e]={exports:{}};return r[e](a,a.exports,i),a.exports}i.m=r,i.d=(e,t)=>{for(var r in t)i.o(t,r)&&!i.o(e,r)&&Object.defineProperty(e,r,{enumerable:!0,get:t[r]})},i.f={},i.e=e=>Promise.all(Object.keys(i.f).reduce(((t,r)=>(i.f[r](e,t),t)),[])),i.u=e=>({63:"nr-full",110:"nr-full-compressor",379:"nr-full-recorder"}[e]+"-1.248.0.min.js"),i.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),e={},t="NRBA-1.248.0.PROD:",i.l=(r,n,a,o)=>{if(e[r])e[r].push(n);else{var s,c;if(void 0!==a)for(var d=document.getElementsByTagName("script"),u=0;u<d.length;u++){var l=d[u];if(l.getAttribute("src")==r||l.getAttribute("data-webpack")==t+a){s=l;break}}s||(c=!0,(s=document.createElement("script")).charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.setAttribute("data-webpack",t+a),s.src=r,0!==s.src.indexOf(window.location.origin+"/")&&(s.crossOrigin="anonymous"),s.integrity=i.sriHashes[o],s.crossOrigin="anonymous"),e[r]=[n];var f=(t,n)=>{s.onerror=s.onload=null,clearTimeout(h);var i=e[r];if(delete e[r],s.parentNode&&s.parentNode.removeChild(s),i&&i.forEach((e=>e(n))),t)return t(n)},h=setTimeout(f.bind(null,void 0,{type:"timeout",target:s}),12e4);s.onerror=f.bind(null,s.onerror),s.onload=f.bind(null,s.onload),c&&document.head.appendChild(s)}},i.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.p="https://js-agent.newrelic.com/",i.sriHashes={63:"sha512-SIaXPLKlh3HV2oRK5bWCYcTyu19+F1rEGmNbcuuAKur3ywZ0Hej3kAH7KGI46Txu3LrMXd8xRsuAPxTZtDbA5w=="},(()=>{var e={29:0,789:0};i.f.j=(t,r)=>{var n=i.o(e,t)?e[t]:void 0;if(0!==n)if(n)r.push(n[2]);else{var a=new Promise(((r,i)=>n=e[t]=[r,i]));r.push(n[2]=a);var o=i.p+i.u(t),s=new Error;i.l(o,(r=>{if(i.o(e,t)&&(0!==(n=e[t])&&(e[t]=void 0),n)){var a=r&&("load"===r.type?"missing":r.type),o=r&&r.target&&r.target.src;s.message="Loading chunk "+t+" failed.\n("+a+": "+o+")",s.name="ChunkLoadError",s.type=a,s.request=o,n[1](s)}}),"chunk-"+t,t)}};var t=(t,r)=>{var n,a,[o,s,c]=r,d=0;if(o.some((t=>0!==e[t]))){for(n in s)i.o(s,n)&&(i.m[n]=s[n]);if(c)c(i)}for(t&&t(r);d<o.length;d++)a=o[d],i.o(e,a)&&e[a]&&e[a][0](),e[a]=0},r=self["webpackChunk:NRBA-1.248.0.PROD"]=self["webpackChunk:NRBA-1.248.0.PROD"]||[];r.forEach(t.bind(null,0)),r.push=t.bind(null,r.push.bind(r))})(),(()=>{"use strict";i(1926);var e=i(50);class t{addPageAction(t,r){(0,e.Z)("Call to agent api addPageAction failed. The page action feature is not currently initialized.")}setPageViewName(t,r){(0,e.Z)("Call to agent api setPageViewName failed. The page view feature is not currently initialized.")}setCustomAttribute(t,r,n){(0,e.Z)("Call to agent api setCustomAttribute failed. The js errors feature is not currently initialized.")}noticeError(t,r){(0,e.Z)("Call to agent api noticeError failed. The js errors feature is not currently initialized.")}setUserId(t){(0,e.Z)("Call to agent api setUserId failed. The js errors feature is not currently initialized.")}setApplicationVersion(t){(0,e.Z)("Call to agent api setApplicationVersion failed. The agent is not currently initialized.")}setErrorHandler(t){(0,e.Z)("Call to agent api setErrorHandler failed. The js errors feature is not currently initialized.")}finished(t){(0,e.Z)("Call to agent api finished failed. The page action feature is not currently initialized.")}addRelease(t,r){(0,e.Z)("Call to agent api addRelease failed. The js errors feature is not currently initialized.")}start(t){(0,e.Z)("Call to agent api addRelease failed. The agent is not currently initialized.")}recordReplay(){(0,e.Z)("Call to agent api recordReplay failed. The agent is not currently initialized.")}pauseReplay(){(0,e.Z)("Call to agent api pauseReplay failed. The agent is not currently initialized.")}}var r=i(3325),n=i(234);const a=Object.values(r.D);function o(e){const t={};return a.forEach((r=>{t[r]=function(e,t){return!1!==(0,n.Mt)(t,"".concat(e,".enabled"))}(r,e)})),t}var s=i(7530);var c=i(8e3),d=i(5938),u=i(3960),l=i(385);class f extends d.W{constructor(e,t,r){let i=!(arguments.length>3&&void 0!==arguments[3])||arguments[3];super(e,t,r),this.auto=i,this.abortHandler=void 0,this.featAggregate=void 0,this.onAggregateImported=void 0,!1===(0,n.Mt)(this.agentIdentifier,"".concat(this.featureName,".autoStart"))&&(this.auto=!1),this.auto&&(0,c.R)(e,r)}importAggregator(){let t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};if(this.featAggregate)return;if(!this.auto)return void this.ee.on("".concat(this.featureName,"-opt-in"),(()=>{(0,c.R)(this.agentIdentifier,this.featureName),this.auto=!0,this.importAggregator()}));const r=l.il&&!0===(0,n.Mt)(this.agentIdentifier,"privacy.cookies_enabled");let a;this.onAggregateImported=new Promise((e=>{a=e}));const o=async()=>{let n;try{if(r){const{setupAgentSession:e}=await i.e(63).then(i.bind(i,3228));n=e(this.agentIdentifier)}}catch(t){(0,e.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.",t)}try{if(!this.shouldImportAgg(this.featureName,n))return(0,c.L)(this.agentIdentifier,this.featureName),void a(!1);const{lazyFeatureLoader:e}=await i.e(63).then(i.bind(i,8582)),{Aggregate:r}=await e(this.featureName,"aggregate");this.featAggregate=new r(this.agentIdentifier,this.aggregator,t),a(!0)}catch(t){(0,e.Z)("Downloading and initializing ".concat(this.featureName," failed..."),t),this.abortHandler?.(),(0,c.L)(this.agentIdentifier,this.featureName),a(!1)}};l.il?(0,u.b2)((()=>o()),!0):o()}shouldImportAgg(e,t){return e!==r.D.sessionReplay||!!n.Yu.MO&&(!1!==(0,n.Mt)(this.agentIdentifier,"session_trace.enabled")&&(!!t?.isNew||!!t?.state.sessionReplayMode))}}var h=i(7633);class p extends f{static featureName=h.t;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,h.t,r),this.importAggregator()}}var g=i(1117),m=i(1284);class v extends g.w{constructor(e){super(e),this.aggregatedData={}}store(e,t,r,n,i){var a=this.getBucket(e,t,r,i);return a.metrics=function(e,t){t||(t={count:0});return t.count+=1,(0,m.D)(e,(function(e,r){t[e]=b(r,t[e])})),t}(n,a.metrics),a}merge(e,t,r,n,i){var a=this.getBucket(e,t,n,i);if(a.metrics){var o=a.metrics;o.count+=r.count,(0,m.D)(r,(function(e,t){if("count"!==e){var n=o[e],i=r[e];i&&!i.c?o[e]=b(i.t,n):o[e]=function(e,t){if(!t)return e;t.c||(t=y(t.t));return t.min=Math.min(e.min,t.min),t.max=Math.max(e.max,t.max),t.t+=e.t,t.sos+=e.sos,t.c+=e.c,t}(i,o[e])}}))}else a.metrics=r}storeMetric(e,t,r,n){var i=this.getBucket(e,t,r);return i.stats=b(n,i.stats),i}getBucket(e,t,r,n){this.aggregatedData[e]||(this.aggregatedData[e]={});var i=this.aggregatedData[e][t];return i||(i=this.aggregatedData[e][t]={params:r||{}},n&&(i.custom=n)),i}get(e,t){return t?this.aggregatedData[e]&&this.aggregatedData[e][t]:this.aggregatedData[e]}take(e){for(var t={},r="",n=!1,i=0;i<e.length;i++)t[r=e[i]]=A(this.aggregatedData[r]),t[r].length&&(n=!0),delete this.aggregatedData[r];return n?t:null}}function b(e,t){return null==e?function(e){e?e.c++:e={c:1};return e}(t):t?(t.c||(t=y(t.t)),t.c+=1,t.t+=e,t.sos+=e*e,e>t.max&&(t.max=e),e<t.min&&(t.min=e),t):{t:e}}function y(e){return{t:e,min:e,max:e,sos:e*e,c:1}}function A(e){return"object"!=typeof e?[]:(0,m.D)(e,x)}function x(e,t){return t}var w=i(8632),_=i(4402),E=i(4351);var T=i(5546),D=i(7956),S=i(3239),j=i(7894),R=i(9251);class C extends f{static featureName=R.t;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,R.t,r),l.il&&((0,D.N)((()=>(0,T.p)("docHidden",[(0,j.z)()],void 0,R.t,this.ee)),!0),(0,S.bP)("pagehide",(()=>(0,T.p)("winPagehide",[(0,j.z)()],void 0,R.t,this.ee))),this.importAggregator())}}var P=i(3081);class O extends f{static featureName=P.t9;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,P.t9,r),this.importAggregator()}}var k=i(6660);class I{constructor(e,t,r,n){this.name="UncaughtError",this.message=e,this.sourceURL=t,this.line=r,this.column=n}}class N extends f{static featureName=k.t;#e=new Set;constructor(e,t){let n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,k.t,n);try{this.removeOnAbort=new AbortController}catch(e){}this.ee.on("fn-err",((e,t,n)=>{this.abortHandler&&!this.#e.has(n)&&(this.#e.add(n),(0,T.p)("err",[this.#t(n),(0,j.z)()],void 0,r.D.jserrors,this.ee))})),this.ee.on("internal-error",(e=>{this.abortHandler&&(0,T.p)("ierr",[this.#t(e),(0,j.z)(),!0],void 0,r.D.jserrors,this.ee)})),l._A.addEventListener("unhandledrejection",(e=>{this.abortHandler&&(0,T.p)("err",[this.#r(e),(0,j.z)(),!1,{unhandledPromiseRejection:1}],void 0,r.D.jserrors,this.ee)}),(0,S.m$)(!1,this.removeOnAbort?.signal)),l._A.addEventListener("error",(e=>{this.abortHandler&&(this.#e.has(e.error)?this.#e.delete(e.error):(0,T.p)("err",[this.#n(e),(0,j.z)()],void 0,r.D.jserrors,this.ee))}),(0,S.m$)(!1,this.removeOnAbort?.signal)),this.abortHandler=this.#i,this.importAggregator()}#i(){this.removeOnAbort?.abort(),this.#e.clear(),this.abortHandler=void 0}#t(e){return e instanceof Error?e:void 0!==e?.message?new I(e.message,e.filename||e.sourceURL,e.lineno||e.line,e.colno||e.col):new I("string"==typeof e?e:(0,E.P)(e))}#r(e){let t="Unhandled Promise Rejection: ";if(e?.reason instanceof Error)try{return e.reason.message=t+e.reason.message,e.reason}catch(t){return e.reason}if(void 0===e.reason)return new I(t);const r=this.#t(e.reason);return r.message=t+r.message,r}#n(e){return e.error instanceof Error?e.error:new I(e.message,e.filename,e.lineno,e.colno)}}var H=i(2210);let z=1;const M="nr@id";function L(e){const t=typeof e;return!e||"object"!==t&&"function"!==t?-1:e===l._A?0:(0,H.X)(e,M,(function(){return z++}))}function Z(e){if("string"==typeof e&&e.length)return e.length;if("object"==typeof e){if("undefined"!=typeof ArrayBuffer&&e instanceof ArrayBuffer&&e.byteLength)return e.byteLength;if("undefined"!=typeof Blob&&e instanceof Blob&&e.size)return e.size;if(!("undefined"!=typeof FormData&&e instanceof FormData))try{return(0,E.P)(e).length}catch(e){return}}}var F=i(1214),B=i(7243);class U{constructor(e){this.agentIdentifier=e}generateTracePayload(e){if(!this.shouldGenerateTrace(e))return null;var t=(0,n.DL)(this.agentIdentifier);if(!t)return null;var r=(t.accountID||"").toString()||null,i=(t.agentID||"").toString()||null,a=(t.trustKey||"").toString()||null;if(!r||!i)return null;var o=(0,_.M)(),s=(0,_.Ht)(),c=Date.now(),d={spanId:o,traceId:s,timestamp:c};return(e.sameOrigin||this.isAllowedOrigin(e)&&this.useTraceContextHeadersForCors())&&(d.traceContextParentHeader=this.generateTraceContextParentHeader(o,s),d.traceContextStateHeader=this.generateTraceContextStateHeader(o,c,r,i,a)),(e.sameOrigin&&!this.excludeNewrelicHeader()||!e.sameOrigin&&this.isAllowedOrigin(e)&&this.useNewrelicHeaderForCors())&&(d.newrelicHeader=this.generateTraceHeader(o,s,c,r,i,a)),d}generateTraceContextParentHeader(e,t){return"00-"+t+"-"+e+"-01"}generateTraceContextStateHeader(e,t,r,n,i){return i+"@nr=0-1-"+r+"-"+n+"-"+e+"----"+t}generateTraceHeader(e,t,r,n,i,a){if(!("function"==typeof l._A?.btoa))return null;var o={v:[0,1],d:{ty:"Browser",ac:n,ap:i,id:e,tr:t,ti:r}};return a&&n!==a&&(o.d.tk=a),btoa((0,E.P)(o))}shouldGenerateTrace(e){return this.isDtEnabled()&&this.isAllowedOrigin(e)}isAllowedOrigin(e){var t=!1,r={};if((0,n.Mt)(this.agentIdentifier,"distributed_tracing")&&(r=(0,n.P_)(this.agentIdentifier).distributed_tracing),e.sameOrigin)t=!0;else if(r.allowed_origins instanceof Array)for(var i=0;i<r.allowed_origins.length;i++){var a=(0,B.e)(r.allowed_origins[i]);if(e.hostname===a.hostname&&e.protocol===a.protocol&&e.port===a.port){t=!0;break}}return t}isDtEnabled(){var e=(0,n.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.enabled}excludeNewrelicHeader(){var e=(0,n.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.exclude_newrelic_header}useNewrelicHeaderForCors(){var e=(0,n.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!1!==e.cors_use_newrelic_header}useTraceContextHeadersForCors(){var e=(0,n.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.cors_use_tracecontext_headers}}var V=i(7825),q=["load","error","abort","timeout"],G=q.length,W=n.Yu.REQ,X=n.Yu.XHR;class K extends f{static featureName=V.t;constructor(e,t){let i=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(super(e,t,V.t,i),(0,n.OP)(e).xhrWrappable){this.dt=new U(e),this.handler=(e,t,r,n)=>(0,T.p)(e,t,r,n,this.ee);try{const e={xmlhttprequest:"xhr",fetch:"fetch",beacon:"beacon"};l._A?.performance?.getEntriesByType("resource").forEach((t=>{if(t.initiatorType in e&&0!==t.responseStatus){const n={status:t.responseStatus},i={rxSize:t.transferSize,duration:Math.floor(t.duration),cbTime:0};Q(n,t.name),this.handler("xhr",[n,i,t.startTime,t.responseEnd,e[t.initiatorType]],void 0,r.D.ajax)}}))}catch(e){}(0,F.u5)(this.ee),(0,F.Kf)(this.ee),function(e,t,i,a){function o(e){var t=this;t.totalCbs=0,t.called=0,t.cbTime=0,t.end=w,t.ended=!1,t.xhrGuids={},t.lastSize=null,t.loadCaptureCalled=!1,t.params=this.params||{},t.metrics=this.metrics||{},e.addEventListener("load",(function(r){_(t,e)}),(0,S.m$)(!1)),l.IF||e.addEventListener("progress",(function(e){t.lastSize=e.loaded}),(0,S.m$)(!1))}function s(e){this.params={method:e[0]},Q(this,e[1]),this.metrics={}}function c(t,r){var i=(0,n.DL)(e);i.xpid&&this.sameOrigin&&r.setRequestHeader("X-NewRelic-ID",i.xpid);var o=a.generateTracePayload(this.parsedOrigin);if(o){var s=!1;o.newrelicHeader&&(r.setRequestHeader("newrelic",o.newrelicHeader),s=!0),o.traceContextParentHeader&&(r.setRequestHeader("traceparent",o.traceContextParentHeader),o.traceContextStateHeader&&r.setRequestHeader("tracestate",o.traceContextStateHeader),s=!0),s&&(this.dt=o)}}function d(e,r){var n=this.metrics,i=e[0],a=this;if(n&&i){var o=Z(i);o&&(n.txSize=o)}this.startTime=(0,j.z)(),this.body=i,this.listener=function(e){try{"abort"!==e.type||a.loadCaptureCalled||(a.params.aborted=!0),("load"!==e.type||a.called===a.totalCbs&&(a.onloadCalled||"function"!=typeof r.onload)&&"function"==typeof a.end)&&a.end(r)}catch(e){try{t.emit("internal-error",[e])}catch(e){}}};for(var s=0;s<G;s++)r.addEventListener(q[s],this.listener,(0,S.m$)(!1))}function u(e,t,r){this.cbTime+=e,t?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&"function"==typeof r.onload||"function"!=typeof this.end||this.end(r)}function f(e,t){var r=""+L(e)+!!t;this.xhrGuids&&!this.xhrGuids[r]&&(this.xhrGuids[r]=!0,this.totalCbs+=1)}function h(e,t){var r=""+L(e)+!!t;this.xhrGuids&&this.xhrGuids[r]&&(delete this.xhrGuids[r],this.totalCbs-=1)}function p(){this.endTime=(0,j.z)()}function g(e,r){r instanceof X&&"load"===e[0]&&t.emit("xhr-load-added",[e[1],e[2]],r)}function m(e,r){r instanceof X&&"load"===e[0]&&t.emit("xhr-load-removed",[e[1],e[2]],r)}function v(e,t,r){t instanceof X&&("onload"===r&&(this.onload=!0),("load"===(e[0]&&e[0].type)||this.onload)&&(this.xhrCbStart=(0,j.z)()))}function b(e,r){this.xhrCbStart&&t.emit("xhr-cb-time",[(0,j.z)()-this.xhrCbStart,this.onload,r],r)}function y(e){var t,r=e[1]||{};if("string"==typeof e[0]?0===(t=e[0]).length&&l.il&&(t=""+l._A.location.href):e[0]&&e[0].url?t=e[0].url:l._A?.URL&&e[0]&&e[0]instanceof URL?t=e[0].href:"function"==typeof e[0].toString&&(t=e[0].toString()),"string"==typeof t&&0!==t.length){t&&(this.parsedOrigin=(0,B.e)(t),this.sameOrigin=this.parsedOrigin.sameOrigin);var n=a.generateTracePayload(this.parsedOrigin);if(n&&(n.newrelicHeader||n.traceContextParentHeader))if(e[0]&&e[0].headers)s(e[0].headers,n)&&(this.dt=n);else{var i={};for(var o in r)i[o]=r[o];i.headers=new Headers(r.headers||{}),s(i.headers,n)&&(this.dt=n),e.length>1?e[1]=i:e.push(i)}}function s(e,t){var r=!1;return t.newrelicHeader&&(e.set("newrelic",t.newrelicHeader),r=!0),t.traceContextParentHeader&&(e.set("traceparent",t.traceContextParentHeader),t.traceContextStateHeader&&e.set("tracestate",t.traceContextStateHeader),r=!0),r}}function A(e,t){this.params={},this.metrics={},this.startTime=(0,j.z)(),this.dt=t,e.length>=1&&(this.target=e[0]),e.length>=2&&(this.opts=e[1]);var r,n=this.opts||{},i=this.target;"string"==typeof i?r=i:"object"==typeof i&&i instanceof W?r=i.url:l._A?.URL&&"object"==typeof i&&i instanceof URL&&(r=i.href),Q(this,r);var a=(""+(i&&i instanceof W&&i.method||n.method||"GET")).toUpperCase();this.params.method=a,this.body=n.body,this.txSize=Z(n.body)||0}function x(e,t){var n;this.endTime=(0,j.z)(),this.params||(this.params={}),this.params.status=t?t.status:0,"string"==typeof this.rxSize&&this.rxSize.length>0&&(n=+this.rxSize);var a={txSize:this.txSize,rxSize:n,duration:(0,j.z)()-this.startTime};i("xhr",[this.params,a,this.startTime,this.endTime,"fetch"],this,r.D.ajax)}function w(e){var t=this.params,n=this.metrics;if(!this.ended){this.ended=!0;for(var a=0;a<G;a++)e.removeEventListener(q[a],this.listener,!1);t.aborted||(n.duration=(0,j.z)()-this.startTime,this.loadCaptureCalled||4!==e.readyState?null==t.status&&(t.status=0):_(this,e),n.cbTime=this.cbTime,i("xhr",[t,n,this.startTime,this.endTime,"xhr"],this,r.D.ajax))}}function _(e,t){e.params.status=t.status;var r=function(e,t){var r=e.responseType;return"json"===r&&null!==t?t:"arraybuffer"===r||"blob"===r||"json"===r?Z(e.response):"text"===r||""===r||void 0===r?Z(e.responseText):void 0}(t,e.lastSize);if(r&&(e.metrics.rxSize=r),e.sameOrigin){var n=t.getResponseHeader("X-NewRelic-App-Data");n&&(e.params.cat=n.split(", ").pop())}e.loadCaptureCalled=!0}t.on("new-xhr",o),t.on("open-xhr-start",s),t.on("open-xhr-end",c),t.on("send-xhr-start",d),t.on("xhr-cb-time",u),t.on("xhr-load-added",f),t.on("xhr-load-removed",h),t.on("xhr-resolved",p),t.on("addEventListener-end",g),t.on("removeEventListener-end",m),t.on("fn-end",b),t.on("fetch-before-start",y),t.on("fetch-start",A),t.on("fn-start",v),t.on("fetch-done",x)}(e,this.ee,this.handler,this.dt),this.importAggregator()}}}function Q(e,t){var r=(0,B.e)(t),n=e.params||e;n.hostname=r.hostname,n.port=r.port,n.protocol=r.protocol,n.host=r.hostname+":"+r.port,n.pathname=r.pathname,e.parsedOrigin=r,e.sameOrigin=r.sameOrigin}var Y=i(3614);const{BST_RESOURCE:J,RESOURCE:ee,START:te,END:re,FEATURE_NAME:ne,FN_END:ie,FN_START:ae,PUSH_STATE:oe}=Y;var se=i(7144);class ce extends f{static featureName=se.t;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,se.t,r),this.importAggregator()}}var de=i(4649);class ue extends f{static featureName=de.t;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,de.t,r),this.importAggregator()}}new class extends t{constructor(t){let r=arguments.length>1&&void 0!==arguments[1]?arguments[1]:(0,_.ky)(16);super(),l._A?(this.agentIdentifier=r,this.sharedAggregator=new v({agentIdentifier:this.agentIdentifier}),this.features={},(0,w.h5)(r,this),this.desiredFeatures=new Set(t.features||[]),this.desiredFeatures.add(p),(0,s.j)(this,t,t.loaderType||"agent"),this.run()):(0,e.Z)("Failed to initial the agent. Could not determine the runtime environment.")}get config(){return{info:(0,n.C5)(this.agentIdentifier),init:(0,n.P_)(this.agentIdentifier),loader_config:(0,n.DL)(this.agentIdentifier),runtime:(0,n.OP)(this.agentIdentifier)}}run(){try{const t=o(this.agentIdentifier),n=[...this.desiredFeatures];n.sort(((e,t)=>r.p[e.featureName]-r.p[t.featureName])),n.forEach((n=>{if(t[n.featureName]||n.featureName===r.D.pageViewEvent){const i=function(e){switch(e){case r.D.ajax:return[r.D.jserrors];case r.D.sessionTrace:return[r.D.ajax,r.D.pageViewEvent];case r.D.sessionReplay:return[r.D.sessionTrace];case r.D.pageViewTiming:return[r.D.pageViewEvent];default:return[]}}(n.featureName);i.every((e=>t[e]))||(0,e.Z)("".concat(n.featureName," is enabled but one or more dependent features has been disabled (").concat((0,E.P)(i),"). This may cause unintended consequences or missing data...")),this.features[n.featureName]=new n(this.agentIdentifier,this.sharedAggregator)}}))}catch(t){(0,e.Z)("Failed to initialize all enabled instrument classes (agent aborted) -",t);for(const e in this.features)this.features[e].abortHandler?.();const r=(0,w.fP)();return delete r.initializedAgents[this.agentIdentifier]?.api,delete r.initializedAgents[this.agentIdentifier]?.features,delete this.sharedAggregator,r.ee?.abort(),delete r.ee?.get(this.agentIdentifier),!1}}addToTrace(t){(0,e.Z)("Call to agent api addToTrace failed. The session trace feature is not currently initialized.")}setCurrentRouteName(t){(0,e.Z)("Call to agent api setCurrentRouteName failed. The spa feature is not currently initialized.")}interaction(){(0,e.Z)("Call to agent api interaction failed. The spa feature is not currently initialized.")}}({features:[p,C,class extends f{static featureName=ne;constructor(e,t){if(super(e,t,ne,!(arguments.length>2&&void 0!==arguments[2])||arguments[2]),!l.il)return;const n=this.ee;let i;(0,F.QU)(n),this.eventsEE=(0,F.em)(n),this.eventsEE.on(ae,(function(e,t){this.bstStart=(0,j.z)()})),this.eventsEE.on(ie,(function(e,t){(0,T.p)("bst",[e[0],t,this.bstStart,(0,j.z)()],void 0,r.D.sessionTrace,n)})),n.on(oe+te,(function(e){this.time=(0,j.z)(),this.startPath=location.pathname+location.hash})),n.on(oe+re,(function(e){(0,T.p)("bstHist",[location.pathname+location.hash,this.startPath,this.time],void 0,r.D.sessionTrace,n)}));try{i=new PerformanceObserver((e=>{const t=e.getEntries();(0,T.p)(J,[t],void 0,r.D.sessionTrace,n)})),i.observe({type:ee,buffered:!0})}catch(e){}this.importAggregator({resourceObserver:i})}},ce,K,O,ue,N],loaderType:"pro"})})()})();</script>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <title>Frontiers | Human- versus Artificial Intelligence</title>

    <link rel="shortcut icon" href="https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/favicon_16x16.ico" type="image/x-icon" />

    <meta property="og:type" content="article" />
    <meta property="frontiers:type" content="Article" />
    <meta property="og:site_name" name="site_name" content="Frontiers" />
    <meta property="og:title" name="Title" content="Human- versus Artificial Intelligence" />
    <meta property="og:description" name="Description" content="AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human- and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and collaborate with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human- and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa..." />
    <meta property="og:url" name="url" content="https://www.frontiersin.org/articles/10.3389/frai.2021.622364/full" />


        <meta property="og:image" content="https://www.frontiersin.org/files/MyHome%20Article%20Library/622364/622364_Thumb_400.jpg" />
            <meta name="citation_volume" content="4" />
        <meta name="citation_journal_title" content="Frontiers in Artificial Intelligence" />
        <meta name="citation_publisher" content="Frontiers" />
        <meta name="citation_journal_abbrev" content="Front. Artif. Intell." />
        <meta name="citation_issn" content="2624-8212" />
        <meta name="citation_doi" content="10.3389/frai.2021.622364" />
        <meta name="citation_firstpage" content="622364" />
        <meta name="citation_language" content="English" />
        <meta name="citation_title" content="Human- versus Artificial Intelligence" />
        <meta name="citation_keywords" content="human intelligence; artificial intelligence; Artificial General Intelligence (AGI); human-level artificial intelligence; Cognitive    complexity; narrow artificial intelligence; human-AI collaboration; cognitive bias" />
        <meta name="citation_abstract" content="AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions. In addition, this lack of understanding may lead to the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. By presenting and discussing three notions on the similarities and differences of human- and artificial intelligence, this paper aims to substantiate possible future research objectives. These notions concern: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple forms of narrow-hybrid AI systems. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and collaborate with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgement required? How can we capitalize on the strengths of human intelligence and how to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition. Should we pursue the development of AI partners with human(-level) intelligence or should we focus at supplementing human constraints and limitations. In order to answer these questions, in professional settings and teams, humans collaborating with AI systems have to develop an adequate mental model of the underlying generic psychological operating mechanisms of AI systems. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans needs to be addressed more emphatically." />
        <meta name="description" content="AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions. In addition, this lack of understanding may lead to the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. By presenting and discussing three notions on the similarities and differences of human- and artificial intelligence, this paper aims to substantiate possible future research objectives. These notions concern: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple forms of narrow-hybrid AI systems. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and collaborate with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgement required? How can we capitalize on the strengths of human intelligence and how to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition. Should we pursue the development of AI partners with human(-level) intelligence or should we focus at supplementing human constraints and limitations. In order to answer these questions, in professional settings and teams, humans collaborating with AI systems have to develop an adequate mental model of the underlying generic psychological operating mechanisms of AI systems. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans needs to be addressed more emphatically." />
        <meta name="citation_online_date" content="2021/02/01" />
        <meta name="citation_publication_date" content="2021/03/25" />
        <meta name="citation_author" content="Korteling, J. E. (Hans)." />
        <meta name="citation_author_institution" content=", Netherlands" />
        <meta name="citation_author" content="van de Boer-Visschedijk, G. C." />
        <meta name="citation_author_institution" content=", Netherlands" />
        <meta name="citation_author" content="Blankendaal, R. A. M." />
        <meta name="citation_author_institution" content=", Netherlands" />
        <meta name="citation_author" content="Boonekamp, R. C." />
        <meta name="citation_author_institution" content=", Netherlands" />
        <meta name="citation_author" content="Eikelboom, A. R." />
        <meta name="citation_author_institution" content=", Netherlands" />
    <meta name="Keywords" content="human intelligence, artificial intelligence, Artificial General Intelligence (AGI), human-level artificial intelligence, Cognitive    complexity, narrow artificial intelligence, human-AI collaboration, cognitive bias" />

        <meta name="dc.identifier" content="doi:10.3389/frai.2021.622364">
        <!--CrossMark widget-->
    
    <script type="text/javascript">

        var CurrentIBarMenu = 'bysubjects';
        var CurrentPageCode = 'ARTICLE_PAGE_NEW';

        var FRConfiguration = (function () {
            return {
                Environment: 'Live',
                SANVirtualPath: 'https://www.frontiersin.org/files/',
                SharepointWebsiteUrl: 'https://www.frontiersin.org',
                FrontiersJournalUIUrl: 'https://www.frontiersin.org',
                FrontiersJournalAPIUrl: 'https://api-journal.frontiersin.org',
                FrontiersReviewUIUrl: '',
                FrontiersReviewAPIUrl: '',
                FrontiersCookie: 'frontiersN',
                FrontiersCookieRememberMe: 'frontiersNt',
                FrontiersLoginUrl: 'https://www.frontiersin.org/Login.aspx',
                FrontiersRegistrationUrl: 'http://www.frontiersin.org/Registration/Register.aspx',
                IsCommentVisible: 'True',
                IsPreview: 'False',
                TenantName: 'Frontiers'
            }
        })();

        var FRLanguage = (function() {
            var languageSet = {"article_accepteddate":"Accepted: ","article_analyticstooltip":"Citation numbers are available from Dimensions","article_analyticsviewimpact":"View article impact","article_bibtex":"BibTex","article_copyrighttext":"This is an open-access article distributed under the terms of the \u003ca href=\"http://creativecommons.org/licenses/by/4.0/\"\u003eCreative Commons Attribution License (CC BY)\u003c/a\u003e. The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.","article_downloadarticle":"Download Article","article_downloadpdf":"Download PDF","article_downloadprovisionalarticle":"Download Provisional Article","article_endnote":"EndNote","article_epub":"EPUB","article_exportcitation":"Export Citation","article_jats":"JATS","article_nlm":"XML (NLM)","article_pdf":"PDF","article_provisionalpdf":"Provisional PDF","article_publisheddate":"Published online: ","article_readfulltext":"Read Full Text","article_receiveddate":"Received: ","article_referencemanager":"Reference Manager","article_shareon":"SHARE ON","article_simpletextfile":"Simple TEXT file","article_supplementaldata":"SUPPLEMENTAL DATA","article_tableofcontent":"TABLE OF CONTENTS","article_viewenhancedpdf":"ReadCube","article_xml":"XML","browserwarningtext":"\u003ch2\u003eWarning!\u003c/h2\u003e\u003cp\u003eYou are using an \u003cstrong\u003eoutdated\u003c/strong\u003e browser. This page doesn\u0027t support Internet Explorer 6, 7 and 8.\u003cbr /\u003ePlease \u003ca class=\"blue\" href=\"http://browsehappy.com/\"\u003eupgrade your browser\u003c/a\u003e or \u003ca class=\"blue\" href=\"http://www.google.com/chromeframe/?redirect=true\"\u003eactivate Google Chrome Frame\u003c/a\u003e to improve your experience.\u003c/p\u003e","impact_backtoarticle":"Back to article","people_also_lookedat":"People also looked at","article_analyticstotalviews":"total views","ART_FRONTIERS":"Frontiers","Article_AnalyticsTotalViews":"total views","Article_ArchiveLinkText":"Articles","Article_Citation":"Citation:","Article_Commentary":"COMMENTARY","Article_Copyright":"Copyright:","Article_Correspondence":"* Correspondence:","Article_DownloadProvisionalPDF":"Download Provisional PDF","Article_EditedBy":"Edited by:","Article_Keywords":"Keywords:","Article_OriginalArticle":"ORIGINAL ARTICLE","Article_PaperPendingPublishedDate":"Paper pending published:","Article_ReviewedBy":"Reviewed by: ","Article_RTInfoText":"This article is part of the Research Topic","COMMENT_HEADERTEXT":"Comment text too long","COMMENT_WARNINGTEXT":"Comments must be less than 4,000 characters. You have entered ","article_lastmodifieddate":"Last modified: "};

            return {
                value: function(key) {
                    if (languageSet[key]) {
                        return languageSet[key];
                    } else {
                        throw new Error('Unable to get the value status from the language set'); // Use Error, not FRError
                    }
                }
            };

        })();

        var FRJournalDetails = (function() {
            return {
                JournalType: 'section',
                JournalId: '1437',
                SectionId: '1476'
            };
    })();

    var FRArticleDetails = (function () {

            return {
                ArticleId: '622364',
                SectionTitle: 'AI for Human Learning and Behavior Change',
                DisclaimerText: 'All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher.',
                IsHTMLPublished:'True',
                IsPreview: 'False',
                LinkedArticles: null,
                DisplayRelatedArticlesBox: 'True',
                DisplayTitlePillLabels: 'False',
                IsOriginalArticle: 'False',
                IsArticleIdsToHideRelatedArticleBox: 'False'
            };
        })();
        var FRArticleRecaptchaSettings = (function() {
            return {
                RecaptchaSiteKey: '6LdG3i0UAAAAAOC4qUh35ubHgJotEHp_STXHgr_v'
            };
        })();

    </script>

    <link href="https://209cd62b0febf2a55d40-715eb384bc6027b876e93ad33d5c5ec3.ssl.cf3.rackcdn.com/font-awesome-4.3.0/css/font-awesome.min.css" rel="stylesheet"/>

    <link href="https://9d0dd7a648345f19af83-877d2ecaf11b88d5e17327c758e17ef6.ssl.cf2.rackcdn.com/museo-sans-1.0.1/css/museo-sans.css" rel="stylesheet"/>

    <link href="https://static.frontiersin.org/areas/articles/css/app?v=szEtYuDUacu1rx0AReqmqv_LiIv4f67cViCvsMT_9pM1" rel="stylesheet"/>

    <style type="text/css"> .journal-artificial-intelligence  {
    background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-artificial-intelligence.png') no-repeat center 51px;
}

@media only screen and (max-width: 1409px) {
    .journal-artificial-intelligence  {
        background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-artificial-intelligence.png') no-repeat center 51px;
    }
}

@media only screen and (max-width: 1250px) {
    .journal-artificial-intelligence  {
        background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-artificial-intelligence.png') no-repeat center 51px;
    }
}

@media only screen and (max-width: 992px) {
    .journal-artificial-intelligence  {
        background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-artificial-intelligence.png') no-repeat center 51px;
    }
}

@media only screen and (max-width: 768px) {
    .journal-artificial-intelligence  {
        background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-artificial-intelligence.png') no-repeat center 51px;
    }
}

@media only screen and (max-width: 480px) {
    .journal-artificial-intelligence  {
        background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-artificial-intelligence.png') no-repeat center 51px;
    }
}

@media only screen and (max-width: 320px) {
    .journal-artificial-intelligence  {
        background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-artificial-intelligence.png') no-repeat center 51px;
    }
} </style>
    
</head>
<body class="journal-artificial-intelligence section-ai-for-human-learning-and-behavior-change">

    
    <a href="#main-content" class="bypassBlock-wrapper" id="bypass">
        <span class="bypassBlock-button">Skip to main content</span>
    </a>
    <script type="text/javascript">

        const container = document.getElementById("bypass");

        //eventListeners
        container.addEventListener('focus', function () {
            document.documentElement.setAttribute("bypass-focus", "true");
        }, true);

        container.addEventListener('blur', function () {
            document.documentElement.setAttribute("bypass-focus", "false");
        }, true);

    </script>

    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://tag-manager.frontiersin.org/ns.html?id=GTM-5SXX286&gtm_auth=u-n-7MEC83-lilAhi0fqhQ&gtm_preview=env-1&gtm_cookies_win=x"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>
    <!-- End Google Tag Manager (noscript) -->

    <script src="https://static.frontiersin.org/areas/articles/js/vendors?v=vbc13dTGr7qL7XerHFDUYx5a5aMhnEvReT2oPGkMwm81"></script>



    <frontiers-ibar main-domain="frontiersin.org" is-frontiers="true" tenant-name="Frontiers" loop-url="https://loop.frontiersin.org" journal-id="1437"></frontiers-ibar>
    <div class="page-container sidemenu-collapsed" id="main-content">
        


<div id="article" class="boxed white no-padding">
    <div class="container-fluid main-container-xxl">
        <div class="row" id="similar-articles">
            <div class="new-wrapper">
                


<style>
    .disabled-supplementary-btn {
        cursor: not-allowed;
        pointer-events: none;
        opacity: .65;
        filter: alpha(opacity=65);
        -webkit-box-shadow: none;
        box-shadow: none;
    }
</style>


<aside class="right-container">

        <div class="side-article-download clearfix">
            <ul class="list-unstyled list-inline clearfix">
                    <li class="dropdown text-center paper">
                        <button data-target="#" class="dropdown-toggle" data-test-id="download-button" data-toggle="dropdown" role="button" aria-expanded="false">
                            <span class="icon-label" data-event="download-button-click">Download Article</span>
                        </button>

                        <div class="dropdown-menu-wrapper">
                            <div class="dropdown-menu-mobile-title">
                                Download Article
                            </div>
                            <ul class="dropdown-menu" role="menu">
                                    <li>
                                        <a class="download-files-pdf action-link" href="/articles/10.3389/frai.2021.622364/pdf?isPublishedV2=False" data-test-id="article-downloadpdf" data-event="downloadpdf-button-click" id="download_article">
                                            Download PDF
                                        </a>
                                    </li>
                                    <li>
                                        <a class="download-files-readcube" href="http://www.readcube.com/articles/10.3389/frai.2021.622364" data-test-id="article-viewenhancedpdf" data-event="downloadreadcube-button-click" id="download_article">
                                            ReadCube
                                        </a>
                                    </li>
                                    <li>
                                        <a class="download-files-epub" href="/articles/10.3389/frai.2021.622364/epub?isPublishedV2=False" data-test-id="article-epub" data-event="downloadepub-button-click" id="download_article">
                                            EPUB
                                        </a>
                                    </li>
                                    <li>
                                        <a class="download-files-nlm" href="/articles/10.3389/frai.2021.622364/xml/nlm?isPublishedV2=False" data-test-id="article-nlm" data-event="downloadnlm-button-click" id="download_article">
                                            XML (NLM)
                                        </a>
                                    </li>
                                
                            </ul>
                            <button class="dropdown-menu-mobile-close-button" aria-label="Close" data-toggle="dropdown"></button>
                        </div>
                    </li>
                                <li class="dropdown">
                    <button type="button" class="navbar-toggle navbar-share" data-toggle="dropdown" data-target="#" aria-expanded="false"></button>
                    <div class="dropdown-menu-wrapper share-dropdown">
                        <div class="dropdown-menu-mobile-title">
                            Share on
                        </div>
                        <ul class="dropdown-menu" role="menu">
                            <li class="social_block">
                                <div class="atButton">
                                    <a class="addthis_button_twitter at300b"
                                       title="Tweet"
                                       target="_blank"
                                       data-test-id="share-article-twitter-mobile"
                                       href="https://twitter.com/intent/tweet?url=https://www.frontiersin.org/articles/10.3389/frai.2021.622364&amp;text=Human- versus Artificial Intelligence">
                                        <span class="at-icon-wrapper"></span>
                                    </a>
                                </div>
                            </li>
                            <li class="social_block">
                                <div class="atButton">
                                    <a class="addthis_button_linkedin at300b"
                                       title="LinkedIn"
                                       target="_blank"
                                       data-test-id="share-article-linkedin-mobile"
                                       href="https://www.linkedin.com/sharing/share-offsite/?url=https://www.frontiersin.org/articles/10.3389/frai.2021.622364&amp;title=Human- versus Artificial Intelligence">
                                        <span class="at-icon-wrapper"></span>
                                    </a>
                                </div>
                            </li>
                            <li class="social_block">
                                <div class="atButton">
                                    <a class="addthis_button_facebook at300b"
                                       title="Facebook"
                                       target="_blank"
                                       data-test-id="share-article-facebook-mobile"
                                       href="https://www.facebook.com/sharer.php?u=https://www.frontiersin.org/articles/10.3389/frai.2021.622364">
                                        <span class="at-icon-wrapper"></span>
                                    </a>
                                </div>
                            </li>
                        </ul>
                        <button class="dropdown-menu-mobile-close-button"
                                aria-label="Close"
                                data-toggle="dropdown"></button>
                    </div>
                </li>

                <li class="dropdown">
                    <button type="button" class="navbar-toggle navbar-citations" data-event="citation-button-click" data-toggle="dropdown" data-target="#" aria-expanded="false"></button>
                    <div class="dropdown-menu-wrapper">
                        <div class="dropdown-menu-mobile-title">
                            Export citation
                        </div>
                        <ul class="dropdown-menu" role="menu">
                                <li>
                                    <a data-test-id="article-endnote" href="/articles/10.3389/frai.2021.622364/endNote" data-event="endnotedownload-button-click">
                                        EndNote
                                    </a>
                                </li>
                                <li>
                                    <a data-test-id="article-referencemanager" href="/articles/10.3389/frai.2021.622364/reference" data-event="referencemanagerdownload-button-click">
                                        Reference Manager
                                    </a>
                                </li>
                                <li>
                                    <a data-test-id="article-simpletextfile" href="/articles/10.3389/frai.2021.622364/text" data-event="simpletextfiledownload-button-click">
                                        Simple TEXT file
                                    </a>
                                </li>
                                <li>
                                    <a data-test-id="article-bibtex" href="/articles/10.3389/frai.2021.622364/bibTex" data-event="bibtexdownload-button-click">
                                        BibTex
                                    </a>
                                </li>
                        </ul>
                        <button class="dropdown-menu-mobile-close-button"
                                aria-label="Close"
                                data-toggle="dropdown"></button>
                    </div>
                </li>
            </ul>
        </div>
        
        <div class="stats-container">
            <div class="divide-container">
                <div class="flex-container-metrics">
                    <ul class="nav">
                            <li class="impact-data" id="views-section" style="display:none">
                                <span class="views-count" id="views-count"></span>
                                <span class="title-views">Total views</span>
                            </li>
                            <li class="impact-data" id="downloads-section" style="display:none">
                                <span class="views-count" id="downloads-count"></span>
                                <span class="title-views">Downloads</span>
                            </li>
                            <li class="impact-data" id="citations-section" style="display:none">
                                <span class="views-count" id="citations-count"></span>
                                <span class="title-views">Citations</span>
                            </li>
                    </ul>
                        <div class="infoPopover" id="infoPopover" style="display:none">
                            <div role="button" class="infoPopover__trigger"></div>
                            <div class="infoPopover__content">
                                <p>
                                    Citation numbers are available from Dimensions
                                </p>
                                <div role="button" class="infoPopover__close"></div>
                            </div>
                        </div>
                </div>
                    <div class="articleImpact" id="article-impact">
                        <span class="borderLine"></span>
                            <a class="articleImpact--url"
                               data-test-id="view-article-impact"
                               data-event="impact-button-click"
                               href="http://loop-impact.frontiersin.org/impact/article/622364#views"
                               target="_blank">View article impact</a>
                    </div>
            </div>
            <div class="side-article-impact">
                <ul class="nav">
                    <li class="altmetric-wrapper">
                        <div class='altmetric-embed' data-badge-type='donut' data-event="altmetric-button-click" data-badge-popover="bottom" data-doi='10.3389/frai.2021.622364' data-condensed="true" data-link-target="new"></div>
                        <a class="altmetricScore_url" href="https://www.altmetric.com/details/doi/10.3389/frai.2021.622364" target="_blank">View altmetric score</a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="side-article-share ">
            <h5 class="like-h4">SHARE ON</h5>
            <ul class="share-media clearfix" style="padding: 0;">
                <li class="social_block">
                    <div class="atButton">
                        <a class="addthis_button_facebook at300b" title="Facebook" data-test-id="share-article-facebook" href="https://www.facebook.com/sharer.php?u=https://www.frontiersin.org/articles/10.3389/frai.2021.622364" target="_blank">
                            <span class="at-icon-wrapper" style="
                              background-color: rgb(29, 161, 242);
                              line-height: 16px;
                              height: 16px;
                              width: 16px;
                            "></span>
                        </a>
                    </div>
                </li>
                <li class="social_block">
                    <div class="atButton">
                        <a class="addthis_button_twitter at300b" title="Tweet" data-test-id="share-article-twitter" href="https://twitter.com/intent/tweet?url=https://www.frontiersin.org/articles/10.3389/frai.2021.622364&amp;text=Human- versus Artificial Intelligence" target="_blank">
                            <span class="at-icon-wrapper" style="
                              background-color: rgb(29, 161, 242);
                              line-height: 16px;
                              height: 16px;
                              width: 16px;
                            "></span>
                        </a>
                    </div>
                </li>

                <li class="social_block">
                    <div class="atButton">
                        <a class="addthis_button_linkedin at300b" title="LinkedIn" data-test-id="share-article-linkedin" href="https://www.linkedin.com/sharing/share-offsite/?url=https://www.frontiersin.org/articles/10.3389/frai.2021.622364&amp;title=Human- versus Artificial Intelligence" target="_blank">
                            <span class="at-icon-wrapper" style="
                              background-color: rgb(29, 161, 242);
                              line-height: 16px;
                              height: 16px;
                              width: 16px;
                            "></span>
                        </a>
                    </div>
                </li>
            </ul>
        </div>

            <aside id="anchors" class="pull-left table-of-contents side-article">
            <div class="side-people hidden-sm hidden-xs">
                    <section class="side-article-editors">
                                <header><h5 class="like-h4">Edited by</h5></header>
                                <a class="authors" href="https://loop.frontiersin.org/people/622928/overview" target="_blank">
                                    <img alt=" " class="pr5 pull-left" onerror="this.src = &#39;/Areas/Articles/Images/Frontiers/Common/Profile/default-loop-profile.jpg&#39;" src="https://loop.frontiersin.org/images/profile/622928/24"></img>
                                    <h6 class="author-link-aside">Esma A&#239;meur</h6>
                                </a>
                                <div class="clearfix"></div>
                                <p><span class="notes">Montreal University, Canada</span></p>
                                                        <header><h5 class="like-h4"> Reviewed by</h5></header>
                                    <a class="authors" href="https://loop.frontiersin.org/people/626150/overview" target="_blank">
                                        <img alt=" " class="pr5 pull-left" onerror="this.src = &#39;/Areas/Articles/Images/Frontiers/Common/Profile/default-loop-profile.jpg&#39;" src="https://loop.frontiersin.org/images/profile/626150/24"></img>
                                        <h6 class="author-link-aside">Ranilson O. Paiva</h6>
                                    </a>
                                    <div class="clearfix"></div>
                                    <p><span class="notes">Federal University of Alagoas, Brazil</span></p>
                                    <a class="authors" href="https://loop.frontiersin.org/people/632655/overview" target="_blank">
                                        <img alt=" " class="pr5 pull-left" onerror="this.src = &#39;/Areas/Articles/Images/Frontiers/Common/Profile/default-loop-profile.jpg&#39;" src="https://loop.frontiersin.org/images/profile/632655/24"></img>
                                        <h6 class="author-link-aside">Cesar Collazos</h6>
                                    </a>
                                    <div class="clearfix"></div>
                                    <p><span class="notes">University of Cauca, Colombia</span></p>
                                            </section>
            </div>

        <nav>
                <header><h5 class="like-h4" style="padding-top: 14px; padding-left: 6px; margin-bottom: 6px;">TABLE OF CONTENTS</h5></header>
                <ul class="nav nav-list list-unstyled contents" data-event="toc-link-click">
                    <li>
<ul class="flyoutJournal"><li><a href="#h1">Abstract</a></li><li><a href="#h2">Introduction: Artificial and Human Intelligence, Worlds of Difference</a></li><li><a href="#h3">We Are Probably Not so Smart as We Think</a></li><li><a href="#h4">General Intelligence Is Not the Same as Human-like Intelligence</a></li><li><a href="#h5">The Impact of Multiple Narrow AI Technology</a></li><li><a href="#h6">Conclusions and Framework</a></li><li><a href="#h7">Author Contributions</a></li><li><a href="#h8">Conflict of Interest</a></li><li><a href="#h9">Acknowledgments</a></li><li><a href="#h10">Footnotes</a></li><li><a href="#h11">References</a></li></ul>                    </li>
                </ul>
        </nav>
    </aside>


        <div class="clearfix"></div>

            <div class="side-article-download side-export clearfix">
                <ul class="list-unstyled list-inline clearfix">
                                            <li class="dropdown text-center citation">
                            <button data-target="#" data-test-id="citation-button" data-toggle="dropdown" role="button" aria-expanded="false">
                                <span class="icon-label" data-event="citation-button-click">Export citation</span>
                            </button>
                            <ul class="dropdown-menu" role="menu">
                                    <li>
                                        <a data-test-id="article-endnote" id="export_citation" href="/articles/10.3389/frai.2021.622364/endNote" data-event="endnotedownload-button-click">
                                            EndNote
                                        </a>
                                    </li>
                                    <li>
                                        <a data-test-id="article-referencemanager" id="export_citation" href="/articles/10.3389/frai.2021.622364/reference" data-event="referencemanagerdownload-button-click">
                                            Reference Manager
                                        </a>
                                    </li>
                                    <li>
                                        <a data-test-id="article-simpletextfile" id="export_citation" href="/articles/10.3389/frai.2021.622364/text" data-event="simpletextfiledownload-button-click">
                                            Simple TEXT file
                                        </a>
                                    </li>
                                    <li>
                                        <a data-test-id="article-bibtex" id="export_citation" href="/articles/10.3389/frai.2021.622364/bibTex" data-event="bibtexdownload-button-click">
                                            BibTex
                                        </a>
                                    </li>
                            </ul>
                        </li>
                </ul>
            </div>

            <div class="side-crossmark">
                <script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"></script>
                <a data-target="crossmark" class="crossmark">
                    <figure>
                        <img id="crossmark-icon" style="border: 0; width:64px; height:64px;" src="https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_BW_square_no_text.svg" title="" alt="">
                    </figure>
                    <div id="crossmark-icon">Check for updates</div>
                </a>

            </div>

            <article class="widget-listing people-also-looked-at side-article-related hidden">

                <h5 class="like-h4" data-event="peoplelookedat-link-click">People also looked at</h5>
            </article>


    </aside>

                
<main class="">
    <div class="article-section">
        <div class="article-container" data-html="True">
            <div class="abstract-container">
                <div class="article-header-container">
                    <!-- Start CrossMark Snippet v2.0 -->
                   
                    <!-- End CrossMark Snippet -->

                        <div class="header-bar-one">
                            <h2>
                                CONCEPTUAL ANALYSIS article
                            </h2>
                        </div>
                    <div class="header-bar-three-container">
                        <div class="header-bar-three">
                            Front. Artif. Intell., 25 March 2021<br>Sec. AI for Human Learning and Behavior Change

                            <br />
                                <span class="volumeInfo"> Volume 4 - 2021 | </span>



                                    <a href="https://doi.org/10.3389/frai.2021.622364">https://doi.org/10.3389/frai.2021.622364</a>
                        </div>
                    </div>
                </div>

<div class="JournalAbstract"><a id="h1" name="h1"></a><h1>Human- versus Artificial Intelligence</h1><div class="authors"><a href="https://www.frontiersin.org/people/u/228168" class="user-id-228168"><img class="pr5" src="https://loop.frontiersin.org/images/profile/228168/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'" alt="www.frontiersin.org">J. E. (Hans). Korteling</a>&#x0002a;  <a href="https://www.frontiersin.org/people/u/1255808" class="user-id-1255808"><img class="pr5" src="https://loop.frontiersin.org/images/profile/1255808/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'" alt="www.frontiersin.org">G. C. van de Boer-Visschedijk</a>  <a href="https://www.frontiersin.org/people/u/1255488" class="user-id-1255488"><img class="pr5" src="https://loop.frontiersin.org/images/profile/1255488/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'" alt="www.frontiersin.org">R. A. M. Blankendaal</a>  <a href="https://www.frontiersin.org/people/u/1254486" class="user-id-1254486"><img class="pr5" src="https://loop.frontiersin.org/images/profile/1254486/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'" alt="www.frontiersin.org">R. C. Boonekamp</a> <a href="https://www.frontiersin.org/people/u/1255931" class="user-id-1255931"><img class="pr5" src="https://loop.frontiersin.org/images/profile/1255931/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'" alt="www.frontiersin.org">A. R. Eikelboom</a></div><ul class="notes"><li>TNO Human Factors, Soesterberg, Netherlands</li></ul><p class="mb15">AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human- and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and &#x0201c;collaborate&#x0201d; with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human- and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa)? Should we pursue the development of AI &#x0201c;partners&#x0201d; with human (-level) intelligence or should we focus more at supplementing human limitations? In order to answer these questions, humans working with AI systems in the workplace or in policy making have to develop an adequate mental model of the underlying &#x02018;psychological&#x02019; mechanisms of AI. So, in order to obtain well-functioning human-AI systems, <em>Intelligence Awareness</em> in humans should be addressed more vigorously. For this purpose a first framework for educational content is proposed.</p><div class="clear"></div></div><div class="JournalFullText"><a id="h2" name="h2"></a><h2>Introduction: Artificial and Human Intelligence, Worlds of Difference</h2><h3 class="pt0">Artificial General Intelligence at the Human Level</h3><p class="mb0">Recent advances in information technology and in AI may allow for more coordination and integration between of humans and technology. Therefore, quite some attention has been devoted to the development of <em>Human-Aware</em> AI, which aims at AI that adapts as a &#x0201c;team member&#x0201d; to the cognitive possibilities and limitations of the human team members. Also metaphors like &#x0201c;mate,&#x0201d; &#x0201c;partner,&#x0201d; &#x0201c;alter ego,&#x0201d; &#x0201c;Intelligent Collaborator,&#x0201d; &#x0201c;buddy&#x0201d; and &#x0201c;mutual understanding&#x0201d; emphasize a high degree of collaboration, similarity, and equality in &#x0201c;hybrid teams&#x0201d;. When human-aware AI partners operate like &#x0201c;human collaborators&#x0201d; they must be able to sense, understand, and react to a wide range of complex human behavioral qualities, like attention, motivation, emotion, creativity, planning, or argumentation, (e.g. <a href="#B57">Kr&#x000e4;mer et al., 2012</a>; <a href="#B105">van den Bosch and Bronkhorst, 2018</a>; <a href="#B106">van den Bosch et al., 2019</a>). Therefore these &#x0201c;AI partners,&#x0201d; or &#x0201c;team mates&#x0201d; have to be endowed with human-like (or humanoid) cognitive abilities enabling mutual understanding and collaboration (i.e. &#x0201c;human awareness&#x0201d;).</p><p class="mb0">However, no matter how intelligent and autonomous AI agents become in certain respects, at least for the foreseeable future, they probably will remain unconscious machines or special-purpose devices that support humans in specific, complex tasks. As digital machines they are equipped with a completely different operating system (digital vs biological) and with correspondingly different cognitive qualities and abilities than biological creatures, like humans and other animals (<a href="#B65">Moravec, 1988</a>; <a href="#B50">Klein et al., 2004</a>; <a href="#B53">Korteling et al., 2018a</a>; <a href="#B85">Shneiderman, 2020a</a>). In general, digital reasoning- and problem-solving agents only compare very superficially to their biological counterparts, (e.g. <a href="#B10">Boden, 2017</a>; <a href="#B86">Shneiderman, 2020b</a>). Keeping that in mind, it becomes more and more important that human professionals working with advanced AI systems, (e.g. in military&#x02010; or policy making teams) develop a proper mental model about the different cognitive capacities of AI systems in relation to human cognition. This issue will become increasingly relevant when AI systems become more advanced and are deployed with higher degrees of autonomy. Therefore, the present paper tries to provide some more clarity and insight into the fundamental characteristics, differences and idiosyncrasies of human/biological and artificial/digital intelligences. In the final section, a global framework for constructing educational content on this &#x0201c;Intelligence Awareness&#x0201d; is introduced. This can be used for the development of education and training programs for humans who have to use or &#x0201c;collaborate with&#x0201d; advanced AI systems in the near and far future.</p><p class="mb0">With the application of AI systems with increasing autonomy more and more researchers consider the necessity of vigorously addressing the real complex issues of &#x0201c;human-level intelligence&#x0201d; and more broadly <em>artificial general intelligence</em>, or AGI, (e.g. <a href="#B23">Goertzel et al., 2014</a>). Many different definitions of A(G)I have already been proposed, (e.g. <a href="#B82">Russell and Norvig, 2014</a> for an overview). Many of them boil down to: <em>technology containing or entailing (human-like) intelligence</em>, (e.g. <a href="#B59">Kurzweil, 1990</a>). This is problematic. Most definitions use the term &#x0201c;intelligence&#x0201d;, as an essential element of the definition itself, which makes the definition tautological. Second, the idea that A(G)I should be <em>human-like</em> seems unwarranted. At least in natural environments there are many other forms and manifestations of highly complex and intelligent behaviors that are very different from specific <em>human</em> cognitive abilities (see <a href="#B33">Grind, 1997</a> for an overview). Finally, like what is also frequently seen in the field of biology, these A(G)I definitions use <em>human</em> intelligence as a central basis or analogy for reasoning about the&#x02014;less familiar&#x02014;phenomenon of A(G)I (<a href="#B19">Coley and Tanner, 2012</a>). Because of the many differences between the underlying substrate and architecture of biological and artificial intelligence this <em>anthropocentric</em> way of reasoning is probably unwarranted. For these reasons we propose a (non-anthropocentric) definition of &#x0201c;intelligence&#x0201d; as: &#x0201c;<em>the capacity to realize complex goals</em>&#x0201d; (<a href="#B92">Tegmark, 2017</a>). These goals may pertain to narrow, restricted tasks (narrow AI) or to broad task domains (AGI). Building on this definition, and on a definition of AGI proposed by <a href="#B9">Bieger et al. (2014)</a> and one of <a href="#B33">Grind (1997)</a>, we define AGI here as: &#x0201c;<em>Non-biological capacities to autonomously and efficiently achieve complex goals in a wide range of environments&#x0201d;.</em> AGI systems should be able to identify and extract the most important features for their operation and learning process automatically and efficiently over a broad range of tasks and contexts. Relevant AGI research differs from the ordinary AI research by addressing the versatility and wholeness of intelligence, and by carrying out the engineering practice according to a system comparable to the human mind in a certain sense (<a href="#B9">Bieger et al., 2014</a>).</p><p class="mb0">It will be fascinating to create copies of ourselves which can learn iteratively by interaction with partners and thus become able to collaborate on the basis of common goals and mutual understanding and adaptation, (e.g.<a href="#B15">Bradshaw et al., 2012</a>; <a href="#B43">Johnson et al., 2014</a>). This would be very useful, for example when a high degree of social intelligence of AI will contribute to more adequate interactions with humans, for example in health care or for entertainment purposes (<a href="#B103">Wyrobek et al., 2008</a>). True collaboration on the basis of common goals and mutual understanding necessarily implies some form of humanoid general intelligence. For the time being, this remains a goal on a far-off horizon. In the present paper we argue why for most applications it also may not be very practical or necessary (and probably a bit misleading) to vigorously aim or to anticipate on systems possessing &#x0201c;human-like&#x0201d; AGI or &#x0201c;human-like&#x0201d; abilities or qualities. The fact that humans possess general intelligence does not imply that new inorganic forms of general intelligence should comply to the criteria of human intelligence. In this connection, the present paper addresses the way we think about (natural and artificial) intelligence in relation to the most probable potentials (and real upcoming issues) of AI in the short- and mid-term future. This will provide food for thought in anticipation of a future that is difficult to predict for a field as dynamic as AI.</p><h3 class="pt0">What Is &#x0201c;Real Intelligence&#x0201d;?</h3><p class="mb0">Implicit in our aspiration of constructing AGI systems possessing humanoid intelligence is the premise that human (general) intelligence is the &#x0201c;real&#x0201d; form of intelligence. This is even already implicitly articulated in the term &#x0201c;Artificial Intelligence&#x0201d;, as if it were not entirely real, i.e., real like non-artificial (biological) intelligence. Indeed, as humans we know ourselves as the entities with the highest intelligence ever observed in the Universe. And as an extension of this, we like to see ourselves as rational beings who are able to solve a wide range of complex problems under all kinds of circumstances using our experience and intuition, supplemented by the rules of logic, decision analysis and statistics. It is therefore not surprising that we have some difficulty to accept the idea that we might be a bit less smart than we keep on telling ourselves, i.e., &#x0201c;the next insult for humanity&#x0201d; (<a href="#B6">van Belkom, 2019</a>). This goes as far that the rapid progress in the field of artificial intelligence is accompanied by a recurring redefinition of what should be considered &#x0201c;real (general) intelligence.&#x0201d; The conceptualization of intelligence, that is, the ability to autonomously and efficiently achieve complex goals, is then continuously adjusted and further restricted to: &#x0201c;those things that only humans can do.&#x0201d; In line with this, AI is then defined as &#x0201c;the study of how to make computers do things at which, at the moment, people are better&#x0201d; (<a href="#B76">Rich and Knight, 1991</a>; <a href="#B77">Rich et al., 2009</a>). This includes thinking of creative solutions, flexibly using contextual- and background information, the use of intuition and feeling, the ability to really &#x0201c;think and understand,&#x0201d; or the inclusion of emotion in an (ethical) consideration. These are then cited as the specific elements of <em>real</em> intelligence, (e.g. <a href="#B8">Bergstein, 2017</a>). For instance, Facebook&#x02019;s director of AI and a spokesman in the field, Yann LeCun, mentioned at a <em>Conference at MIT on the Future of Work</em> that machines are still far from having &#x0201c;the essence of intelligence.&#x0201d; That includes the ability to understand the physical world well enough to make predictions about basic aspects of it&#x02014;to observe one thing and then use background knowledge to figure out what other things must also be true. Another way of saying this is that machines don&#x02019;t have <em>common sense</em> (<a href="#B8">Bergstein, 2017</a>), like submarines that cannot swim (<a href="#B6">van Belkom, 2019</a>). When exclusive human capacities become our pivotal navigation points on the horizon we may miss some significant problems that may need our attention first.</p><p class="mb0">To make this point clear, we first will provide some insight into the basic nature of both human and artificial intelligence. This is necessary for the substantiation of an adequate awareness of intelligence (<em>Intelligence Awareness</em>), and adequate research and education anticipating the development and application of A(G)I. For the time being, this is based on three essential notions that can (and should) be further elaborated in the near future.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02022; With regard to cognitive tasks, we are probably less smart than we think. So why should we vigorously focus on <em>human</em>-like AGI?</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02022; Many different forms of intelligence are possible and general intelligence is therefore not necessarily the same as <em>humanoid</em> general intelligence (or &#x0201c;AGI on human level&#x0201d;).</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02022; AGI is often not necessary; many complex problems can also be tackled effectively using multiple narrow AI&#x02019;s.<a href="#fn1"><sup>1</sup></a></p><a id="h3" name="h3"></a><h2>We Are Probably Not so Smart as We Think</h2><p class="mb15">How intelligent are we actually? The answer to that question is determined to a large extent by the perspective from which this issue is viewed, and thus by the measures and criteria for intelligence that is chosen. For example, we could compare the nature and capacities of human intelligence with other animal species. In that case we appear highly intelligent. Thanks to our enormous learning capacity, we have by far the most extensive arsenal of cognitive abilities<a href="#fn2"><sup>2</sup></a> to autonomously solve complex problems and achieve complex objectives. This way we can solve a huge variety of arithmetic, conceptual, spatial, economic, socio-organizational, political, etc. problems. The primates&#x02014;which differ only slightly from us in genetic terms&#x02014;are far behind us in that respect. We can therefore legitimately qualify humans, as compared to other animal species that we know, as highly intelligent.</p><h3 class="pt0">Limited Cognitive Capacity</h3><p class="mb0">However, we can also look beyond this &#x0201c;<em>relative</em> interspecies perspective&#x0201d; and try to qualify our intelligence in more <em>absolute</em> terms, i.e., using a scale ranging from zero to what is physically possible. For example, we could view the computational capacity of a human brain as a physical system (<a href="#B13">Bostrom, 2014</a>; <a href="#B92">Tegmark, 2017</a>). The prevailing notion in this respect among AI scientists is that intelligence is ultimately a matter of information and computation, and (thus) not of flesh and blood and carbon atoms. In principle, there is no physical law preventing that physical systems (consisting of quarks and atoms, like our brain) can be built with a much greater computing power and intelligence than the human brain. This would imply that there is no insurmountable physical reason why machines one day cannot become much more intelligent than ourselves in all possible respects (<a href="#B92">Tegmark, 2017</a>). Our intelligence is therefore <em>relatively</em> high compared to other animals, but in absolute terms it may be very limited in its physical computing capacity, albeit only by the limited size of our brain and its maximal possible number of neurons and glia cells, (e.g. <a href="#B44">Kahle, 1979</a>).</p><p class="mb0">To further define and assess our own (biological) intelligence, we can also discuss the evolution and nature of our biological thinking abilities. As a biological neural network of flesh and blood, necessary for survival, our brain has undergone an evolutionary optimization process of more than a billion years. In this extended period, it developed into a highly effective and efficient system for regulating essential biological functions and performing perceptive-motor and pattern-recognition tasks, such as gathering food, fighting and flighting, and mating. Almost during our entire evolution, the neural networks of our brain have been further optimized for these basic biological and perceptual motor processes that also lie at the basis of our daily practical skills, like cooking, gardening, or household jobs. Possibly because of the resulting proficiency for these kinds of tasks we may forget that these processes are characterized by extremely high <em>computational</em> complexity, (e.g. <a href="#B65">Moravec, 1988</a>). For example, when we tie our shoelaces, many millions of signals flow in and out through a large number of different sensor systems, from tendon bodies and muscle spindles in our extremities to our retina, otolithic organs and semi-circular channels in the head, (e.g. <a href="#B16">Brodal, 1981</a>). This enormous amount of information from many different perceptual-motor systems is continuously, parallel, effortless and even without conscious attention, processed in the neural networks of our brain (<a href="#B64">Minsky, 1986</a>; <a href="#B65">Moravec, 1988</a>; <a href="#B33">Grind, 1997</a>). In order to achieve this, the brain has a number of universal (inherent) working mechanisms, such as association and associative learning (<a href="#B84">Shatz, 1992</a>; <a href="#B4">Bar, 2007</a>), potentiation and facilitation (<a href="#B48">Katz and Miledi, 1968</a>; <a href="#B3">Bao et al., 1997</a>), saturation and lateral inhibition (<a href="#B42">Isaacson and Scanziani, 2011</a>; <a href="#B53">Korteling et al., 2018a</a>).</p><p class="mb0">These kinds of basic biological and perceptual-motor capacities have been developed and set down over many millions of years. Much later in our evolution&#x02014;actually only very recently&#x02014;our cognitive abilities and rational functions have started to develop. These cognitive abilities, or capacities, are probably less than 100 thousand years old, which may be qualified as &#x0201c;embryonal&#x0201d; on the time scale of evolution, (e.g. <a href="#B73">Petraglia and Korisettar, 1998</a>; <a href="#B62">McBrearty and Brooks, 2000</a>; <a href="#B38">Henshilwood and Marean, 2003</a>). In addition, this very thin layer of human achievement has necessarily been built on these &#x0201c;ancient&#x0201d; neural intelligence for essential survival functions. So, our &#x0201c;higher&#x0201d; cognitive capacities are developed <em>from</em> and <em>with</em> these (neuro) biological regulation mechanisms (<a href="#B21">Damasio, 1994</a>; <a href="#B52">Korteling and Toet, 2020</a>). As a result, it should not be a surprise that the capacities of our brain for performing these recent cognitive functions are still rather limited. These limitations are manifested in many different ways, for instance:</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;The amount of cognitive information that we can consciously process (our working memory, span or attention) is very limited (<a href="#B89">Simon, 1955</a>). The capacity of our working memory is approximately 10&#x02013;50 bits per second (<a href="#B92">Tegmark, 2017</a>).</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Most cognitive tasks, like reading text or calculation, require our full attention and we usually need a lot of time to execute them. Mobile calculators can perform millions times more complex calculations than we can (<a href="#B92">Tegmark, 2017</a>).</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Although we can process lots of information in parallel, we cannot simultaneously execute cognitive tasks that require deliberation and attention, i.e., &#x0201c;multi-tasking&#x0201d; (<a href="#B51">Korteling, 1994</a>; <a href="#B80">Rogers and Monsell, 1995</a>; <a href="#B81">Rubinstein, Meyer, and Evans, 2001</a>).</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Acquired cognitive knowledge and skills of people (memory) tend to decay over time, much more than perceptual-motor skills. Because of this limited &#x0201c;retention&#x0201d; of information we easily forget substantial portions of what we have learned (<a href="#B101">Wingfield and Byrnes, 1981</a>).</p><h3 class="pt0">Ingrained Cognitive Biases</h3><p class="mb0">Our limited processing capacity for cognitive tasks is not the only factor determining our cognitive intelligence. Except for an overall limited processing capacity, human cognitive information processing shows systematic distortions. These are manifested in many cognitive biases (<a href="#B96">Tversky and Kahneman, 1973</a>, <a href="#B94">Tversky and Kahneman, 1974</a>). Cognitive biases are systematic, universally occurring tendencies, inclinations, or dispositions that skew or distort information processes in ways that make their outcome inaccurate, suboptimal, or simply wrong, (e.g. <a href="#B61">Lichtenstein and Slovic, 1971</a>; <a href="#B95">Tversky and Kahneman, 1981</a>). Many biases occur in virtually the same way in many different decision situations (<a href="#B83">Shafir and LeBoeuf, 2002</a>; <a href="#B47">Kahneman, 2011</a>; <a href="#B104">Toet et al., 2016</a>). The literature provides descriptions and demonstrations of over 200 biases. These tendencies are largely implicit and unconscious and feel quite naturally and self/evident when we are aware of these cognitive inclinations (<a href="#B75">Pronin et al., 2002</a>; <a href="#B78">Risen, 2015</a>; <a href="#B54">Korteling et al., 2018b</a>). That is why they are often termed &#x0201c;intuitive&#x0201d; (<a href="#B45">Kahneman and Klein, 2009</a>) or &#x0201c;irrational&#x0201d; (<a href="#B83">Shafir and LeBoeuf, 2002</a>). Biased reasoning can result in quite acceptable outcomes in natural or everyday situations, especially when the time cost of reasoning is taken into account (<a href="#B89">Simon, 1955</a>; <a href="#B31">Gigerenzer and Gaissmaier, 2011</a>). However, people often deviate from rationality and/or the tenets of logic, calculation, and probability in inadvisable ways (<a href="#B94">Tversky and Kahneman, 1974</a>; <a href="#B83">Shafir and LeBoeuf, 2002</a>) leading to suboptimal decisions in terms of invested time and effort (costs) given the available information and expected benefits.</p><p class="mb0">Biases are largely caused by <em>inherent</em> (or structural) characteristics and mechanisms of the brain as a neural network (<a href="#B53">Korteling et al., 2018a</a>; <a href="#B52">Korteling and Toet, 2020</a>). Basically, these mechanisms&#x02014;such as association, facilitation, adaptation, or lateral inhibition&#x02014;result in a modification of the original or available data and its processing, (e.g. weighting its importance). For instance, lateral inhibition is a universal neural process resulting in the magnification of differences in neural activity (contrast enhancement), which is very useful for perceptual-motor functions, maintaining physical integrity and allostasis, (i.e. biological survival functions). For these functions our nervous system has been optimized for millions of years. However, &#x0201c;higher&#x0201d; cognitive functions, like conceptual thinking, probability reasoning or calculation, have been developed only very recently in evolution. These functions are probably less than 100 thousand years old, and may, therefore, be qualified as &#x0201c;embryonal&#x0201d; on the time scale of evolution, (e.g. <a href="#B62">McBrearty and Brooks, 2000</a>; <a href="#B38">Henshilwood and Marean, 2003</a>; <a href="#B73">Petraglia and Korisettar, 2003</a>). In addition, evolution could not develop these new cognitive functions from scratch, but instead had to build this embryonal, and thin layer of human achievement from its &#x0201c;ancient&#x0201d; neural heritage for the essential biological survival functions (<a href="#B65">Moravec, 1988</a>). Since cognitive functions typically require exact calculation and proper weighting of data, data transformations&#x02014;like lateral inhibition&#x02014;may easily lead to systematic distortions, (i.e. biases) in cognitive information processing. Examples of the large number of biases caused by the inherent properties of biological neural networks are: Anchoring bias (biasing decisions toward previously acquired information, <a href="#B27">Furnham and Boo, 2011</a>; <a href="#B96">Tversky and Kahneman, 1973</a>, <a href="#B94">Tversky and Kahneman, 1974</a>), the Hindsight bias (the tendency to erroneously perceive events as inevitable or more likely once they have occurred, <a href="#B40">Hoffrage et al., 2000</a>; <a href="#B79">Roese and Vohs, 2012</a>) the Availability bias (judging the frequency, importance, or likelihood of an event by the ease with which relevant instances come to mind, <a href="#B96">Tversky and Kahnemann, 1973</a>; <a href="#B94">Tversky and Kahneman, 1974</a>), and the Confirmation bias (the tendency to select, interpret, and remember information in a way that confirms one&#x02019;s preconceptions, views, and expectations, <a href="#B68">Nickerson, 1998</a>). In addition to these inherent (structural) limitations of (biological) neural networks, biases may also originate from functional evolutionary principles promoting the survival of our ancestors who, as hunter-gatherers, lived in small, close-knit groups (<a href="#B37">Haselton et al., 2005</a>; <a href="#B93">Tooby and Cosmides, 2005</a>). Cognitive biases can be caused by a mismatch between evolutionarily rationalized &#x0201c;heuristics&#x0201d; (&#x0201c;evolutionary rationality&#x0201d;: <a href="#B36">Haselton et al., 2009</a>) and the current context or environment (<a href="#B93">Tooby and Cosmides, 2005</a>). In this view, the same heuristics that optimized the chances of survival of our ancestors in their (natural) environment can lead to maladaptive (biased) behavior when they are used in our current (artificial) settings. Biases that have been considered as examples of this kind of mismatch are the Action bias (preferring action even when there is no rational justification to do this, <a href="#B5">Baron and Ritov, 2004</a>; <a href="#B71">Patt and Zeckhauser, 2000</a>), Social proof (the tendency to mirror or copy the actions and opinions of others, <a href="#B18">Cialdini, 1984</a>), the Tragedy of the commons (prioritizing personal interests over the common good of the community, <a href="#B34">Hardin, 1968</a>), and the Ingroup bias (favoring one&#x02019;s own group above that of others, <a href="#B91">Taylor and Doria, 1981</a>).</p><p class="mb0">This hard-wired (neurally inherent and/or evolutionary ingrained) character of biased thinking makes it unlikely that simple and straightforward methods like training interventions or awareness courses will be very effective to ameliorate biases. This difficulty of bias mitigation seems indeed supported by the literature (<a href="#B55">Korteling et al., 2021</a>).</p><a id="h4" name="h4"></a><h2>General Intelligence Is Not the Same as Human-like Intelligence</h2><h3 class="pt0">Fundamental Differences Between Biological and Artificial Intelligence</h3><p class="mb0">We often think and deliberate about intelligence with an anthropocentric conception of our own intelligence in mind as an obvious and unambiguous reference. We tend to use this conception as a basis for reasoning about other, less familiar phenomena of intelligence, such as other forms of biological and artificial intelligence (<a href="#B19">Coley and Tanner, 2012</a>). This may lead to fascinating questions and ideas. An example is the discussion about how and when the point of &#x0201c;intelligence at human level&#x0201d; will be achieved. For instance, <a href="#B1">Ackermann. (2018)</a> writes: &#x0201c;Before reaching superintelligence, general AI means that a machine will have the same cognitive capabilities as a human being&#x0201d;. So, researchers deliberate extensively about the point in time when we will reach general AI, (e.g., <a href="#B32">Goertzel, 2007</a>; <a href="#B67">M&#x000fc;ller and Bostrom, 2016</a>). We suppose that these kinds of questions are not quite on target. There are (in principle) many different possible types of (general) intelligence conceivable of which human-like intelligence is just one of those. This means, for example that the development of AI is determined by the constraint of physics and technology, and not by those of biological evolution. So, just as the intelligence of a hypothetical extraterrestrial visitor of our planet earth is likely to have a different (in-)organic structure with different characteristics, strengths, and weaknesses, than the human residents this will also apply to artificial forms of (general) intelligence. Below we briefly summarize a few fundamental differences between human and artificial intelligence (<a href="#B13">Bostrom, 2014</a>):</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Basic structure: Biological (carbon) intelligence is based on neural &#x0201c;wetware&#x0201d; which is fundamentally different from artificial (silicon-based) intelligence. As opposed to biological wetware, in silicon, or digital, systems &#x0201c;hardware&#x0201d; and &#x0201c;software&#x0201d; are independent of each other (<a href="#B56">Kosslyn and Koenig, 1992</a>). When a biological system has learned a new skill, this will be bounded to the system itself. In contrast, if an AI system has learned a certain skill then the constituting algorithms can be directly copied to all other similar digital systems.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Speed: Signals from AI systems propagate with almost the speed of light. In humans, the conduction velocity of nerves proceeds with a speed of at most 120&#x000a0;m/s, which is extremely slow in the time scale of computers (<a href="#B87">Siegel and Sapru, 2005</a>).</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Connectivity and communication: People cannot directly communicate with each other. They communicate via language and gestures with limited bandwidth. This is slower and more difficult than the communication of AI systems that can be connected directly to each other. Thanks to this direct connection, they can also collaborate on the basis of integrated algorithms.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Updatability and scalability: AI systems have almost no constraints with regard to keep them up to date or to upscale and/or re-configure them, so that they have the right algorithms and the data processing and storage capacities necessary for the tasks they have to carry out. This capacity for rapid, structural expansion and immediate improvement hardly applies to people.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;In contrast, biology does a lot with a little: organic brains are millions of times more efficient in energy consumption than computers. The human brain consumes less energy than a lightbulb, whereas a supercomputer with comparable computational performance uses enough electricity to power quite a village (<a href="#B26">Fischetti, 2011</a>).</p><p class="mb0">These kinds of differences in basic structure, speed, connectivity, updatability, scalability, and energy consumption will necessarily also lead to different qualities and limitations between human and artificial intelligence. Our response speed to simple stimuli is, for example, many thousands of times slower than that of artificial systems. Computer systems can very easily be connected directly to each other and as such can be part of one integrated system. This means that AI systems do not have to be seen as individual entities that can easily work alongside each other or have mutual misunderstandings. And if two AI systems are engaged in a task then they run a minimal risk to make a mistake because of miscommunications (think of autonomous vehicles approaching a crossroad). After all, they are intrinsically connected parts of the same system and the same algorithm (<a href="#B28">Gerla et al., 2014</a>).</p><h3 class="pt0">Complexity and Moravec&#x02019;s Paradox</h3><p class="mb0">Because biological, carbon-based, brains and digital, silicon-based, computers are optimized for completely different kinds of tasks (e.g., <a href="#B65">Moravec, 1988</a>; <a href="#B54">Korteling et al., 2018b</a>), human and artificial intelligence show fundamental and probably far-stretching differences. Because of these differences it may be very misleading to use our own mind as a basis, model or analogy for reasoning about AI. This may lead to erroneous conceptions, for example about the presumed abilities of humans and AI to perform complex tasks. Resulting flaws concerning information processing capacities emerge often in the psychological literature in which &#x0201c;complexity&#x0201d; and &#x0201c;difficulty&#x0201d; of tasks are used interchangeably (see for examples: <a href="#B102">Wood et al., 1987</a>; <a href="#B14">McDowd and Craik, 1988</a>). Task complexity is then assessed in an anthropocentric way, that is: by the degree to which we humans can perform or master it. So, we use the <em>difficulty</em> to perform or master a task as a measure of its <em>complexity</em>, and task performance (speed, errors) as a measure of skill and intelligence of the task performer. Although this could sometimes be acceptable in psychological research, this may be misleading if we strive for understanding the intelligence of AI systems. For us it is much more difficult to multiply two random numbers of six digits than to recognize a friend on a photograph. But when it comes to counting or arithmetic operations, computers are thousands of times faster and better, while the same systems have only recently taken steps in image recognition (which only succeeded when deep learning technology, based on some principles of biological neural networks, was developed). In general: cognitive tasks that are relatively difficult for the human brain (and which we therefore find subjectively difficult) do not have to be computationally complex, (e.g., in terms of objective arithmetic, logic, and abstract operations). And vice versa: tasks that are relatively easy for the brain (recognizing patterns, perceptual-motor tasks, well-trained tasks) do not have to be computationally simple. This phenomenon, that which is easy for the ancient, neural &#x0201c;technology&#x0201d; of people and difficult for the modern, digital technology of computers (and vice versa) has been termed the <em>moravec&#x02019;s Paradox.</em> Hans <a href="#B65">Moravec (1988)</a> wrote: &#x0201c;It is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility.&#x0201d;</p><h3 class="pt0">Human Superior Perceptual-Motor Intelligence</h3><p class="mb0">Moravec&#x02019;s paradox implies that biological neural networks are intelligent in different ways than artificial neural networks. Intelligence is not limited to the problems or goals that we as humans, equipped with biological intelligence, find difficult (<a href="#B33">Grind, 1997</a>). Intelligence, defined as the ability to realize complex goals or solve complex problems, is much more than that. According to <a href="#B65">Moravec (1988)</a> high-level reasoning requires very little computation, but low-level perceptual-motor skills require enormous computational resources. If we express the complexity of a problem in terms of the number of elementary calculations needed to solve it, then our biological perceptual motor intelligence is <em>highly superior</em> to our cognitive intelligence. Our organic perceptual-motor intelligence is especially good at associative processing of higher-order invariants (&#x0201c;patterns&#x0201d;) in the ambient information. These are computationally more complex and contain more information than the simple, individual elements (<a href="#B30">Gibson, 1966</a>, <a href="#B29">Gibson, 1979</a>). An example of our superior perceptual-motor abilities is the <em>Object Superiority Effect</em>: we perceive and interpret whole objects faster and more effective than the (more simple) individual elements that make up these objects (<a href="#B98">Weisstein and Harris, 1974</a>; <a href="#B63">McClelland, 1978</a>; <a href="#B100">Williams and Weisstein, 1978</a>; <a href="#B74">Pomerantz, 1981</a>). Thus, letters are also perceived more accurately when presented as part of a word than when presented in isolation, i.e. the Word superiority effect, (e.g. <a href="#B46">Reicher, 1969</a>; <a href="#B90">Wheeler, 1970</a>). So, the <em>difficulty</em> of a task does not necessarily indicate its inherent <em>complexity</em>. As <a href="#B65">Moravec (1988)</a> puts it: &#x0201c;We are all prodigious Olympians in perceptual and motor areas, so good that we make the difficult look easy. Abstract thought, though, is a new trick, perhaps less than 100 thousand years old. We have not yet mastered it. It is not all that intrinsically difficult; it just seems so when we do it.&#x0201d;</p><h3 class="pt0">The Supposition of Human-like AGI</h3><p class="mb0">So, if there would exist AI systems with general intelligence that can be used for a wide range of complex problems and objectives, those AGI machines would probably have a completely different intelligence profile, including other cognitive qualities, than humans have (<a href="#B32">Goertzel, 2007</a>). This will be even so, if we manage to construct AI agents who display similar behavior like us and if they are enabled to adapt to our way of thinking and problem-solving in order to promote human-AI teaming. Unless we decide to deliberately <em>degrade</em> the capabilities of AI systems (which would not be very smart), the underlying capacities and abilities of man and machines with regard to collection and processing of information, data analysis, probability reasoning, logic, memory capacity etc. will still remain dissimilar. Because of these differences we should focus at systems that effectively <em>complement</em> us, and that make the human-AI system stronger and more effective. Instead of pursuing human-level AI it would be more beneficial to focus on autonomous machines and (support) systems that fill in, or extend on, the manifold gaps of human cognitive intelligence. For instance, whereas people are forced&#x02014;by the slowness and other limitations of biological brains&#x02014;to think heuristically in terms of goals, virtues, rules and norms expressed in (fuzzy) language, AI has already established excellent capacities to process and calculate directly on highly complex data. Therefore, or the execution of specific (narrow) cognitive tasks (logical, analytical, computational), modern digital intelligence may be more effective and efficient than biological intelligence. AI may thus help to produce better answers for complex problems using high amounts of data, consistent sets of ethical principles and goals, probabilistic-, and logic reasoning, (e.g. <a href="#B54">Korteling et al., 2018b</a>). Therefore, we conjecture that ultimately the development of AI systems for supporting human decision making may appear the most effective way leading to the making of better choices or the development of better solutions on complex issues. So, the cooperation and division of tasks between people and AI systems will have to be primarily determinated by their mutually specific qualities. For example, tasks or task components that appeal to capacities in which AI systems excel, will have to be less (or less fully) mastered by people, so that less training will probably be required. AI systems are already much better than people at logically and arithmetically correct gathering (selecting) and processing (weighing, prioritizing, analyzing, combining) large amounts of data. They do this quickly, accurately and reliably. They are also more stable (consistent) than humans, have no stress and emotions and have a great perseverance and a much better retention of knowledge and skills than people. As a machine, they serve people completely and without any &#x0201c;self-interest&#x0201d; or &#x0201c;own hidden agenda.&#x0201d; Based on these qualities AI systems may effectively take over tasks, or task components, from people. However, it remains important that people continue to master those tasks to a certain extent, so that they can take over tasks or take adequate action if the machine system fails.</p><p class="mb0">In general, people are better suited than AI systems for a much broader spectrum of cognitive and social tasks under a wide variety of (unforeseen) circumstances and events (<a href="#B54">Korteling et al., 2018b</a>). People are also better at the social-psychosocial interaction for the time being. For example, it is difficult for AI systems to interpret human language and -symbolism. This requires a very extensive frame of reference, which, at least until now and for the near future, is difficult to achieve within AI. As a result of all these differences, people are still better at responding (as a flexible team) to unexpected and unpredictable situations and creatively devising possibilities and solutions in open and ill-defined tasks and across a wide range of different, and possibly unexpected, circumstances. People will have to make extra use of their specific human qualities, (i.e. what people are relatively good at) and train to improve relevant competencies. In addition, human team members will have to learn to deal well with the overall limitations of AIs. With such a proper division of tasks, capitalizing on the specific qualities and limitations of humans and AI systems, human decisional biases may be circumvented and better performance may be expected. This means that enhancement of a team with intelligent machines having less cognitive constraints and biases, may have more surplus value than striving at collaboration between humans and AI that have developed the same (human) biases. Although cooperation in teams with AI systems may need extra training in order to effectively deal with this bias-mismatch, this heterogeneity will probably be better and safer. This also opens up the possibility of a combination of high levels of meaningful human control AND high levels of automation which is likely to produce the most effective and safe human-AI systems (<a href="#B22">Elands et al., 2019</a>; <a href="#B85">Shneiderman, 2020a</a>). In brief: human intelligence is not the golden standard for general intelligence; instead of aiming at <em>human-like</em> AGI, the pursuit of AGI should thus focus on effective <em>digital/silicon AGI</em> in conjunction with an optimal configuration and allocation of tasks.</p><h3 class="pt0">Explainability and Trust</h3><p class="mb0">Developments in relation to artificial learning, or deep (reinforcement) learning, in particular have been revolutionary. Deep learning simulates a network resembling the layered neural networks of our brain. Based on large quantities of data, the network learns to recognize patterns and links to a high level of accuracy and then connect them to courses of action without knowing the underlying causal links. This implies that it is difficult to provide deep learning AI with some kind of transparency in how or why it has made a particular choice by, for example, by expressing an intelligible reasoning (for humans) about its decision process, like we do, (e.g. <a href="#B6">Belkom, 2019</a>). In addition, reasoning about decisions like humans do is a very malleable and ad hoc process (at least in humans). Humans are generally unaware of their implicit cognitions or attitudes, and therefore not be able to adequately report on them. It is therefore rather difficult for many humans to introspectively analyze their mental states, as far as these are conscious, and attach the results of this analysis to verbal labels and descriptions, (e.g. <a href="#B69">Nosek et al. (2011)</a>. First, the human brain hardly reveals how it creates conscious thoughts, (e.g. <a href="#B24">Feldman-Barret, 2017</a>). What it actually does is giving us the illusion that its products reveal its inner workings. In other words: our conscious thoughts tell us nothing about the way in which these thoughts came about. There is also no subjective marker that distinguishes correct reasoning processes from erroneous ones (<a href="#B45">Kahneman and Klein, 2009</a>). The decision maker therefore has no way to distinguish between correct thoughts, emanating from genuine knowledge and expertize, and incorrect ones following from inappropriate neuro-evolutionary processes, tendencies, and primal intuitions. So here we could ask the question: isn&#x02019;t it more trustworthy to have a real black box, than to listen to a confabulating one? In addition, according to <a href="#B99">Werkhoven et al. (2018)</a> demanding explainability observability, or transparency (<a href="#B6">Belkom, 2019</a>; <a href="#B106">van den Bosch et al., 2019</a>) may cause artificial intelligent systems to constrain their potential benefit for human society, to what can be understood by humans.</p><p class="mb0">Of course we should not blindly trust the results generated by AI. Like other fields of complex technology, (e.g. Modeling &#x00026; Simulation), AI systems need to be verified (meeting specifications) and validated (meeting the systems&#x02019; goals) with regard to the objectives for which the system was designed. In general, when a system is properly verified and validated, it may be considered safe, secure and fit for purpose. It therefore deserves our trust for (logically) comprehensible and objective reasons (although mistakes still can happen). Likewise people trust in the performance of aero planes and cell phones despite we are almost completely ignorant about their complex inner processes. Like our own brains, artificial neural networks are fundamentally intransparant (<a href="#B69">Nosek et al., 2011</a>; <a href="#B24">Feldman-Barret, 2017</a>). Therefore, trust in AI should be primarily based on its objective performance. This forms a more important base than providing trust on the basis of subjective (trickable) impressions, stories, or images aimed at belief and appeal to the user. Based on empirical validation research, developers and users can explicitly verify how well the system is doing with respect to the set of values and goals for which the machine was designed. At some point, humans may want to trust that goals can be achieved against less cost and better outcomes, when we accept solutions even if they may be less transparent for humans (<a href="#B99">Werkhoven et al., 2018</a>).</p><a id="h5" name="h5"></a><h2>The Impact of Multiple Narrow AI Technology</h2><h3 class="pt0">AGI as the Holy Grail</h3><p class="mb0">AGI, like human general intelligence, would have many obvious advantages, compared to narrow (limited, weak, specialized) AI. An AGI system would be much more flexible and adaptive. On the basis of generic training and reasoning processes it would understand autonomously how multiple problems in all kinds of different domains can be solved in relation to their context, (e.g. <a href="#B58">Kurzweil, 2005</a>). AGI systems also require far fewer human interventions to accommodate the various loose ends among partial elements, facets, and perspectives in complex situations. AGI would really understand problems and is capable to view them from different perspectives (as people&#x02014;ideally&#x02014;also can do). A characteristic of the current (narrow) AI tools is that they are skilled in a very specific task, where they can often perform at superhuman levels, (e.g. <a href="#B32">Goertzel, 2007</a>; <a href="#B88">Silver et al., 2017</a>). These specific tasks have been well-defined and structured. Narrow AI systems are less suitable, or totally unsuitable, for tasks or task environments that offer little structure, consistency, rules or guidance, in which all sorts of unexpected, rare or uncommon events, (e.g. emergencies) may occur. Knowing and following fixed procedures usually does not lead to proper solutions in these varying circumstances. In the context of (unforeseen) changes in goals or circumstances, the adequacy of current AI is considerably reduced because it cannot reason from a general perspective and adapt accordingly (<a href="#B60">Lake et al., 2017</a>; <a href="#B41">Horowitz, 2018</a>). As with narrow AI systems, people are then needed to supervise on these deviations in order to enable flexible and adaptive system performance. Therefore the quest of AGI may be considered as looking for a kind of holy grail.</p><h3 class="pt0">Multiple Narrow AI is Most Relevant Now!</h3><p class="mb0">The potential high prospects of AGI, however, do not imply that AGI will be the most crucial factor in future AI R&#x00026;D, at least for the short- and mid-term. When reflecting on the great potential benefits of general intelligence, we tend to consider narrow AI applications as separate entities that can very well be outperformed by a broader AGI that presumably can deal with everything. But just as our modern world has evolved rapidly through a diversity of specific (limited) technological innovations, at the system level the total and wide range of emerging AI applications will also have a groundbreaking technological and societal impact (<a href="#B72">Peeters et al., 2020</a>). This will be all the more relevant for the future world of big data, in which everything is connected to everything through the <em>Internet of Things</em>. So, it will be much more profitable and beneficial to develop and build (non-human-like) AI variants that will excel in areas where people are inherently limited. It seems not too far-fetched to suppose that the multiple variants of narrow AI applications also gradually get more broadly interconnected. In this way, a development toward an ever broader realm of integrated AI applications may be expected. In addition, it is already possible to train a language model AI (Generative Pre-trained Transformer3, GPT-3) with a gigantic dataset and then have it learn various tasks based on a handful of examples&#x02014;one or few-shot learning. GPT-3 (developed by OpenAI) can do this with language-related tasks, but there is no reason why this should not be possible with image and sound, or with combinations of these three (<a href="#B17">Brown, 2020</a>).</p><p class="mb0">Besides, the moravec Paradox implies that the development of AI &#x0201c;partners&#x0201d; with many kinds of human (-level) qualities will be very difficult to obtain, whereas their added value, (i.e. beyond the boundaries of human capabilities) will be relatively low. The most fruitful AI applications will mainly involve supplementing human constraints and limitations. Given the present incentives for competitive technological progress, multiple forms of (connected) narrow AI systems will be the major driver of AI impact on our society for short- and mid-term. For the near future, this may imply that AI applications will remain very different from, and in many aspects almost incomparable with, human agents. This is likely to be true even if the hypothetical match of artificial general intelligence (AGI) with human cognition were to be achieved in the future in the longer term. Intelligence is a multi-dimensional (quantitative, qualitative) concept. All dimensions of AI unfold and grow along their own different path with their own dynamics. Therefore, over time an increasing number of specific (narrow) AI capacities may gradually match, overtake and transcend human cognitive capacities. Given the enormous advantages of AI, for example in the field of data availability and data processing capacities, the realization of AGI probably would at the same time outclass human intelligence in many ways. Which implies that the hypothetical point of time of matching human- and artificial cognitive capacities, i.e. human-level AGI, will probably be hard to define in a meaningful way (<a href="#B32">Goertzel, 2007</a>).<a href="#fn3"><sup>3</sup></a></p><p class="mb0">So when AI will truly understand us as a &#x0201c;friend,&#x0201d; &#x0201c;partner,&#x0201d; &#x0201c;alter ego&#x0201d; or &#x0201c;buddy,&#x0201d; as we do when we collaborate with other humans as humans, it will surpass us in many areas at the same <a href="#B66">Moravec (1998)</a> time. It will have a completely different profile of capacities and abilities and thus it will not be easy to really understand the way it &#x0201c;thinks&#x0201d; and comes to its decisions. In the meantime, however, as the capacities of robots expand and move from simple tools to more integrated systems, it is important to calibrate our expectations and perceptions toward robots appropriately. So, we will have to enhance our awareness and insight concerning the continuous development and progression of multiple forms of (integrated) AI systems. This concerns for example the multi-facetted nature of intelligence. Different kind of agents may have different combinations of intelligences of very different levels. An agent with general intelligence may for example be endowed with excellent abilities on the area of image recognition and navigation, calculation, and logical reasoning while at the same time being dull on the area of social interaction and goal-oriented problem solving. This awareness of the multi-dimensional nature of intelligence also concerns the way we have to deal with (<em>and</em> capitalize on) anthropomorphism. That is the human tendency in human-robot interaction to characterize non-human artifacts that superficially look similar to us as possessing human-like traits, emotions, and intentions, (e.g., <a href="#B49">Kiesler and Hinds, 2004</a>; <a href="#B25">Fink, 2012</a>; <a href="#B35">Haring et al., 2018</a>). Insight into these human factors issues is crucial to optimize the utility, performance and safety of human-AI systems (<a href="#B72">Peeters et al., 2020</a>).</p><p class="mb0">From this perspective, the question whether or not &#x0201c;AGI at the human level&#x0201d; will be realized is not the most relevant question for the time being. According to most AI scientists, this will certainly happen, and the key question is not IF this will happen, but WHEN, (e.g., <a href="#B67">M&#x000fc;ller and Bostrom, 2016</a>). At a system level, however, multiple narrow AI applications are likely to overtake human intelligence in an increasingly wide range of areas.</p><a id="h6" name="h6"></a><h2>Conclusions and Framework</h2><p class="mb15">The present paper focused on providing some more clarity and insight into the fundamental characteristics, differences and idiosyncrasies of human and artificial intelligences. First we presented ideas and arguments to scale up and differentiate our conception of intelligence, whether this may be human or artificial. Central to this broader, multi-faceted, conception of intelligence is the notion that intelligence in itself is a matter of information and computation, independent of its physical substrate. However, the nature of this physical substrate (biological/carbon or digital/silicon), will substantially determine its potential envelope of cognitive abilities and limitations. Organic cognitive faculties of humans have been very recently developed during the evolution of mankind. These &#x0201c;embryonal&#x0201d; faculties have been built on top of a biological neural network apparatus that has been optimized for allostasis and (complex) perceptual motor functions. Human cognition is therefore characterized by various structural limitations and distortions in its capacity to process certain forms of non-biological information. Biological neural networks are, for example, not very capable of performing arithmetic calculations, for which my pocket calculator fits millions of times better. These inherent and ingrained limitations, that are due to the biological and evolutionary origin of human intelligence, may be termed &#x0201c;hard-wired.&#x0201d;</p><p class="mb15">In line with the <em>Moravic&#x02019;s paradox</em>, we argued that intelligent behavior is more than what <em>we, as homo sapiens,</em> find difficult. So we should not confuse task-difficulty (subjective, anthropocentric) with task-complexity (objective). Instead we advocated a versatile conceptualization of intelligence and an acknowledgment of its many possible forms and compositions. This implies a high variety in types of biological or other forms of high (general) intelligence with a broad range of possible intelligence profiles and cognitive qualities (which may or may not surpass ours in many ways). This would make us better aware of the most probable potentials of AI applications for the short- and medium-term future. For example, from this perspective, our primary research focus should be on those components of the intelligence spectrum that are relatively difficult for the human brain and relatively easy for machines. This involves primarily the <em>cognitive</em> component requiring calculation, arithmetic analysis, statistics, probability calculation, data analysis, logical reasoning, memorization, et cetera.</p><p class="mb15">In line with this we have advocated a modest, more humble, view of our human, general intelligence. Which also implies that human-level AGI should not be considered as the &#x0201c;golden standard&#x0201d; of intelligence (to be pursued with foremost priority). Because of the many fundamental differences between natural and artificial intelligences, human-like AGI will be very difficult to accomplish in the first place (and also with relatively limited added value). In case an AGI will be accomplished in the (far) future it will therefore probably have a completely different profile of cognitive capacities and abilities than we, as humans, have. When such an AGI has come so far that it is able to &#x0201c;collaborate&#x0201d; like a human, it will at the same time be likely that can in many respects already function at highly superior levels relative to what we are able to. For the time being, however, it will not be very realistic and useful to aim at AGI that includes the broad scope of human perceptual-motor and cognitive abilities. Instead, the most profitable AI applications for the short- and mid-term future, will probably be based on multiple narrow AI systems. These multiple narrow AI applications may catch up with human intelligence in an increasingly broader range of areas.</p><p class="mb15">From this point of view we advocate not to dwell too intensively on the AGI question, whether or when AI will outsmart us, take our jobs, or how to endow it with all kinds of human abilities. Given the present state of the art it may be wise to focus more on the whole system of multiple AI innovations with humans as a crucial connecting and supervising factor. This also implies the establishment and formalization of legal boundaries and proper (effective, ethical, safe) goals for AI systems (<a href="#B22">Elands et al., 2019</a>; <a href="#B2">Aliman, 2020</a>). So this human factor (legislator, user, &#x0201c;collaborator&#x0201d;) needs to have good insight into the characteristics and capacities of biological and artificial intelligence (under all sorts of tasks and working conditions). Both in the workplace and in policy making the most fruitful AI applications will be to complement and compensate for the inherent biological and cognitive constraints of humans. For this reason, prominent issues concern how to use it intelligently? For what tasks and under what conditions decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the strengths of human intelligence and how to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition. See (<a href="#B39">Hoffman and Johnson, 2019</a>; <a href="#B85">Shneiderman, 2020a</a>; <a href="#B86">Shneiderman, 2020b</a>) for recent overviews.</p><p class="mb15">In summary: No matter how intelligent autonomous AI agents become in certain respects, at least for the foreseeable future, they will remain unconscious machines. These machines have a fundamentally different operating system (biological vs digital) with correspondingly different cognitive abilities and qualities than people and other animals. So, before a proper &#x0201c;team collaboration&#x0201d; can start, the human team members will have to understand these kinds of differences, i.e., how human information processing and intelligence differs from that of&#x02013;the many possible and specific variants of&#x02014;AI systems. Only when humans develop a proper of these &#x0201c;interspecies&#x0201d; differences they can effectively capitalize on the potential benefits of AI in (future) human-AI teams. Given the high flexibility, versatility, and adaptability of humans relative to AI systems, the first challenge becomes then how to ensure human adaptation to the more rigid abilities of AI?<a href="#fn4"><sup>4</sup></a> In other words: how can we achieve a proper conception the differences between human- and artificial intelligence?</p><h3 class="pt0">Framework for Intelligence Awareness Training</h3><p class="mb0">For this question, the issue of <em>Intelligence Awareness</em> in human professionals needs to be addressed more vigorously. Next to computer tools for the distribution of relevant awareness information (<a href="#B20">Collazos et al., 2019</a>) in human-machine systems, this requires better education and training on how to deal with the very new and different characteristics, idiosyncrasies, and capacities of AI systems. This includes, for example, a proper understanding of the basic characteristics, possibilities, and limitations of the AI&#x02019;s cognitive system properties without anthropocentric and/or anthropomorphic misconceptions. In general, this <em>&#x0201c;Intelligence Awareness&#x0201d;</em> is highly relevant in order to better understand, investigate, and deal with the manifold possibilities and challenges of machine intelligence. This practical human-factors challenge could, for instance, be tackled by developing new, targeted and easily configurable (adaptive) training forms and learning environments for human-AI systems. These flexible training forms and environments, (e.g. simulations and games) should focus at developing knowledge, insight and practical skills concerning the specific, non-human characteristics, abilities, and limitations of AI systems and how to deal with these in practical situations. People will have to understand the critical factors determining the goals, performance, and choices of AI? Which may in some cases even include the simple notion that AIs excite as much about their performance in achieving their goals as your refrigerator does for keeping your milkshake well. They have to learn when and under what conditions decisions are safe to leave to AI and when is human judgment required or essential? And more in general: how does it &#x0201c;think&#x0201d; and decide? The relevance of this kind of knowledge, skills and practices will only become bigger when the degree of autonomy (and genericity) of advanced AI systems will grow.</p><p class="mb0">What does such an <em>Intelligence Awareness</em> training curriculum look like? It needs to include at least a module on the cognitive characteristics of AI. This is basically a subject similar to those subjects that are also included in curricula on <em>human</em> cognition. This broad module on the &#x0201c;Cognitive Science of AI&#x0201d; may involve a range of sub-topics starting with a revision of the concept of "Intelligence" stripped of anthropocentric and anthropomorphic misunderstandings. In addition, this module should focus at providing knowledge about the structure and operation of the AI operating system or the &#x0201c;AI mind.&#x0201d; This may be followed by subjects like: Perception and interpretation of information by AI, AI cognition (memory, information processing, problem solving, biases), dealing with AI possibilities and limitations in the &#x0201c;human&#x0201d; areas like creativity, adaptivity, autonomy, reflection, and (self-) awareness, dealing with goal functions (valuation of actions in relation to cost-benefit), AI ethics and AI security. In addition, such a curriculum should include technical modules providing insight into the working of the AI operating system. Due to the enormous speed with which the AI technology and application develops, the content of such a curriculum is also very dynamic and continuously evolving on the basis of technological progress. This implies that the curriculum and training-aids and -environments should be flexible, experiential, and adaptive, which makes the work form of <em>serious gaming</em> ideally suited. Below, we provide a global framework for the development of new educational curricula on AI awareness. These subtopics go beyond learning to effectively &#x0201c;operate,&#x0201d; &#x0201c;control&#x0201d; or interact with specific AI applications (i.e. conventional human-machine interaction): </p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Understanding of underlying system characteristics of the AI (the &#x0201c;AI brain&#x0201d;). Understanding the specific qualities and limitations of AI relative to human intelligence.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Understanding the complexity of the tasks and of the environment from the perspective of AI systems.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Understanding the problem of biases in human cognition, relative to biases in AI.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Understanding the problems associated with the control of AI, predictability of AI behavior (decisions), building trust, maintaining situation awareness (complacency), dynamic task allocation, (e.g. taking over each other&#x02019;s tasks) and responsibility (accountability).</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;How to deal with possibilities and limitations of AI in the field of &#x0201c;creativity&#x0201d;, adaptability of AI, &#x0201c;environmental awareness&#x0201d;, and generalization of knowledge.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Learning to deal with perceptual and cognitive limitations and possible errors of AI which may be difficult to comprehend.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Trust in the performance of AI (possibly in spite of limited transparency or ability to &#x0201c;explain&#x0201d;) based on verification and validation.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Learning to deal with our natural inclination to anthropocentrism and anthropomorphism (&#x0201c;theory of mind&#x0201d;) when reasoning about human-robot interaction.</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;How to capitalize on the powers of AI in order to deal with the inherent constraints of human information processing (and vice versa).</p><p style="margin-left: 2em;text-indent:-0.7em;margin-top: 0em;margin-bottom: 0em;">&#x02010;Understanding the specific characteristics and qualities of the man-machine system and being able to decide on when, for what, and how the integrated combination of human- and AI faculties may perform at best overall system potential.</p><p class="mb0">In conclusion: due to the enormous speed with which the AI technology and application evolves we need a more versatile conceptualization of intelligence and an acknowledgment of its many possible forms and combinations. A revised conception of intelligence includes also a good understanding of the basic characteristics, possibilities, and limitations of different (biological, artificial) cognitive system properties without anthropocentric and/or anthropomorphic misconceptions. This &#x0201c;Intelligence Awareness&#x0201d; is highly relevant in order to better understand and deal with the manifold possibilities and challenges of machine intelligence, for instance to decide when to use or deploy AI in relation to tasks and their context. The development of educational curricula with new, targeted, and easily configurable training forms and learning environments for human-AI systems are therefore recommended. Further work should focus on training tools, methods and content that are flexible and adaptive enough to be able to keep up with the rapid changes in the field of AI and with the wide variety of target groups and learning goals.</p><a id="h7" name="h7"></a><h2>Author Contributions</h2><p class="mb0">The literature search, analysis, conceptual work, and the writing of the manuscript was done by JEK. All authors listed have made substantial, direct, and intellectual contribution to the work and approved it for publication.</p><a id="h8" name="h8"></a><h2>Conflict of Interest</h2><p class="mb0">The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p><a id="h9" name="h9"></a><h2>Acknowledgments</h2><p class="mb0">The authors want to thank J. van Diggelen, L.J.H.M. Kester for their useful inputs for this manuscript. The present paper was a deliverable of 1) the BIHUNT program (Behavioral Impact of NIC Teaming, V1719) funded by the Dutch Ministry of Defense and of the Wise Policy Making program funded by the Netherlands Organization for Applied Scientific Research (TNO).</p><a id="h10" name="h10"></a><h2>Footnotes</h2><p class="editor"><sup><a id="fn1" class="color1">1</a></sup>Narrow AI can be defined as the production of systems displaying intelligence regarding specific, highly constrained tasks, like playing chess, facial recognition, autonomous navigation, or locomotion (<a href="#B23">Goertzel et al., 2014</a>).</p><p class="editor"><sup><a id="fn2" class="color1">2</a></sup>Cognitive abilities involve deliberate, conceptual or analytic thinking (e.g., calculation, statistics, analysis, reasoning, abstraction)</p><p class="editor"><sup><a id="fn3" class="color1">3</a></sup>Unless of course AI will be deliberately constrained or degraded to human-level functioning.</p><p class="editor"><sup><a id="fn4" class="color1">4</a></sup>Next to the issue of Human-Aware AI, i.e. tuning AI to the cognitive characteristics of humans.</p><a id="h11" name="h11"></a><h2>References</h2><div class="References"><p class="ReferencesCopy1"><a name="B1" id="B1"></a>Ackermann, N. (2018). <em>Artificial Intelligence Framework: a visual introduction to machine learning and AI</em> Retrieved from: <a href="https://towardsdatascience.com/artificial-intelligence-framework-a-visual-introduction-to-machine-learning-and-ai-d7e36b304f87">https://towardsdatascience.com/artificial-intelligence-framework-a-visual-introduction-to-machine-learning-and-ai-d7e36b304f87</a>. (September 9, 2019).</p></div><div class="References"><p class="ReferencesCopy1"><a name="B2" id="B2"></a>Aliman, N-M. (2020). <em><em>Hybrid cognitive-affective Strategies for AI safety</em>. PhD thesis</em>. Utrecht, Netherlands: <span class="publisher-name">Utrecht University</span>. doi:10.33540/203 </p><p class="ReferencesCopy2"><a href="https://doi.org/10.33540/203">CrossRef Full Text</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B3" id="B3"></a>Bao, J. X., Kandel, E. R., and Hawkins, R. D. (1997). Involvement of pre- and postsynaptic mechanisms in posttetanic potentiation at Aplysia synapses. <em>Science</em> 275, 969&#x02013;973. doi:10.1126/science.275.5302.969Dane</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/9020078/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1126/science.275.5302.969Dane">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Involvement+of+pre-+and+postsynaptic+mechanisms+in+posttetanic+potentiation+at+Aplysia+synapses&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B4" id="B4"></a>Bar, M. (2007). The proactive brain: using analogies and associations to generate predictions. <em>Trends Cogn. Sci.</em> 11, 280&#x02013;289. doi:10.1016/j.tics.2007.05.005</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/17548232/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1016/j.tics.2007.05.005">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+proactive+brain:+using+analogies+and+associations+to+generate+predictions&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B5" id="B5"></a>Baron, J., and Ritov, I. (2004). Omission bias, individual differences, and normality. <em>Organizational Behav. Hum. Decis. Process.</em> 94, 74&#x02013;85. doi:10.1016/j.obhdp.2004.03.003</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.obhdp.2004.03.003">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Omission+bias,+individual+differences,+and+normality&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B6" id="B6"></a>Belkom, R. v. (2019). Duikboten zwemmen niet: de zoektocht naar intelligente machines. <span class="publisher-name">Den Haag: Stichting Toekomstbeeld der Techniek (STT)</span>. </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Duikboten+zwemmen+niet:+de+zoektocht+naar+intelligente+machines&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B8" id="B8"></a>Bergstein, B. (2017). <em>AI isn&#x02019;t very smart yet. But we need to get moving to make sure automation works for more people</em>. Cambridge, MA, United States: <span class="publisher-name">MIT Technology</span> Retrieved from: <a href="https://www.technologyreview.com/s/609318/the-great-ai-paradox/">https://www.technologyreview.com/s/609318/the-great-ai-paradox/</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B9" id="B9"></a>Bieger, J. B., Thorisson, K. R., and Garrett, D. (2014). &#x0201c;Raising AI: tutoring matters,&#x0201d; in <em>7th international conference, AGI 2014 quebec city, QC, Canada, august 1&#x02013;4, 2014 proceedings</em>. Editors B. Goertzel, L. Orseau, and J. Snaider (Berlin, Germany: <span class="publisher-name">Springer</span>). doi:10.1007/978-3-319-09274-4</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1007/978-3-319-09274-4">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Raising+AI:+tutoring+matters&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B10" id="B10"></a>Boden, M. (2017). Principles of robotics: regulating robots in the real world. <em>Connect. Sci.</em> 29 (2), 124&#x02013;129.</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1080/09540091.2016.1271400">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Principles+of+robotics:+regulating+robots+in+the+real+world&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B13" id="B13"></a>Bostrom, N. (2014). <em>Superintelligence: pathts, dangers, strategies</em>. Oxford United Kingdom: <span class="publisher-name">Oxford University Press</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B15" id="B15"></a>Bradshaw, J. M., Dignum, V., Jonker, C. M., and Sierhuis, M. (2012). Introduction to special issue on human-agent-robot teamwork. <em>IEEE Intell. Syst.</em> 27, 8&#x02013;13. doi:10.1109/MIS.2012.37</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1109/MIS.2012.37">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Introduction+to+special+issue+on+human-agent-robot+teamwork&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B16" id="B16"></a>Brodal, A. (1981). <em>Neurological anatomy in relation to clinical medicine</em>. New York, NY, United States: <span class="publisher-name">Oxford University Press</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B17" id="B17"></a>Brown, T. B. (2020). Language models are few-shot learners, <em>arXiv</em> 2005, 14165v4. </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Language+models+are+few-shot+learners&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B18" id="B18"></a>Cialdini, R. D. (1984). <em>Influence: the psychology of persuation</em>. New York, NY, United States: <span class="publisher-name">Harper</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B19" id="B19"></a>Coley, J. D., and Tanner, K. D. (2012). Common origins of diverse misconceptions: cognitive principles and the development of biology thinking. <em>CBE Life Sci. Educ.</em> 11 (3), 209&#x02013;215. doi:10.1187/cbe.12-06-0074</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/22949417/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1187/cbe.12-06-0074">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Common+origins+of+diverse+misconceptions:+cognitive+principles+and+the+development+of+biology+thinking&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B20" id="B20"></a>Collazos, C. A., Gutierrez, F. L., Gallardo, J., Ortega, M., Fardoun, H. M., and Molina, A. I. (2019). Descriptive theory of awareness for groupware development. <em>J. Ambient Intelligence Humanized Comput.</em> 10, 4789&#x02013;4818. doi:10.1007/s12652-018-1165-9</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1007/s12652-018-1165-9">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Descriptive+theory+of+awareness+for+groupware+development&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B21" id="B21"></a>Damasio, A. R. (1994). <em>Descartes&#x02019; error: emotion, reason and the human brain</em>. New York, NY, United States: <span class="publisher-name">G. P. Putnam&#x02019;s Sons</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B22" id="B22"></a>Elands, P., HuizingKester, A. L., Oggero, S., and Peeters, M. (2019). Governing ethical and effective behavior of intelligent systems: a novel framework for meaningful human control in a military context. <em>Militaire Spectator</em> 188 (6), 302&#x02013;313. </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Governing+ethical+and+effective+behavior+of+intelligent+systems:+a+novel+framework+for+meaningful+human+control+in+a+military+context&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B24" id="B24"></a>Feldman-Barret, L. (2017). <em>How emotions are made: the secret life of the brain</em>. Boston, MA, United States: <span class="publisher-name">Houghton Mifflin Harcourt</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B25" id="B25"></a>Fink, J. (2012). &#x0201c;Anthropomorphism and human likeness in the design of robots and human-robot interaction,&#x0201d; in <em>Social robotics. ICSR 2012</em>. <em>Lecture notes in computer science</em>. Editors S. S. Ge, O. Khatib, J. J. Cabibihan, R. Simmons, and M. A. Williams (Berlin, Germany: <span class="publisher-name">Springer</span>), 7621. doi:10.1007/978-3-642-34103-8_20</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1007/978-3-642-34103-8_20">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Anthropomorphism+and+human+likeness+in+the+design+of+robots+and+human-robot+interaction&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B26" id="B26"></a>Fischetti, M. (2011). <em>Computers vs brains. Scientific American 175<sup>th</sup> anniversary issue</em> Retreived from: <a href="https://www.scientificamerican.com/article/computers-vs-brains/">https://www.scientificamerican.com/article/computers-vs-brains/</a>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B27" id="B27"></a>Furnham, A., and Boo, H. C. (2011). A literature review of the anchoring effect. <em>The J. Socio-Economics</em> 40, 35&#x02013;42. doi:10.1016/j.socec.2010.10.008</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.socec.2010.10.008">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=A+literature+review+of+the+anchoring+effect&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B28" id="B28"></a>Gerla, M., Lee, E-K., and Pau, G. (2014). Internet of vehicles: from intelligent grid to autonomous cars and vehicular clouds. <em>WF-IoT</em> 12, 241&#x02013;246. doi:10.1177/1550147716665500</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1177/1550147716665500">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Internet+of+vehicles:+from+intelligent+grid+to+autonomous+cars+and+vehicular+clouds&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B29" id="B29"></a>Gibson, J. J. (1979). <em>The ecological approach to visual perception</em>. Boston, MA, United States: <span class="publisher-name">Houghton Mifflin</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B30" id="B30"></a>Gibson, J. J. (1966). <em>The senses considered as perceptual systems</em>. Boston, MA, United States: <span class="publisher-name">Houghton Mifflin.</span></p></div><div class="References"><p class="ReferencesCopy1"><a name="B31" id="B31"></a>Gigerenzer, G., and Gaissmaier, W. (2011). Heuristic decision making. <em>Annu. Rev. Psychol.</em> 62, 451&#x02013;482. doi:10.1146/annurev-psych-120709-145346</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/21126183/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1146/annurev-psych-120709-145346">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Heuristic+decision+making&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B32" id="B32"></a>Goertzel, B. (2007). Human-level artificial general intelligence and the possibility of a technological singularity: a reaction to Ray Kurzweil&#x00027;s the singularity is near, and McDermott&#x02019;s critique of Kurzweil. <em>Artif. Intelligence</em> 171 (18), 1161&#x02013;1173. doi:10.1016/j.artint.2007.10.011</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.artint.2007.10.011">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Human-level+artificial+general+intelligence+and+the+possibility+of+a+technological+singularity:+a+reaction+to+Ray+Kurzweil's+the+singularity+is+near,+and+McDermotts+critique+of+Kurzweil&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B23" id="B23"></a>Goertzel, B., Orseau, L., and Snaider, J., (Editors). (2014). Preface. <em>7th international conference, AGI 2014 Quebec City, QC, Canada, August 1&#x02013;4, 2014 Proceedings</em><span class="publisher-name">Springer</span>.</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.artint.2007.10.011">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Human-level+artificial+general+intelligence+and+the+possibility+of+a+technological+singularity:+a+reaction+to+Ray+Kurzweil's+the+singularity+is+near,+and+McDermotts+critique+of+Kurzweil&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B33" id="B33"></a>Grind, W. A. van. de. (1997). <em>Natuurlijke intelligentie: over denken, intelligentie en bewustzijn van mensen en andere dieren</em>. 2nd edn. Amsterdam, Netherlands: <span class="publisher-name">Nieuwezijds</span> Retrieved from <a href="https://www.nieuwezijds.nl/boek/natuurlijke-intelligentie/">https://www.nieuwezijds.nl/boek/natuurlijke-intelligentie/</a>. (July 9, 2019).</p></div><div class="References"><p class="ReferencesCopy1"><a name="B34" id="B34"></a>Hardin, G. (1968). The tragedy of the commons. The population problem has no technical solution; it requires a fundamental extension in morality. <em>Science</em> 162, 1243&#x02013;1248. doi:10.1126/science.162.3859.1243</p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+tragedy+of+the+commons.+The+population+problem+has+no+technical+solution;+it+requires+a+fundamental+extension+in+morality&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B35" id="B35"></a>Haring, K. S., Watanabe, K., Velonaki, M., Tosell, C. C., and Finomore, V. (2018). Ffab&#x02014;the form function attribution bias in human-robot interaction. <em>IEEE Trans. Cogn. Dev. Syst.</em> 10 (4), 843&#x02013;851. doi:10.1109/TCDS.2018.2851569</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TCDS.2018.2851569">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Ffabthe+form+function+attribution+bias+in+human-robot+interaction&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B36" id="B36"></a>Haselton, M. G., Bryant, G. A., Wilke, A., Frederick, D. A., Galperin, A., Frankenhuis, W. E., et al. (2009). Adaptive rationality: an evolutionary perspective on cognitive bias. <em>Soc. Cogn.</em> 27, 733&#x02013;762. doi:10.1521/soco.2009.27.5.733</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1521/soco.2009.27.5.733">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Adaptive+rationality:+an+evolutionary+perspective+on+cognitive+bias&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B37" id="B37"></a>Haselton, M. G., Nettle, D., and Andrews, P. W. (2005). &#x0201c;The evolution of cognitive bias,&#x0201d; in <em>The handbook of evolutionary psychology</em>. Editor D.M. Buss (Hoboken, NJ, United States: <span class="publisher-name">John Wiley &#x00026; Sons</span>), 724&#x02013;746. </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+evolution+of+cognitive+bias&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B38" id="B38"></a>Henshilwood, C., and Marean, C. (2003). The origin of modern human behavior. <em>Curr. Anthropol.</em> 44 (5), 627&#x02013;651. doi:10.1086/377665</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/14971366/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1086/377665">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+origin+of+modern+human+behavior&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B39" id="B39"></a>Hoffman, R. R., and Johnson, M. (2019). &#x0201c;The quest for alternatives to &#x0201c;levels of automation&#x0201d; and &#x0201c;task allocation,&#x0201d; in <em>Human performance in automated and autonomous systems</em>. Editors M. Mouloua, and P. A. Hancock (Boca Raton, FL, United States: <span class="publisher-name">CRC Press</span>), 43&#x02013;68.</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1201/9780429458330-3">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+quest+for+alternatives+to+levels+of+automation+and+task+allocation&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B40" id="B40"></a>Hoffrage, U., Hertwig, R., and Gigerenzer, G. (2000). Hindsight bias: a by-product of knowledge updating? <em>J. Exp. Psychol. Learn. Mem. Cogn.</em> 26, 566&#x02013;581. doi:10.1037/0278-7393.26.3.566</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/10855418/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1037/0278-7393.26.3.566">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Hindsight+bias:+a+by-product+of+knowledge+updating?&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B41" id="B41"></a>Horowitz, M. C. (2018). The promise and peril of military applications of artificial intelligence. <em>Bulletin of the atomic scientists</em> Retrieved from <a href="https://thebulletin.org/militaryapplications-artificial-intelligence/promise-and-peril-military-applications-artificial-intelligence">https://thebulletin.org/militaryapplications-artificial-intelligence/promise-and-peril-military-applications-artificial-intelligence</a> (Accessed March 27, 2019). </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+promise+and+peril+of+military+applications+of+artificial+intelligence&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B42" id="B42"></a>Isaacson, J. S., and Scanziani, M. (2011). How inhibition shapes cortical activity. <em>Neuron</em> 72, 231&#x02013;243. doi:10.1016/j.neuron.2011.09.027</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/22017986/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1016/j.neuron.2011.09.027">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=How+inhibition+shapes+cortical+activity&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B43" id="B43"></a>Johnson, M., Bradshaw, J. M., Feltovich, P. J., Jonker, C. M., van Riemsdijk, M. B., and Sierhuis, M. (2014). Coactive design: designing support for interdependence in joint activity. <em>J. Human-Robot Interaction</em> 3 (1), 43&#x02013;69. doi:10.5898/JHRI.3.1.Johnson</p><p class="ReferencesCopy2"><a href="https://doi.org/10.5898/JHRI.3.1.Johnson">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Coactive+design:+designing+support+for+interdependence+in+joint+activity&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B44" id="B44"></a>Kahle, W. (1979). <em>Band 3: nervensysteme und Sinnesorgane</em>, in <em>Taschenatlas de anatomie. Stutttgart</em>. Editors W. Kahle, H. Leonhardt, and W. Platzer (New York, NY, United States: <span class="publisher-name">Thieme Verlag</span>).</p></div><div class="References"><p class="ReferencesCopy1"><a name="B45" id="B45"></a>Kahneman, D., and Klein, G. (2009). Conditions for intuitive expertize: a failure to disagree. <em>Am. Psychol.</em> 64, 515&#x02013;526. doi:10.1037/a0016755</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/19739881/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1037/a0016755">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Conditions+for+intuitive+expertize:+a+failure+to+disagree&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B47" id="B47"></a>Kahneman, D. (2011). <em>Thinking, fast and slow</em>. New York, NY, United States: <span class="publisher-name">Farrar, Straus and Giroux</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B48" id="B48"></a>Katz, B., and Miledi, R. (1968). The role of calcium in neuromuscular facilitation. <em>J. Physiol.</em> 195, 481&#x02013;492. doi:10.1113/jphysiol.1968.sp008469</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/4296699/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1113/jphysiol.1968.sp008469">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+role+of+calcium+in+neuromuscular+facilitation&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B49" id="B49"></a>Kiesler, S., and Hinds, P. (2004). Introduction to this special issue on human&#x02013;robot interaction. <em>Int J Hum-Comput. Int.</em> 19 (1), 1&#x02013;8. doi:10.1080/07370024.2004.9667337</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1080/07370024.2004.9667337">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Introduction+to+this+special+issue+on+humanrobot+interaction&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B50" id="B50"></a>Klein, G., Woods, D. D., Bradshaw, J. M., Hoffman, R. R., and Feltovich, P. J. (2004). Ten challenges for making automation a &#x02018;team player&#x02019; in joint human-agent activity. <em>IEEE Intell. Syst.</em> 19 (6), 91&#x02013;95. doi:10.1109/MIS.2004.74</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1109/MIS.2004.74">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Ten+challenges+for+making+automation+a+team+player+in+joint+human-agent+activity&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B51" id="B51"></a>Korteling, J. E. (1994). <em>Multiple-task performance and aging</em>. Bariet, Ruinen, Netherlands: <span class="publisher-name">Dissertation. TNO-Human Factors Research Institute/State University Groningen</span> <a href="https://www.researchgate.net/publication/310626711_Multiple-Task_Performance_and_Aging">https://www.researchgate.net/publication/310626711_Multiple-Task_Performance_and_Aging</a>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B52" id="B52"></a>Korteling, J. E., and Toet, A. (2020). Cognitive biases. in <em>Encyclopedia of behavioral neuroscience</em>. 2nd Edn (Amsterdam-Edinburgh: <span class="publisher-name">Elsevier Science</span>) doi:10.1016/B978-0-12-809324-5.24105-9</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1016/B978-0-12-809324-5.24105-9">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Cognitive+biases&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B53" id="B53"></a>Korteling, J. E., Brouwer, A. M., and Toet, A. (2018a). A neural network framework for cognitive bias. <em>Front. Psychol.</em> 9, 1561. doi:10.3389/fpsyg.2018.01561</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/30233451/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.3389/fpsyg.2018.01561">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=A+neural+network+framework+for+cognitive+bias&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B54" id="B54"></a>Korteling, J. E., van de Boer-Visschedijk, G. C., Boswinkel, R. A., and Boonekamp, R. C. (2018b). Effecten van de inzet van Non-Human Intelligent Collaborators op Opleiding and Training [V1719]. <em>Report TNO 2018 R11654. Soesterberg: TNO defense safety and security</em>, Soesterberg, Netherlands: <span class="publisher-name">TNO, Soesterberg</span>. </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Effecten+van+de+inzet+van+Non-Human+Intelligent+Collaborators+op+Opleiding+and+Training+%5BV1719%5D&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B55" id="B55"></a>Korteling, J. E., Gerritsma, J., and Toet, A. (2021). Retention and transfer of cognitive bias mitigation interventions: a systematic literature study. <em>Front. Psychol.</em> 1&#x02013;20. doi:10.13140/RG.2.2.27981.56800 </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Retention+and+transfer+of+cognitive+bias+mitigation+interventions:+a+systematic+literature+study&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B56" id="B56"></a>Kosslyn, S. M., and Koenig, O. (1992). <em>Wet Mind: the new cognitive neuroscience</em>. New York, NY, United States: <span class="publisher-name">Free Press</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B57" id="B57"></a>Kr&#x000e4;mer, N. C., von der P&#x000fc;tten, A., and Eimler, S. (2012). &#x0201c;Human-agent and human-robot interaction theory: similarities to and differences from human-human interaction,&#x0201d; in <em>Human-computer interaction: the agency perspective</em>. <em>Studies in computational intelligence</em>. Editors M. Zacarias, and J. V. de Oliveira (Berlin, Germany: <span class="publisher-name">Springer</span>), 396, 215&#x02013;240. doi:10.1007/978-3-642-25691-2_9</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1007/978-3-642-25691-2_9">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Human-agent+and+human-robot+interaction+theory:+similarities+to+and+differences+from+human-human+interaction&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B58" id="B58"></a>Kurzweil, R. (2005). <em>The singularity is near</em>. New York, NY, United States: <span class="publisher-name">Viking press</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B59" id="B59"></a>Kurzweil, R. (1990). <em>The age of intelligent machines</em>. Cambridge, MA, United States: <span class="publisher-name">MIT Press</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B60" id="B60"></a>Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2017). Building machines that learn and think like people. <em>Behav. Brain Sci.</em> 40, e253. doi:10.1017/S0140525X16001837</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/27881212/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1017/S0140525X16001837">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Building+machines+that+learn+and+think+like+people&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B61" id="B61"></a>Lichtenstein, S., and Slovic, P. (1971). Reversals of preference between bids and choices in gambling decisions. <em>J. Exp. Psychol.</em> 89, 46&#x02013;55. doi:10.1037/h0031207</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1037/h0031207">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Reversals+of+preference+between+bids+and+choices+in+gambling+decisions&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B62" id="B62"></a>McBrearty, S., and Brooks, A. (2000). The revolution that wasn&#x00027;t: a new interpretation of the origin of modern human behavior. <em>J. Hum. Evol.</em> 39 (5), 453&#x02013;563. doi:10.1006/jhev.2000.0435</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/11102266/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1006/jhev.2000.0435">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+revolution+that+wasn't:+a+new+interpretation+of+the+origin+of+modern+human+behavior&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B63" id="B63"></a>McClelland, J. L. (1978). Perception and masking of wholes and parts. <em>J. Exp. Psychol. Hum. Percept Perform.</em> 4, 210&#x02013;223. doi:10.1037//0096-1523.4.2.210</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/660096/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1037//0096-1523.4.2.210">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Perception+and+masking+of+wholes+and+parts&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B14" id="B14"></a>McDowd, J. M., and Craik, F. I. M. (1988). Effects of aging and task difficulty on divided attention performance. <em>J. Exp. Psychol. Hum. Percept. Perform</em>. 14, 267&#x02013;280.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B64" id="B64"></a>Minsky, M. (1986). <em>The Society of Mind</em>. London, United Kingdom: <span class="publisher-name">Simon and Schuster</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B65" id="B65"></a>Moravec, H. (1988). <em>Mind children</em>. Cambridge, MA, United States: <span class="publisher-name">Harvard University Press</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B66" id="B66"></a>Moravec, H. (1998). When will computer hardware match the human brain? <em>J. Evol. Tech.</em> 1Retreived from <a href="https://jetpress.org/volume1/moravec.htm">https://jetpress.org/volume1/moravec.htm</a>. </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=When+will+computer+hardware+match+the+human+brain?&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B67" id="B67"></a>M&#x000fc;ller, V. C., and Bostrom, N. (2016). Future progress in artificial intelligence: a survey of expert opinion. <em>Fundamental issues of artificial intelligence</em>. Cham, Switzerland: <span class="publisher-name">Springer</span>. doi:10.1007/978-3-319-26485-1</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1007/978-3-319-26485-1">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Future+progress+in+artificial+intelligence:+a+survey+of+expert+opinion&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B68" id="B68"></a>Nickerson, R. S. (1998). Confirmation bias: a ubiquitous phenomenon in many guises. <em>Rev. Gen. Psychol.</em> 2, 175&#x02013;220. doi:10.1037/1089-2680.2.2.175</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1037/1089-2680.2.2.175">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Confirmation+bias:+a+ubiquitous+phenomenon+in+many+guises&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B69" id="B69"></a>Nosek, B. A., Hawkins, C. B., and Frazier, R. S. (2011). Implicit social cognition: from measures to mechanisms. <em>Trends Cogn. Sci.</em> 15 (4), 152&#x02013;159. doi:10.1016/j.tics.2011.01.005</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/21376657/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1016/j.tics.2011.01.005">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Implicit+social+cognition:+from+measures+to+mechanisms&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B71" id="B71"></a>Patt, A., and Zeckhauser, R. (2000). Action bias and environmental decisions. <em>J. Risk Uncertain.</em> 21, 45&#x02013;72. doi:10.1023/a:1026517309871</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1023/a:1026517309871">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Action+bias+and+environmental+decisions&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B72" id="B72"></a>Peeters, M. M., van Diggelen, J., van Den Bosch, K., Bronkhorst, A., Neerincx, M. A., Schraagen, J. M., et al. (2020). Hybrid collective intelligence in a human&#x02013;AI society. <em>AI and Society</em> 38, 217&#x02013;(238.) doi:10.1007/s00146-020-01005-y</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1007/s00146-020-01005-y">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Hybrid+collective+intelligence+in+a+humanAI+society&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B73" id="B73"></a>Petraglia, M. D., and Korisettar, R. (1998). <em>Early human behavior in global context</em>. Oxfordshire, United Kingdom: <span class="publisher-name">Routledge</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B74" id="B74"></a>Pomerantz, J. (1981). &#x0201c;Perceptual organization in information processing,&#x0201d; in <em>Perceptual organization</em>. Editors M. Kubovy, and J. Pomerantz (Hillsdale, NJ, United States: <span class="publisher-name">Lawrence Erlbaum</span>). </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Perceptual+organization+in+information+processing&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B75" id="B75"></a>Pronin, E., Lin, D. Y., and Ross, L. (2002). The bias blind spot: perceptions of bias in self versus others. <em>Personal. Soc. Psychol. Bull.</em> 28, 369&#x02013;381. doi:10.1177/0146167202286008</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1177/0146167202286008">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+bias+blind+spot:+perceptions+of+bias+in+self+versus+others&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B46" id="B46"></a>Reicher, G. M. (1969). Perceptual recognition as a function of meaningfulness of stimulus material. <em>J. Exp. Psychol.</em> 81, 274&#x02013;280.</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/14584987/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1037/0003-066X.58.9.697">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=A+perspective+on+judgment+and+choice:+mapping+bounded+rationality&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B76" id="B76"></a>Rich, E., and Knight, K. (1991). <em>Artificial intelligence</em>. 2nd edition. New York, NY, United States: <span class="publisher-name">McGraw-Hill</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B77" id="B77"></a>Rich, E., Knight, K., and Nair, S. B. (2009). <em>Articial intelligence</em>. 3rd Edn. New Delhi, India: <span class="publisher-name">Tata McGraw-Hill</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B78" id="B78"></a>Risen, J. L. (2015). Believing what we do not believe: acquiescence to superstitious beliefs and other powerful intuitions. <em>Psychol. Rev.</em> 123, 182&#x02013;207. doi:10.1037/rev0000017</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/26479707/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1037/rev0000017">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Believing+what+we+do+not+believe:+acquiescence+to+superstitious+beliefs+and+other+powerful+intuitions&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B79" id="B79"></a>Roese, N. J., and Vohs, K. D. (2012). Hindsight bias. <em>Perspect. Psychol. Sci.</em> 7, 411&#x02013;426. doi:10.1177/1745691612454303</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/26168501/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1177/1745691612454303">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Hindsight+bias&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B80" id="B80"></a>Rogers, R. D., and Monsell, S. (1995). Costs of a predictible switch between simple cognitive tasks. <em>J. Exp. Psychol. Gen.</em> 124, 207e231. doi:10.1037/0096-3445.124.2.207</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1037/0096-3445.124.2.207">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Costs+of+a+predictible+switch+between+simple+cognitive+tasks&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B81" id="B81"></a>Rubinstein, J. S., Meyer, D. E., and Evans, J. E. (2001). Executive control of cognitive processes in task switching. <em>J. Exp. Psychol. Hum. Percept Perform.</em> 27, 763&#x02013;797. doi:10.1037//0096-1523.27.4.763</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/11518143/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1037//0096-1523.27.4.763">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Executive+control+of+cognitive+processes+in+task+switching&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B82" id="B82"></a>Russell, S., and Norvig, P. (2014). <em>Artificial intelligence: a modern approach</em>. 3rd ed. Harlow, United Kingdom: <span class="publisher-name">Pearson Education</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B83" id="B83"></a>Shafir, E., and LeBoeuf, R. A. (2002). Rationality. <em>Annu. Rev. Psychol.</em> 53, 491&#x02013;517. doi:10.1146/annurev.psych.53.100901.135213</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/11752494/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1146/annurev.psych.53.100901.135213">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Rationality&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B84" id="B84"></a>Shatz, C. J. (1992). The developing brain. <em>Sci. Am.</em> 267, 60&#x02013;67. doi:10.1038/scientificamerican0992-60</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/1502524/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1038/scientificamerican0992-60">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+developing+brain&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B85" id="B85"></a>Shneiderman, B. (2020a). Design lessons from AI&#x02019;s two grand goals: human emulation and useful applications. <em>IEEE Trans. Tech. Soc.</em> 1, 73&#x02013;82. doi:10.1109/TTS.2020.2992669</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TTS.2020.2992669">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Design+lessons+from+AIs+two+grand+goals:+human+emulation+and+useful+applications&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B86" id="B86"></a>Shneiderman, B. (2020b). Human-centered artificial intelligence: reliable, safe &#x00026; trustworthy. <em>Int. J. Human&#x02013;Computer Interaction</em> 36 (6), 495&#x02013;504. doi:10.1080/10447318.2020.1741118</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1080/10447318.2020.1741118">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Human-centered+artificial+intelligence:+reliable,+safe+&amp;+trustworthy&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B87" id="B87"></a>Siegel, A., and Sapru, H. N. (2005). <em>Essential neuroscience</em>. Philedelphia, PA, United States: <span class="publisher-name">Lippincott Williams and Wilkins</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B88" id="B88"></a>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., et al. (2017). Mastering the game of go without human knowledge. <em>Nature</em> 550 (7676), 354. doi:10.1038/nature24270</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/29052630/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1038/nature24270">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Mastering+the+game+of+go+without+human+knowledge&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B89" id="B89"></a>Simon, H. A. (1955). A behavioral model of rational choice. <em>Q. J. Econ.</em> 69, 99&#x02013;118. doi:10.2307/1884852</p><p class="ReferencesCopy2"><a href="https://doi.org/10.2307/1884852">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=A+behavioral+model+of+rational+choice&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B91" id="B91"></a>Taylor, D. M., and Doria, J. R. (1981). Self-serving and group-serving bias in attribution. <em>J. Soc. Psychol.</em> 113, 201&#x02013;211. doi:10.1080/00224545.1981.9924371</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1080/00224545.1981.9924371">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Self-serving+and+group-serving+bias+in+attribution&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B92" id="B92"></a>Tegmark, M. (2017). <em>Life 3.0: being human in the age of artificial intelligence</em>. New York, NY, United States: <span class="publisher-name">Borzoi Book published by A.A. Knopf</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B104" id="B104"></a>Toet, A., Brouwer, A. M., van den Bosch, K., and Korteling, J. E. (2016). Effects of personal characteristics on susceptibility to decision bias: a literature study. <em>Int. J. Humanities Soc. Sci.</em> 8, 1&#x02013;17.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B93" id="B93"></a>Tooby, J., and Cosmides, L. (2005). &#x0201c;Conceptual foundations of evolutionary psychology,&#x0201d; in <em>Handbook of evolutionary psychology</em>. Editor D.M. Buss (Hoboken, NJ, United States: <span class="publisher-name">John Wiley &#x00026; Sons</span>), 5&#x02013;67. </p><p class="ReferencesCopy2"><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Conceptual+foundations+of+evolutionary+psychology&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B94" id="B94"></a>Tversky, A., and Kahneman, D. (1974). Judgment under uncertainty: heuristics and biases. <em>Science</em> 185 (4157), 1124&#x02013;1131. doi:10.1126/science.185.4157.1124</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/17835457/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1126/science.185.4157.1124">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Judgment+under+uncertainty:+heuristics+and+biases&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B95" id="B95"></a>Tversky, A., and Kahneman, D. (1981). The framing of decisions and the psychology of choice. <em>Science</em> 211, 453&#x02013;458. doi:10.1126/science.7455683</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/7455683/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1126/science.7455683">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=The+framing+of+decisions+and+the+psychology+of+choice&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B96" id="B96"></a>Tversky, A., and Kahneman, D. (1973). Availability: a heuristic for judging frequency and probability. <em>Cogn. Psychol.</em> 5, 207&#x02013;232. doi:10.1016/0010-0285(73)90033-9</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1016/0010-0285(73)90033-9">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Availability:+a+heuristic+for+judging+frequency+and+probability&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B105" id="B105"></a>van den Bosch, K., and Bronkhorst, K. (2018). Human-AI cooperation to benefit military decision making. Soesterberg, Netherlands: TNO.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B106" id="B106"></a>van den Bosch, K., and Bronkhorst, K. (2019). Six challenges for human-AI Co-learning. <em>Adaptive instructional systems</em> 11597, 572&#x02013;589. doi:10.1007/978-3-030-22341-0_45</p></div><div class="References"><p class="ReferencesCopy1"><a name="B98" id="B98"></a>Weisstein, N., and Harris, C. S. (1974). Visual detection of line segments: an object-superiority effect. <em>Science</em> 186, 752&#x02013;755. doi:10.1126/science.186.4165.752</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/4417613/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.1126/science.186.4165.752">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Visual+detection+of+line+segments:+an+object-superiority+effect&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B99" id="B99"></a>Werkhoven, P., Neerincx, M., and Kester, L. (2018). Telling autonomous systems what to do. <span class="conf-name">Proceedings of the 36th European Conference on Cognitive Ergonomics, ECCE 2018</span>, <span class="conf-loc">Utrecht, Nehterlands</span>, <span class="conf-date">5&#x02013;7 September, 2018</span>, 1&#x02013;8. doi:10.1145/3232078.3232238</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1145/3232078.3232238">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Telling+autonomous+systems+what+to+do&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B90" id="B90"></a>Wheeler, D., (1970). Processes in word recognition <em>Cogn. Psychol.</em> 1, 59&#x02013;85.</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1017/S0140525X00003435">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Individual+differences+in+reasoning:+implications+for+the+rationality+debate?&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B100" id="B100"></a>Williams, A., and Weisstein, N. (1978). Line segments are perceived better in a coherent context than alone: an object-line effect in visual perception. <em>Mem. Cognit</em> 6, 85&#x02013;90. doi:10.3758/bf03197432</p><p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/661560/">PubMed Abstract</a> &#x0007c; <a href="https://doi.org/10.3758/bf03197432">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Line+segments+are+perceived+better+in+a+coherent+context+than+alone:+an+object-line+effect+in+visual+perception&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B101" id="B101"></a>Wingfield, A., and Byrnes, D. (1981). <em>The psychology of human memory</em>. New York, NY, united States: <span class="publisher-name">Academic Press</span>.</p></div><div class="References"><p class="ReferencesCopy1"><a name="B102" id="B102"></a>Wood, R. E., Mento, A. J., and Locke, E. A. (1987). Task complexity as a moderator of goal effects: a meta-analysis. <em>J. Appl. Psychol.</em> 72 (3), 416&#x02013;425. doi:10.1037/0021-9010.72.3.416</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1037/0021-9010.72.3.416">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Task+complexity+as+a+moderator+of+goal+effects:+a+meta-analysis&amp;btnG=">Google Scholar</a></p></div><div class="References"><p class="ReferencesCopy1"><a name="B103" id="B103"></a>Wyrobek, K. A., Berger, E. H., van der Loos, H. F. M., and Salisbury, J. K. (2008). Toward a personal robotics development platform: rationale and design of an intrinsically safe personal robot. <span class="conf-name">Proceedinds of</span><a href="https://ieeexplore.ieee.org/xpl/conhome/4534525/proceeding">2008 IEEE International Conference on Robotics and Automation</a>, <span class="conf-loc">Pasadena, CA, United States</span>, <span class="conf-date">19-23 May 2008</span>. doi:10.1109/ROBOT.2008.4543527</p><p class="ReferencesCopy2"><a href="https://doi.org/10.1109/ROBOT.2008.4543527">CrossRef Full Text</a> &#x0007c; <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Toward+a+personal+robotics+development+platform:+rationale+and+design+of+an+intrinsically+safe+personal+robot&amp;btnG=">Google Scholar</a></p></div></div><div class="thinLineM20"></div><div class="AbstractSummary"><p><span>Keywords:</span> human intelligence, artificial intelligence, artificial general intelligence, human-level artificial intelligence, cognitive complexity, narrow artificial intelligence, human-AI collaboration, cognitive bias</p><p><span>Citation:</span> Korteling JE(, van de Boer-Visschedijk GC, Blankendaal RAM, Boonekamp RC and Eikelboom AR (2021) Human- versus Artificial Intelligence. <em>Front. Artif. Intell.</em> 4:622364. doi: 10.3389/frai.2021.622364</p><p id="timestamps"><span>Received:</span> 29 October 2020; <span>Accepted:</span> 01 February 2021;<br><span>Published:</span> 25 March 2021.</p><div><p>Edited by:</p><a href="https://loop.frontiersin.org/people/622928/overview">Esma A&#x000ef;meur</a>, Universit&#x000e9; de Montr&#x000e9;al, Canada</div><div><p>Reviewed by:</p><a href="https://loop.frontiersin.org/people/632655/overview">Cesar Collazos</a>, University of Cauca, Colombia<br><a href="https://loop.frontiersin.org/people/626150/overview">Ranilson Oscar Ara&#x000fa;jo Paiva</a>, Federal University of Alagoas, Brazil</div><p><span>Copyright</span> &#x000a9; 2021 Korteling, van de Boer-Visschedijk, Blankendaal, Boonekamp and Eikelboom. This is an open-access article distributed under the terms of the <a rel="license" href="http://creativecommons.org/licenses/by/4.0/" target="_blank">Creative Commons Attribution License (CC BY).</a> The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p><p><span>&#x0002a;Correspondence:</span> J. E. (Hans). Korteling, <a href="mailto:hans.korteling@tno.nl">hans.korteling@tno.nl</a></p><div class="clear"></div></div>
    <div class="research-topic-container" style="display: none;">
        <a href="/research-topics/32834#articles" data-test-id="relatedRT-link">
            <div class="research-topic-data">
                <h5 class="topic-title">
                    <span>This article is part of the Research Topic</span>
                </h5>
                <p style="margin-bottom:0;">

                    Skills-in-Demand: Bridging the Gap between Educational Attainment and Labor Market with Learning Analytics and Machine Learning Applications
                </p>
                    <div class="topic-link-trim" data-event="rt-link-click">
                        View all
4                        Articles <span style="position: relative;top: 1px;"></span>
                    </div>


            </div>
        </a>
    </div>


<!-- TODO:BEGIN IS PART OF RT-->
<script type="text/javascript">
      
            const isPartOfRTElements = document.getElementsByClassName('research-topic-container');

            if (isPartOfRTElements.length > 0) {
                const headerBar3Container = document.getElementsByClassName('header-bar-three-container');

                if (headerBar3Container.length > 0) {
                    const container = headerBar3Container[0];
                    container.appendChild(isPartOfRTElements[0]);
                    isPartOfRTElements[0].style.display = 'block';
                }
            }
       
</script>
<!-- TODO:END IS PART OF RT-->

                <div class="thin-line-dark"></div>

            </div>
        </div>
    </div>
</main>
            </div>

                <div class="side-article-subjects only-devices  widget-listing people-also-looked-at hidden">
                    <h5 class="like-h4">People also looked at</h5>
                </div>

        </div>
    </div>
</div>
<div id="divTemplates">
    <div class="modal fade modal-container impact-modal-container" data-backdrop="static" id="impactModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
</div>






    <div class="modal fade modal-container supplementary-modal-container"
     data-backdrop="static"
     data-keyboard="false"
     id="supplementaryFilesModal"
     tabindex="-1" role="dialog"
     aria-labelledby="mySupplementaryModalLabel"
     aria-hidden="true"
     style="display: none; padding-right: 17px;">
</div>




<script type="text/template" id="template-supplementary-files-modal">
    <div class="supplementary-material-wrapper modal-dialog" role="document">
        <div class="modal-content">
            <div class="modal-body">
                <a class="close" data-dismiss="modal"  aria-label="Close"><span data-event="supplementaryclose-icon-click" aria-hidden="true"></span></a>
                <h4>Supplementary Material</h4><br>
                <p class="supplementary-empty-message" style="display: none;">There is no supplementary material currently available for this article</p>
                <div class="loading-wrapper" id="loader" style="display: none;">
                    Loading supplemental data...
                    <img style="" src="/Areas/Articles/Images/Icon/loading.gif" alt="Loading..">
                </div>
                <br>
                <div class="table-responsive" id="localFiles">

                </div>
                <div id="figshare-widget-container">

                </div>
            </div>
            <br>
            <div class="modal-footer bottom-links">
                <a class="btn btn-default" data-dismiss="modal">Close</a>
            </div>
        </div>

    </div>
</script>





<script type="text/template" id="template-local-files-modal">
    <table class="table table-striped supplementary-content" id="localFilesTable">
        <thead>
            <tr>
                <th>&nbsp;</th>
                <th>File Name</th>
                <th>&nbsp;</th>
            </tr>
        </thead>
        <tbody class="file-content">
            <% _.each(supplimentalFileDetails.FileDetails, function(fileViewModel){   %>
            <tr>
                <th scope="row"><a data-test-id="<%= fileViewModel.TestId %>" href="<%= fileViewModel.FileDownloadUrl %>"><img class="img-figure" src="<%= fileViewModel.ImageUrl %>"></a></th>
                <td>
                    <a  href="<%= fileViewModel.FileDownloadUrl %>" data-event="supplementarydownload-button-click">
                        <%= fileViewModel.FileName %>
                    </a>
                </td>
                <td><a  href="<%= fileViewModel.FileDownloadUrl %>" data-event="supplementarydownload-button-click" class="btn-link"><i class="fa fa-download" aria-hidden="true"></i></a></td>
            </tr>
            <% }); %>
        </tbody>
    </table>
</script>







    <div class="modal fade modal-container notifyme-modal-container" data-backdrop="static" id="notifyModal" tabindex="-1" role="dialog" aria-labelledby="Notify on publication" aria-hidden="true">
</div>



<script type="text/template" id="template-notify-me-modal">
    <div class="modal-f page-container simple-modal" role="dialog" aria-labelledby="NotfifyMeModalLabel" id="notify_me_popup">
            <div class="modal-dialog" role="document">
                <div class="modal-content">
                    <form onsubmit="return false" id="modalnotifyme">
                        <div class="modal-header">
                            <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"></span></button>
                            <h3>Notify me on publication</h3>
                        </div>
                        <div class="modal-body">

                            <div class="form-group">
                                <label>Please enter your email address:</label>
                            <input type="email" required class="form-control" id="txt_notification_email_id" placeholder="Email">
                            </div>

                        <div class="g-recaptcha" id="recaptcha"></div><br />

                        <p class="small" style="color: #555;">If you already have an account, please <a href="<%=login.FrontiersLoginUrl%>?returnUrl=<%=currentPage%>" class="text-blue">login</a>.</p>
                        <p class="small" style="color: #555;">You don't have a Frontiers account ? You can <a href="<%=login.FrontiersRegistrationUrl%>" class="text-blue">register here</a>.</p>
                        </div>
                        <div class="modal-footer">
                        <button type="button" id="article_notify_non_registered_user" class="btn btn-default btn-progress" style="width: 118px; visibility:hidden;">Notify me</button>
                        </div>
                    </form>
                </div>
            </div>
        </div>
</script>

<script type="text/template" id="template-notifyme-mailselection-modal">
    <div class="modal-f page-container simple-modal" tabindex="-1" role="dialog" aria-labelledby="NotfifyMeModalLabel">
        <div class="modal-dialog" role="document">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"></span></button>
                    <h3>Select one of your emails</h3>
                </div>
                <div class="modal-body">
                    <form>
                        <div class="form-group">
                            <label>You have multiple emails registered with Frontiers:</label>
                        </div>
                                           
                        <% _.each(userEmailList, function(user){   %>
                        <input type="radio" value="<%=user%>" name="notify_Emailcollection"> <%=user%><br>
                        <% }); %>

                    </form>
                </div>
                <div class="modal-footer">
                    <button type="button" id="article_notify_loggedin_user" class="btn btn-default btn-progress" style="width: 118px;">Notify me</button>
                </div>
            </div>

        </div>

    </div>
</script>

<script type="text/template" id="template_notifyme_error_modal">
    <div class="modal-f page-container simple-modal" tabindex="-1" role="dialog" aria-labelledby="NotfifyMeModalLabel">
        <div class="modal-dialog" role="document">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"></span></button>
                    <h3>Notify me on publication</h3>
                </div>
                <div class="modal-body">
                    <form>
                        <div class="form-group">
                            <p class="text-red"><strong><%=message%></strong></p>
                        </div>
                    </form>
                </div>
                <div class="modal-footer">
                    <button type="button" data-dismiss="modal" class="btn btn-default btn-progress" style="width: 118px;">Cancel</button>
                </div>
            </div>
        </div>
    </div>
</script>







</div>

    </div>
        <a id="floating-download-pdf-btn" class="floating-button" data-event="downloadfloating-button-click" href="/articles/10.3389/frai.2021.622364/pdf?isPublishedV2=False">
            <span class="floating-button-text" data-event="downloadfloating-button-click">Download</span>
        </a>
        <script type="text/javascript">

            $(document).ready(function () {
                document.onload = observerDowloadFloatingBtn();
            });
            function downloadButtonVisibilityChange(entries) {
                const btn = entries[0];
                const floatingBtn = document.querySelector("#floating-download-pdf-btn");
                if (!btn.isIntersecting) {
                    floatingBtn.classList.add("floating-button--show");
                } else {
                    floatingBtn.classList.remove("floating-button--show");
                }
            }

            function observerDowloadFloatingBtn() {
                const options = {
                    root: document.getElementsByName('body')[0],
                    rootMargin: '0px',
                    threshold: 1.0
                }

                const observer = new IntersectionObserver(downloadButtonVisibilityChange, options);
                const target = document.querySelector('[data-test-id="download-button"]');
                observer.observe(target);
            }
        </script>
    <frontiers-footer main-domain="frontiersin.org"></frontiers-footer>

    <script src="https://crossmark.crossref.org/javascripts/v1.5/crossmark.min.js"></script>
        <script type="text/javascript">
            var $ = jQuery.noConflict(); //==>>ToDo: Avoid this..!!
        </script>

    <script src=" https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
 
    <script src=" https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js" async></script>
 
    <script src="https://static.frontiersin.org/areas/articles/js/frontiers?v=O9-mxZHGMRH9UWcWq5BW6IczDmW5Nz2Fjt5wgZmuXZA1"></script>


<script type="text/javascript">

        var FRArticle = (function() {
            return {
                ArticleId: '622364',
                DOI:'10.3389/frai.2021.622364',
                LoginUserId: '0',
                DomainId: '1',
                FieldId: '118',
                SpecialityId: '1920',
                IsPreview: FRSafe.boolean('False'),
                IsViewImpactFromLoop: FRSafe.boolean('False'),
                IsPublished: 'True',
                FigShareApiUrl: 'https://api.figshare.com/v2/collections/search',
                FigShareTimeOut: '3000',
                ProvisionallyAcceptedText: 'The final, formatted version of the article will be published soon.',
                NotifyMeButtonText: 'Notify me',
                NotifyMeButtonSecondaryText: 'We&#39;ll notify you',
                IsSimilarArticleVisible: FRSafe.boolean('True'),
                JournalId: '1437',
                SectionId: '1476',
                JournalName: 'Artificial Intelligence',
                SectionName: 'AI for Human Learning and Behavior Change',
                ArticleTitle: 'Human- versus Artificial Intelligence',
                AcceptedDate: '20210201',
                PublishedDate: '20210325',
                ContentType: 'Published',
                ResearchTopicId: '32834',
                IsImpactMetricsVisible: FRSafe.boolean('True'),
                IsPublishedV2: FRSafe.boolean('False')
            };
        })();

        var FRSocial = (function() {
            return {
                itemId: 14,
                itemTypeId: 1,
                entityId: '622364',
                ownerId: '228168',
                sanPath: 'https://www.frontiersin.org/files/',
                subItemId: '8',
                loginUserId: '0',
                ownerNWDBId: '29',
                pageType: 1,
                loopUrl:'https://loop.frontiersin.org'
            };
        })();
</script>
    <script>
        var FRAjaxSettings = (function () {

            function urlLowercase() {
                $.ajaxSetup({
                    beforeSend: function (jqXHR, settings) {
                        settings.url = settings.url.toLowerCase();
                    }
                });
            }

            return { urlLowercase: urlLowercase };
        })();

        FRAjaxSettings.urlLowercase();
    </script>

    <script>
        function setBodyOverflow(dropdownElement) {
            if (!dropdownElement) return;
            const windowWidth = window.innerWidth;
            if (windowWidth <= 1024) {
                const body = document.querySelector("body");
                if (dropdownElement.classList.contains("open")) {
                    body.style.overflow = "hidden";
                } else {
                    body.style.overflow = "auto";
                }
            }
        }
        $(document).ready(function () {
            $(document).on('click', '.dropdown-menu-wrapper', function (e) {
                e.stopPropagation();
            });

            $(document).on('click', '[data-toggle="dropdown"]', function (e) {
                const dropdownElement = e.target.closest(".dropdown")
                if (dropdownElement) {
                    setBodyOverflow(dropdownElement);
                }
            });
        });

        function supplementalDataToggle() {
            const el = document.getElementById("supplementaryFilesModal");
            if (el) {
                el.classList.toggle("in");
                if (el.classList.contains("in")) {
                    el.style.display = "block";
                } else {
                    el.style.display = "none";
                }
            }
        }

        function initToggleSupplementalData() {
            const btn = document.getElementsByClassName("btn-open-supplemental")[0];
            const btn2 = document.getElementsByClassName("btn-open-supplemental")[1];
            const closeButtons = document.getElementById("supplementaryFilesModal").querySelectorAll("[data-dismiss='modal']");

            if (btn) { btn.onclick = supplementalDataToggle; }
            if (btn2) { btn2.onclick = supplementalDataToggle; }
            if (closeButtons) { closeButtons.forEach(b => b.onclick = supplementalDataToggle); }
        }


        initToggleSupplementalData();
    </script>

    <script type="text/plain" class="optanon-category-C0002">
        function trackUserViewItemGA4() {
            const dataLayer = (window).dataLayer;
            if (dataLayer) {
                dataLayer.push({
                    'event': 'view_article',
                    'content_type': 'Published',
                    'article_title': 'Human- versus Artificial Intelligence',
                    'journal_title': 'Artificial Intelligence',
                    'research_topic_title': 'Skills-in-Demand: Bridging the Gap between Educational Attainment and Labor Market with Learning Analytics and Machine Learning Applications',
                    'section_title': "AI for Human Learning and Behavior Change",
                    'published_date': '20210325',
                    'accepted_date': '20210201',
                    'article_id': '622364',
                    'journal_id': '1437',
                    'research_topic_id': '32834',
                    'section_id' :'1476',
                    'article_type': 'CONCEPTUAL ANALYSIS article'
                });
            }
        }

            trackUserViewItemGA4();

    </script>

        <script type="text/javascript">
            dataLayer.push({
                'loopId': undefined,
            });
        </script>
    <script type="text/javascript">
        var FRTitles = (function () {
            return {
                ResearchTopicTitle: 'Skills-in-Demand: Bridging the Gap between Educational Attainment and Labor Market with Learning Analytics and Machine Learning Applications',
                ArticleTypeName: 'CONCEPTUAL ANALYSIS article'
            };
    })();
    </script>
    <!-- BEGIN TODO UXD-1800: Inconsistent sizing of equations -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          CommonHTML: {
            scale: 80
          },
          PreviewHTML: {
            scale: 80
          }
        });
    </script>
    <!-- END TODO UXD-1800: Inconsistent sizing of equations -->

    <script src="https://static.frontiersin.org/areas/articles/js/schema?v=13am1aL5JpCh_IIFOIFRdlFySxVv16JLaVykDEyHCu81"></script>


    
    <script src="https://static.frontiersin.org/areas/articles/js/app?v=YjUBWqiD48g3sYWHcKsl6wgD48UoAIPxorLdY2NDA1k1"></script>

    <script src="https://static.frontiersin.org/areas/articles/js/webcomponents?v=4s0o0wUdD4vuVOucEIlhny5_p7_CfPfRGJsHYUuYXHE1"></script>


    <script type="text/plain" class="optanon-category-C0004" src="https://widgets.figshare.com/static/figshare.js">
    </script>

</body>
</html>
