<!doctype html>
<html data-n-head-ssr lang="zh" data-n-head="%7B%22lang%22:%7B%22ssr%22:%22zh%22%7D%7D">
  <head >
    <title>CV大模型系列之：全面解读VIT，它到底给植树人挖了多少坑 - 掘金</title><meta data-n-head="ssr" charset="utf-8"><meta data-n-head="ssr" name="viewport" content="width=device-width, initial-scale=1, user-scalable=no, viewport-fit=cover"><meta data-n-head="ssr" name="apple-itunes-app" content="app-id=987739104"><meta data-n-head="ssr" name="theme-color" content="#ffffff"><meta data-n-head="ssr" name="msapplication-TileColor" content="#da532c"><meta data-n-head="ssr" vmid="description" name="description" content="全面图解CV大模型开山之作: Vision Transformer（ViT），带领大家阅读VIT中的细节和VIT超越图像分类本身的意义。"><meta data-n-head="ssr" vmid="keywords" name="keywords" content="人工智能,计算机视觉,AIGC"><link data-n-head="ssr" rel="preconnect" href="//unpkg.byted-static.com/" crossorigin="anonymous"><link data-n-head="ssr" rel="preconnect" href="//lf3-cdn-tos.bytescm.com" crossorigin="anonymous"><link data-n-head="ssr" rel="preconnect" href="//mcs.snssdk.com" crossorigin="anonymous"><link data-n-head="ssr" rel="preconnect" href="//i.snssdk.com" crossorigin="anonymous"><link data-n-head="ssr" rel="dns-prefetch" href="//lf3-short.ibytedapm.com"><link data-n-head="ssr" rel="dns-prefetch" href="//lf3-cdn-tos.bytescm.com"><link data-n-head="ssr" rel="dns-prefetch" href="//api.juejin.cn"><link data-n-head="ssr" rel="dns-prefetch" href="//lf-cdn-tos.bytescm.com"><link data-n-head="ssr" rel="dns-prefetch" href="//unpkg.byted-static.com"><link data-n-head="ssr" rel="dns-prefetch" href="//p1-juejin.byteimg.com"><link data-n-head="ssr" rel="dns-prefetch" href="//p3-juejin.byteimg.com"><link data-n-head="ssr" rel="dns-prefetch" href="//p6-juejin.byteimg.com"><link data-n-head="ssr" rel="dns-prefetch" href="//p9-juejin.byteimg.com"><link data-n-head="ssr" rel="dns-prefetch" href="//p1-jj.byteimg.com"><link data-n-head="ssr" rel="dns-prefetch" href="//p2-jj.byteimg.com"><link data-n-head="ssr" rel="dns-prefetch" href="//p6-jj.byteimg.com"><link data-n-head="ssr" rel="dns-prefetch" href="//p9-jj.byteimg.com"><link data-n-head="ssr" rel="dns-prefetch" href="//mcs.snssdk.com"><link data-n-head="ssr" rel="dns-prefetch" href="//i.snssdk.com"><link data-n-head="ssr" rel="apple-touch-icon" sizes="180x180" href="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/favicons/apple-touch-icon.png"><link data-n-head="ssr" rel="icon" type="image/png" sizes="32x32" href="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/favicons/favicon-32x32.png"><link data-n-head="ssr" rel="icon" type="image/png" sizes="16x16" href="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/favicons/favicon-16x16.png"><link data-n-head="ssr" rel="mask-icon" href="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/favicons/safari-pinned-tab.svg" color="#1E80FF"><link data-n-head="ssr" rel="manifest" href="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/favicons/site.webmanifest"><link data-n-head="ssr" rel="search" title="掘金" href="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/search.xml" type="application/opensearchdescription+xml"><link data-n-head="ssr" rel="stylesheet" href="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/bytedesign.min.css"><link data-n-head="ssr" rel="canonical" href="https://juejin.cn/post/7254341178258489404"><script data-n-head="ssr" type="text/javascript" data-sdk-glue-default="load" src="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/rc-client-security-web-glue/1.0.0.19/sdk-glue.js"></script><script data-n-head="ssr" type="text/javascript" data-sdk-glue-default="init">
        (function (){
          var options = {
            bdms: {
              aid: 2608,
              paths: [
                '/growth_api/v1/publish_benefit_history',
                '/growth_api/v1/check_in',
                '/growth_api/v1/lottery/draw',
                '/growth_api/v1/lottery/ten_draw',
                '/web_shorten',
                '/user_api/v1/user/get',
                '/interact_api/v1/digg/save',
                '/interact_api/v1/digg/query_page',
                '/interact_api/v1/comment/list',
                '/interact_api/v1/comment/hots',
                '/content_api/v1/article/detail',
                '/user_api/v1/follow/followees',
                '/user_api/v1/follow/followers',
                '/interact_api/v1/follow/tag_list',
                '/recommend_api/v1/article/recommend_cate_feed',
              ]
            },
            verifyCenter: {
              interceptPathList: [
                '/user_api/v1/user/get',
                '/interact_api/v1/digg/save',
                '/interact_api/v1/digg/query_page',
                '/interact_api/v1/comment/list',
                '/interact_api/v1/comment/hots',
                '/content_api/v1/article/detail',
                '/user_api/v1/follow/followees',
                '/user_api/v1/follow/followers',
                '/interact_api/v1/follow/tag_list',
                '/recommend_api/v1/article/recommend_cate_feed',
              ],
              commonOptions: {
                aid: 2608,
              },
              captchaOptions: {
                showMode: 'mask',
              },
            }
          }
          window._SdkGlueInit(options)
        })();
        </script><script data-n-head="ssr" vmid="slardar" type="text/javascript" crossorigin="anonymous">;(function (w, d, u, b, n, pc, ga, ae, po, s, p, e, t, pp) {pc = 'precollect';ga = 'getAttribute';ae = 'addEventListener';po = 'PerformanceObserver';s = function (m) {p = [].slice.call(arguments);p.push(Date.now(), location.href);(m == pc ? s.p.a : s.q).push(p)};s.q = [];s.p = { a: [] };w[n] = s;e = document.createElement('script');e.src = u + '?bid=' + b + '&globalName=' + n;e.crossOrigin = u.indexOf('sdk-web') > 0 ? 'anonymous' : 'use-credentials';d.getElementsByTagName('head')[0].appendChild(e);if (ae in w) {s.pcErr = function (e) {e = e || w.event;t = e.target || e.srcElement;if (t instanceof Element || t instanceof HTMLElement) {if (t[ga]('integrity')) {w[n](pc, 'sri', t[ga]('href') || t[ga]('src'))} else {w[n](pc, 'st', { tagName: t.tagName, url: t[ga]('href') || t[ga]('src') })}} else {w[n](pc, 'err', e.error || e.message)}};s.pcRej = function (e) {e = e || w.event;w[n](pc, 'err', e.reason || (e.detail && e.detail.reason))};w[ae]('error', s.pcErr, true);w[ae]('unhandledrejection', s.pcRej, true);};if('PerformanceLongTaskTiming' in w) {pp = s.pp = { entries: [] };pp.observer = new PerformanceObserver(function (l) {pp.entries = pp.entries.concat(l.getEntries())});pp.observer.observe({ entryTypes: ['longtask', 'largest-contentful-paint','layout-shift'] })}})(window,document,'https://lf3-short.ibytedapm.com/slardar/fe/sdk-web/browser.cn.js','2608','SlardarWeb')</script><script data-n-head="ssr" type="text/javascript" src="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/slardar-plugin/imageReport.js"></script><script data-n-head="ssr" type="text/javascript" src="https://lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/static/cdn-retry/bundle-dcf007.js" id="cdn-retry" defer></script><script data-n-head="ssr" type="application/ld+json">[{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://juejin.cn/post/7254341178258489404"},"headline":"CV大模型系列之：全面解读VIT，它到底给植树人挖了多少坑","description":"全面图解CV大模型开山之作: Vision Transformer（ViT），带领大家阅读VIT中的细节和VIT超越图像分类本身的意义。","image":["https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/71b5ff03cf154a65b1906fee95a35e70~tplv-k3u1fbpfcp-watermark.image?"],"author":{"@type":"Organization","name":"猛猿"},"publisher":{"@type":"Organization","name":"掘金","logo":{"@type":"ImageObject","url":"//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/e08da34488b114bd4c665ba2fa520a31.svg"}},"datePublished":"2023-07-11T05:03:08+00:00","dateModified":"2023-09-07T06:01:37+00:00"},{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","name":"稀土掘金","position":1,"item":"https://juejin.cn"},{"@type":"ListItem","name":"人工智能","position":2,"item":"https://juejin.cn/ai"},{"@type":"ListItem","name":"文章","position":3}]}]</script><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/e28e478.js" as="script"><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/60514e1.js" as="script"><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/643346e.js" as="script"><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/app.5fe4196.css" as="style"><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/adfe0b1.js" as="script"><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/layouts/default.846c2b6.css" as="style"><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/a99f4f3.js" as="script"><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/2f4bee9.js" as="script"><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/18.7b3f598.css" as="style"><link rel="preload" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/6a32973.js" as="script"><link rel="stylesheet" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/app.5fe4196.css"><link rel="stylesheet" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/layouts/default.846c2b6.css"><link rel="stylesheet" href="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/18.7b3f598.css">
  </head>
  <body >
    <script data-n-head="ssr" type="text/javascript" data-pbody="true">(function () {
    const pages = [
        /^\/$/,
        /^\/following$/,
        /^\/recommended$/,
        '^/pins.*',
        '^/pin.*',
        /^\/course(?!\/payment\/)/,
        /^\/post\/.*/,
        '^/hot.*',
        /^\/book\/\d+/,
        /^\/video\/\d+/,
        /^\/user\/settings.*/,
        /^\/spost\/\d+/,
        /^\/notification(?!\/im)/,
        '^/backend',
        '^/frontend',
        '^/android',
        '^/ios',
        '^/ai',
        '^/freebie',
        '^/career',
        '^/article',
    ];
    function isInJuejinApp() {
        const userAgent = typeof navigator !== 'undefined' ? navigator.userAgent : '';
        return /juejin/i.test(userAgent);
    }
    if (typeof window !== 'undefined' && !isInJuejinApp()) {
        try {
            const path = window.location.pathname;
            const isAvailable = pages.some((page) => {
                const reg = new RegExp(page);
                return reg.test(path);
            });
            if (isAvailable) {
                const localValue = localStorage.getItem('juejin_2608_theme') || '{}';
                let { theme = 'light', isFollowSystem = false } = JSON.parse(localValue);
                if (isFollowSystem) {
                    const themeMedia = window.matchMedia('(prefers-color-scheme: light)');
                    theme = themeMedia.matches ? 'light' : 'dark';
                    localStorage.setItem('juejin_2608_theme', JSON.stringify({ theme, isFollowSystem }));
                }
                document.body.setAttribute('data-theme', theme);
            }
            else {
                document.body.setAttribute('data-theme', 'light');
            }
        }
        catch (e) {
            console.error('浏览器不支持localStorage');
        }
    }
})()</script><div data-server-rendered="true" id="__nuxt"><div id="__layout"><div id="juejin"><div class="view-container" data-v-5762947c data-v-5b3ac010><div class="main-header-box" data-v-5762947c><header data-fetch-key="0" class="main-header main-header unauthorized visible" data-v-925171a8 data-v-5762947c><div class="container" data-v-925171a8><a href="/" class="logo" data-v-925171a8><img src="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/e08da34488b114bd4c665ba2fa520a31.svg" alt="稀土掘金" class="logo-img" data-v-925171a8> <img src="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/6c61ae65d1c41ae8221a670fa32d05aa.svg" alt="稀土掘金" class="mobile" data-v-925171a8></a> <!----> <nav role="navigation" class="main-nav" data-v-925171a8><ul class="nav-list" data-v-925171a8><!----> <li class="main-nav-list" data-v-925171a8><div class="phone-show-menu" data-v-925171a8><span data-v-925171a8>首页</span> <svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg" class="unfold16-icon" data-v-925171a8 data-v-925171a8><path d="M2.45025 4.82431C2.17422 4.49957 2.40501 4.00049 2.83122 4.00049H9.16878C9.59498 4.00049 9.82578 4.49957 9.54975 4.82431L6.38097 8.55229C6.1813 8.78719 5.8187 8.78719 5.61903 8.55229L2.45025 4.82431Z" data-v-925171a8 data-v-925171a8></path></svg></div> <ul class="phone-hide" data-v-925171a8><li class="nav-item link-item" data-v-925171a8><a href="/" data-v-925171a8>首页</a></li> <li class="nav-item link-item activities" data-v-925171a8><a href="/pins" data-v-925171a8>
                沸点
                <span class="text" data-v-925171a8><!----></span></a></li> <li class="nav-item link-item book" data-v-925171a8><a href="/course" data-v-925171a8>
                课程
                <!----></a></li> <li class="nav-item link-item" data-v-925171a8><a href="/live" data-v-925171a8>
                直播
              </a></li> <li class="nav-item link-item" data-v-925171a8><a href="/events/all" data-v-925171a8>活动</a></li> <li class="nav-item link-item" data-v-925171a8><a href="/challenge" data-v-925171a8>
                竞赛
                <!----></a></li> <nav class="nav-item link-item" data-v-925171a8><a href="https://detail.youzan.com/show/goods/newest?kdt_id=104340304" target="_blank" rel="nofollow noopener noreferrer" class="jj-link nav-item link-item no-border" data-v-04ff4e9e data-v-925171a8><span data-v-04ff4e9e data-v-925171a8>商城</span></a> <!----></nav> <nav class="nav-item link-item download-icon" data-v-925171a8><a href="/app?utm_source=jj_nav" target="_blank" class="download-app no-border" data-v-925171a8>
                APP
              </a></nav> <nav class="nav-item link-item extension-icon" data-v-925171a8><a href="https://juejin.cn/extension?utm_source=jj_nav" target="_blank" rel="nofollow noopener noreferrer" class="jj-link broswer-extension no-border" data-v-04ff4e9e data-v-925171a8><span data-v-04ff4e9e data-v-925171a8>插件</span></a></nav> <!----></ul></li> <ul class="right-side-nav" data-v-925171a8><li class="search-add" data-v-925171a8><ul class="search-add-ul" data-v-925171a8><li class="nav-item search" data-v-925171a8><form role="search" class="search-form" data-v-925171a8><input type="search" maxlength="64" placeholder="" value="" class="search-input" data-v-925171a8> <div class="seach-icon-container" data-v-925171a8><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg" class="search-icon" data-v-925171a8 data-v-925171a8><path d="M12.4008 12.4008C14.744 10.0577 14.744 6.25871 12.4008 3.91556C10.0577 1.57242 6.25871 1.57242 3.91556 3.91556C1.57242 6.25871 1.57242 10.0577 3.91556 12.4008C6.25871 14.744 10.0577 14.744 12.4008 12.4008ZM12.4008 12.4008L15.5828 15.5828" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" data-v-925171a8 data-v-925171a8></path></svg></div> <!----> <div class="typehead" style="display:none;" data-v-925171a8><!----> <div class="title" data-v-925171a8><span data-v-925171a8>搜索历史</span> <span class="clear" data-v-925171a8>
                        清空
                      </span></div> <div class="list" data-v-925171a8></div></div></form></li> <li class="nav-item add creator-item" data-v-925171a8><div class="add-group" data-v-17cc1c49 data-v-925171a8><!----> <button class="add-btn" data-v-17cc1c49>
    创作者中心
  </button> <div class="more" data-v-17cc1c49><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg" class="unfold12-icon" data-v-17cc1c49 data-v-17cc1c49><path d="M2.45025 4.82383C2.17422 4.49908 2.40501 4 2.83122 4H9.16878C9.59499 4 9.82578 4.49908 9.54975 4.82382L6.38097 8.5518C6.1813 8.7867 5.8187 8.7867 5.61903 8.5518L2.45025 4.82383Z" fill="white" data-v-17cc1c49 data-v-17cc1c49></path></svg> <div class="more-mask" data-v-17cc1c49></div> <div class="more-list" data-v-17cc1c49><ul class="menu" data-v-17cc1c49><li class="item" data-v-17cc1c49><div class="icon write-article" data-v-17cc1c49></div> <div class="title" data-v-17cc1c49>写文章</div></li><li class="item" data-v-17cc1c49><div class="icon issue-points" data-v-17cc1c49></div> <div class="title" data-v-17cc1c49>发沸点</div></li><li class="item" data-v-17cc1c49><div class="icon write-note" data-v-17cc1c49></div> <div class="title" data-v-17cc1c49>写笔记</div></li><li class="item" data-v-17cc1c49><div class="icon create-jcode" data-v-17cc1c49></div> <div class="title" data-v-17cc1c49>写代码</div></li><li class="item" data-v-17cc1c49><div class="icon drafts" data-v-17cc1c49></div> <div class="title" data-v-17cc1c49>草稿箱</div></li></ul> <div class="divider" data-v-17cc1c49></div> <div class="inspiration" data-v-17cc1c49><div class="info" data-v-17cc1c49><span class="title" data-v-17cc1c49>创作灵感</span> <span class="more-info" data-v-17cc1c49>
            查看更多
            <i class="icon byte-icon byte-icon--right" data-v-17cc1c49><svg t="1561636167146" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="404349" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M630.4 512L283.52 165.12a21.12 21.12 0 0 1 0-30.08l30.08-30.08a21.12 21.12 0 0 1 30.08 0l377.6 376.96a42.24 42.24 0 0 1 0 60.16l-377.6 376.96a21.12 21.12 0 0 1-30.08 0l-30.08-30.08a21.12 21.12 0 0 1 0-30.08z" p-id="404350"></path></svg></i></span></div> <div class="list" data-v-17cc1c49>  <div class="item" data-v-090682e2 data-v-17cc1c49><div class="xitu-skeleton xitu-skeleton-animated" data-v-090682e2><div class="xitu-skeleton-item" data-v-090682e2><!----> <div class="xitu-skeleton-content" data-v-090682e2><div class="xitu-skeleton-line" data-v-090682e2></div><div class="xitu-skeleton-line" data-v-090682e2></div><div class="xitu-skeleton-line" data-v-090682e2></div></div></div></div></div></div></div></div></div> <!----></div></li></ul></li> <li class="nav-item vip-entry" data-v-925171a8><div class="vip-title" data-v-925171a8><div class="vip-entry-img" data-v-925171a8><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ffd3e238ee7f46eab42bf88af17f5528~tplv-k3u1fbpfcp-image.image#?w=25&h=26&s=5968&e=svg&a=1&b=dacbbc" alt="vip" class="vip-img" data-v-925171a8> <!----></div> <div class="vip-words" data-v-925171a8>会员</div></div></li> <!----> <!----> <!----> <li class="nav-item auth hidden" data-v-925171a8><div class="login-button-wrap" data-v-925171a8><button class="login-button" data-v-925171a8>
                登录
                <div class="login-button-inner" data-v-925171a8><div class="login-button-line" data-v-925171a8></div>
                  注册
                </div></button> <!----></div></li></ul></ul></nav></div></header></div>  <main class="container main-container" style="max-width:1140px;" data-v-5762947c><div class="view column-view" data-v-5762947c data-v-5b3ac010><!----> <div class="main-area article-area" data-v-5762947c data-v-5b3ac010><article itemscope="itemscope" itemtype="http://schema.org/Article" data-entry-id="7254341178258489404" data-draft-id="7254361990075400229" data-original-type="0" class="article" data-v-5b3ac010><!----> <meta itemprop="headline" content="CV大模型系列之：全面解读VIT，它到底给植树人挖了多少坑"> <meta itemprop="keywords" content="人工智能,计算机视觉,AIGC"> <meta itemprop="datePublished" content="2023-07-11T05:03:08.000Z"> <meta itemprop="image" content="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-assets/icon/icon-128.png~tplv-t2oaga2asx-image.image"> <div itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><meta itemprop="name" content="猛猿"> <meta itemprop="url" content="https://juejin.cn/user/970190899118414"></div> <div itemprop="publisher" itemscope="itemscope" itemtype="http://schema.org/Organization"><meta itemprop="name" content="掘金"> <div itemprop="logo" itemscope="itemscope" itemtype="https://schema.org/ImageObject"><meta itemprop="url" content="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-assets/icon/icon-white-180.png~tplv-t2oaga2asx-image.image"> <meta itemprop="width" content="180"> <meta itemprop="height" content="180"></div></div> <h1 class="article-title" data-v-5b3ac010>
            CV大模型系列之：全面解读VIT，它到底给植树人挖了多少坑
            <!----> <!----></h1> <div class="author-info-block block-hidden" data-v-5b3ac010><div class="author-info-box" data-v-5b3ac010><div class="author-name" data-v-5b3ac010><a href="/user/970190899118414/posts" target="_blank" rel="" class="jj-link username username ellipsis" data-v-04ff4e9e data-v-1800aadb data-v-5b3ac010><span class="name" style="max-width:160px;" data-v-04ff4e9e data-v-1800aadb>
    猛猿
  </span> <!----> <!----> <!----> </a></div> <div class="meta-box" data-v-5b3ac010><time datetime="2023-07-11T05:03:08.000Z" title="Tue Jul 11 2023 05:03:08 GMT+0000 (Coordinated Universal Time)" class="time" data-v-5b3ac010>
                    2023-07-11
                  </time> <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg" class="read-icon" data-v-5b3ac010><path d="M7.90078 2.80078C4.49278 2.80078 1.74745 6.11672 0.800781 7.77469C1.74745 9.58339 4.49278 13.2008 7.90078 13.2008C11.3088 13.2008 14.0541 9.58339 15.0008 7.77469C14.0541 6.11672 11.3088 2.80078 7.90078 2.80078Z" stroke="currentColor" data-v-5b3ac010></path><circle cx="7.89922" cy="8.00078" r="2.2" stroke="currentColor" data-v-5b3ac010></circle></svg> <span class="views-count" data-v-5b3ac010>
                    2,446
                  </span> <!----> <!----></div></div> <div style="flex:1;" data-v-5b3ac010></div> <!----> <!----></div> <!----> <!----> <div id="article-root" itemprop="articleBody" class="article-viewer markdown-body cache result" data-v-5b3ac010><style>.markdown-body{word-break:break-word;line-height:1.75;font-weight:400;font-size:15px;overflow-x:hidden;color:#333}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{line-height:1.5;margin:30px 0 10px;color:#cca152;position:relative;padding-left:50px;border-bottom:2px solid rgba(209,163,78,.6);padding-bottom:0}.markdown-body h1{font-size:30px}.markdown-body h1:before{content:"";width:50px;height:42px;display:block;position:absolute;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACcAAAAhBAMAAACo1K8bAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAFZaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIj4KICAgICAgICAgPHRpZmY6T3JpZW50YXRpb24+MTwvdGlmZjpPcmllbnRhdGlvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkzCJ1kAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAnUExURUdwTMyhUsqkUMyhUsyiUcyhUsyhUsyhUcyhUsyhUs2hUtytWNWoVX44si8AAAAKdFJOUwD8Bfcge95QncCWSSDAAAABh0lEQVQoz2WSPUvDQBjHjyPSOB7drh2OUOs36NA6pKAFwaEo4tDFIUWxGYLgKw4BEYUuHaQg7XJ0kccsUmrT2qUUQZp8KO/y1hSfIYEf/5eHu0MoGU1+sIbSw5Bypd92S+dWWrd34b15vuOPY4QxuvtYZp0lIVk3Zgjt+5zkh12AvBHnbW85BOjXySPwhc5CYcY0OdDh5dEUqIEj4cN8Dty3a1MqYJS4MaWU5wzVLwMMxqGSNQgA/VFa4gf50yhxSUW+db8ACa2gBxfnABVDFYHCPYrc7TLwCepJM8/ZoVsRwpxdHEozHUXd6idwVzFFM5BFmIhQVQjrNdnCvdd48wblI2VHtsAZioSsTYVwLoXvjMXH1icuNgPhQE+Ot1+xi8HeA3d1Df1f1JPVUOmsYLujBjuSySqSXYt+yTwbJ0pcyIhqTrxkn0BazRLizJosxRBued+z0jPD6VewOXl5utHRGmMNK3k0iTkzxpq2/oIQO6gLprF12kT/R20eyzlcg7iwK0dPsz/ibpdsCHweWwAAAABJRU5ErkJggg==) 0 0 no-repeat;background-size:80%;bottom:-10px;left:-2px}.markdown-body h2{font-size:24px}.markdown-body h2:before{content:"";width:50px;height:42px;display:block;position:absolute;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACcAAAAhBAMAAACo1K8bAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAFZaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIj4KICAgICAgICAgPHRpZmY6T3JpZW50YXRpb24+MTwvdGlmZjpPcmllbnRhdGlvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkzCJ1kAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAnUExURUdwTMyhUsqkUMyhUsyiUcyhUsyhUsyhUcyhUsyhUs2hUtytWNWoVX44si8AAAAKdFJOUwD8Bfcge95QncCWSSDAAAABh0lEQVQoz2WSPUvDQBjHjyPSOB7drh2OUOs36NA6pKAFwaEo4tDFIUWxGYLgKw4BEYUuHaQg7XJ0kccsUmrT2qUUQZp8KO/y1hSfIYEf/5eHu0MoGU1+sIbSw5Bypd92S+dWWrd34b15vuOPY4QxuvtYZp0lIVk3Zgjt+5zkh12AvBHnbW85BOjXySPwhc5CYcY0OdDh5dEUqIEj4cN8Dty3a1MqYJS4MaWU5wzVLwMMxqGSNQgA/VFa4gf50yhxSUW+db8ACa2gBxfnABVDFYHCPYrc7TLwCepJM8/ZoVsRwpxdHEozHUXd6idwVzFFM5BFmIhQVQjrNdnCvdd48wblI2VHtsAZioSsTYVwLoXvjMXH1icuNgPhQE+Ot1+xi8HeA3d1Df1f1JPVUOmsYLujBjuSySqSXYt+yTwbJ0pcyIhqTrxkn0BazRLizJosxRBued+z0jPD6VewOXl5utHRGmMNK3k0iTkzxpq2/oIQO6gLprF12kT/R20eyzlcg7iwK0dPsz/ibpdsCHweWwAAAABJRU5ErkJggg==) 0 0 no-repeat;background-size:70%;bottom:-15px;left:-1px}.markdown-body h3{font-size:18px}.markdown-body h3:before{content:"";width:50px;height:42px;display:block;position:absolute;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACcAAAAhBAMAAACo1K8bAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAFZaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIj4KICAgICAgICAgPHRpZmY6T3JpZW50YXRpb24+MTwvdGlmZjpPcmllbnRhdGlvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkzCJ1kAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAnUExURUdwTMyhUsqkUMyhUsyiUcyhUsyhUsyhUcyhUsyhUs2hUtytWNWoVX44si8AAAAKdFJOUwD8Bfcge95QncCWSSDAAAABh0lEQVQoz2WSPUvDQBjHjyPSOB7drh2OUOs36NA6pKAFwaEo4tDFIUWxGYLgKw4BEYUuHaQg7XJ0kccsUmrT2qUUQZp8KO/y1hSfIYEf/5eHu0MoGU1+sIbSw5Bypd92S+dWWrd34b15vuOPY4QxuvtYZp0lIVk3Zgjt+5zkh12AvBHnbW85BOjXySPwhc5CYcY0OdDh5dEUqIEj4cN8Dty3a1MqYJS4MaWU5wzVLwMMxqGSNQgA/VFa4gf50yhxSUW+db8ACa2gBxfnABVDFYHCPYrc7TLwCepJM8/ZoVsRwpxdHEozHUXd6idwVzFFM5BFmIhQVQjrNdnCvdd48wblI2VHtsAZioSsTYVwLoXvjMXH1icuNgPhQE+Ot1+xi8HeA3d1Df1f1JPVUOmsYLujBjuSySqSXYt+yTwbJ0pcyIhqTrxkn0BazRLizJosxRBued+z0jPD6VewOXl5utHRGmMNK3k0iTkzxpq2/oIQO6gLprF12kT/R20eyzlcg7iwK0dPsz/ibpdsCHweWwAAAABJRU5ErkJggg==) 0 0 no-repeat;background-size:60%;bottom:-19px;left:-2px}.markdown-body h4{font-size:16px;padding-left:0;border-bottom:1px solid rgba(209,163,78,.6)}.markdown-body h5{font-size:15px;padding-left:0}.markdown-body em{color:#cca152}.markdown-body del{text-decoration-color:#cca152;text-decoration-thickness:2px}.markdown-body p{line-height:inherit;margin-top:22px;margin-bottom:22px}.markdown-body img{max-width:80%;margin:6px auto;box-shadow:0 6px 15px #8e8e8e;display:block;margin:20px auto!important;object-fit:contain;border-radius:8px}.markdown-body hr{border:none;border-top:2px solid #e0c9a1;margin-top:32px 0}.markdown-body code{word-break:break-word;border-radius:2px;overflow-x:auto;background:#f6efde;color:#b69454;font-size:.87em;padding:.065em .4em}.markdown-body code,.markdown-body pre{font-family:Mono,Menlo,Monaco,Consolas,Courier New,monospace}.markdown-body pre{overflow:auto;position:relative;line-height:1.75;background:#fef6e1;border-radius:4px;box-shadow:0 0 8px hsla(0,0%,47%,.45)}.markdown-body pre:before{content:"";display:block;height:30px;width:100%;margin-bottom:-7px;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAOCAMAAABaWb9VAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAFZaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIj4KICAgICAgICAgPHRpZmY6T3JpZW50YXRpb24+MTwvdGlmZjpPcmllbnRhdGlvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkzCJ1kAAAAJcEhZcwAADsQAAA7EAZUrDhsAAACHUExURUdwTMxCKdc8NvdYT/9hVyLJPABBAGubKNiIOv+/K+ytJBakH9aZG+5QR5VZDOBDPhmtJ8+VGuGjH/CwJR26MR6+NBq0LPW1KBmwKetNRfJUTONHP92fHuSmIeFEPhu3LR29M/BTS+mqIuqsI5qdHyLKPP++Kv9gVf9lW//KLSXXQCTQP//FLMVm5KQAAAAldFJOUwAlPPz7+woBBPu8Mzu+FlRRKmvnweeG+Wqg6mVNXmuc1OCbmjZJWF/9AAABKklEQVQoz3WS6XaCMBCFRzAM++KGsqhtDwES3//5OtmA2uP9A9zwzRoAgBC0QgQrdA68OZsDr229YGHoIEj7Pl0ZOgjKa5lYJ4Rd1vijn3nuD4Qurjmv48o6RFyfbGDnS6DeEXZfk5ZfuBhdPb9I87ECNMh1EFJKIU6agWzaa01NbmLkxznSmmObJBkkUxrERf3i+ftRiZi7ShPCYY64UhTx1AR5CDYoMXkOKEY7GmTcTzeD/FiER68DfSLgSRqEIBoB3P8h3x8RZhBXGJGusJdD6rfCBl0YqvZNkrV9zej2cWlfJWG6fRpyM41qYH+GTPPi2yFLQYC0Qybm5o96lW77kMbcrBKtgeWTsthVamNXFF4I6x0DTPuugsVRFyYpyxzWqNvHJwed8ws7QyP1UwjNjwAAAABJRU5ErkJggg==) 10px 10px no-repeat;background-size:40px}.markdown-body pre>code{font-size:12px;padding:15px 12px;margin:0;word-break:normal;display:block;overflow-x:auto;color:#333;background:#fef6e1}.markdown-body a{text-decoration:none;color:#d8ac5a;border-bottom:1px solid #d8ac5a}.markdown-body a:active,.markdown-body a:hover{color:#93753f;border-color:#93753f}.markdown-body table{display:inline-block!important;font-size:12px;width:auto;max-width:100%;overflow:auto;border:1px solid #f4e8c7;border-collapse:collapse}.markdown-body thead{background:rgba(255,227,176,.6588235294);text-align:left;display:table-header-group;border-bottom:1px solid rgba(255,227,176,.6588235294)}.markdown-body tbody{background:rgba(255,247,229,.3882352941)}.markdown-body td,.markdown-body th{padding:7px;line-height:24px;min-width:100px}.markdown-body blockquote{color:#bd954f;padding:1px 23px;margin:22px 0;border-left:4px solid #dcb267;background-color:#fff7e5}.markdown-body blockquote:after{display:block;content:""}.markdown-body blockquote>p{margin:10px 0}.markdown-body ol,.markdown-body ul{padding-left:28px}.markdown-body ol .task-list-item,.markdown-body ul .task-list-item{list-style:none}.markdown-body ol li{padding-left:6px}.markdown-body::marker{color:#dcb267}.markdown-body .contains-task-list{padding-left:0}.markdown-body .contains-task-list .task-list-item{list-style:none;position:relative}.markdown-body .contains-task-list .task-list-item input[type=checkbox]{position:relative;top:2px}.markdown-body .contains-task-list .task-list-item input[type=checkbox]:before{content:"";display:inline-block;height:12px;width:12px;position:absolute;left:-2px;top:-2px;border:2px solid #cda152;border-radius:5px}.markdown-body .contains-task-list .task-list-item input[type=checkbox]:checked{position:relative;top:2px}.markdown-body .contains-task-list .task-list-item input[type=checkbox]:checked:before{border:none;content:"";display:inline-block;height:17px;width:17px;position:absolute;left:-2px;top:-2px;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAFo9M/3AAAAAXNSR0IArs4c6QAAAYhJREFUOBFjYACC0/MDGsDEiyvr/zOCeUwMJxn/A8HZRUEMYBFGJsZ6kFoQYALiAyAGCgDpA+sFijKeWRj4H1kWpIWBW1SDQcm+BCzOAiK/vr7BcO/gDbAAI4hE1waWgRBnmWCGIwkyKFjnwow0BhsJk9QLncvw5dV1oPE9MCEGmBWrgCKhcFEowyR+PSNYwemFAZ4M/xjMkRWYJm5oAPFB/joDpI1BHHQANgGPDxj+//vfCA4IdJ3GcevgQnAFhlHLwYIgSVA0wABcwY3tFQzokiBFGIEP0wmigW5wZAK5FFkQib0a6NUDcEmgb7AGFpIGZOZqoMFhIAFQknEAJpn9yLLEssFOBCp2IFaDkl0xg6C8FbJyB3gowUS5RdQYBORQYpVB1iwVHIIMwJh///AYTCmYRklNIBFmNi4GeYt0BhnjeIa3dw8wSBlEgDUhxw2yCRgGfHp2geHiqiSwUwUVrFAiFVkjjA1LrjgTHEwhFvosMCZM4NEIUoAtWWNoBGZ70/gN22HiAD2uhAzsYNBZAAAAAElFTkSuQmCC) 0 0 no-repeat;background-size:100%}@media (max-width:720px){.markdown-body h1{font-size:24px}.markdown-body h2{font-size:20px}.markdown-body h3{font-size:18px}}</style><style data-highlight>.markdown-body pre,.markdown-body pre>code.hljs{color:#333;background:#f8f8f8}.hljs-comment,.hljs-quote{color:#998;font-style:italic}.hljs-keyword,.hljs-selector-tag,.hljs-subst{color:#333;font-weight:700}.hljs-literal,.hljs-number,.hljs-tag .hljs-attr,.hljs-template-variable,.hljs-variable{color:teal}.hljs-doctag,.hljs-string{color:#d14}.hljs-section,.hljs-selector-id,.hljs-title{color:#900;font-weight:700}.hljs-subst{font-weight:400}.hljs-class .hljs-title,.hljs-type{color:#458;font-weight:700}.hljs-attribute,.hljs-name,.hljs-tag{color:navy;font-weight:400}.hljs-link,.hljs-regexp{color:#009926}.hljs-bullet,.hljs-symbol{color:#990073}.hljs-built_in,.hljs-builtin-name{color:#0086b3}.hljs-meta{color:#999;font-weight:700}.hljs-deletion{background:#fdd}.hljs-addition{background:#dfd}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}</style><blockquote>
<p><strong>⚠️⚠️⚠️本文为稀土掘金技术社区首发签约文章，30天内禁止转载，30天后未获授权禁止转载，侵权必究！</strong></p>
</blockquote>
<p>大家好，最近越演越热的AIGC浪潮，将Transformer这个模型带进了大家的视野。如果你从事NLP相关的职业，你一定知道Bert（来自Transformer的Encoder）在5年前卷起的NLP预训练的热潮。而在当时因为受到训练数据和算力的限制，以GPT（来自Transformer的Decoder）为代表的生成式自然语言模型，还没有展现出惊人的涌现能力，但它却启发了人们对“自监督训练”的不懈研究，以此来解决“自然语言训练中标注数据不足”的问题。时间来到2020年，此时还是Bert独占鳌头，在Transformer Encoder架构的启发下，CV算法工程师们开始思考一个问题：”当CNN的架构快做到极致时，我们能否换一个新方向？”于是这一年，Google推出了VIT（Vision Transformer）：<strong>一个和Bert几乎一致，同时不添加任何卷积结构的图像分类模型</strong>。VIT在Transformer上的成功，<strong>证明了可以用统一的模型，来处理不同领域（语言/图像/视频）的任务</strong>，进而开启了多模态模型研究的新篇章。VIT作为众多大模型的backbone（骨架结构），是我们在研究AIGC时绕不过的话题。</p>
<p>今天这篇文章，就和大家一起来全面解读VIT。如果大家对Transformer和Bert不了解，强烈建议大家在阅读本文前速读以下2篇文章：</p>
<ul>
<li><a href="https://juejin.cn/post/7241859817563389989" target="_blank" title="https://juejin.cn/post/7241859817563389989">深入浅出Transformer系列之二：Self-attention（attention是Transformer的核心模块）</a></li>
<li><a href="https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F461267517" target="_blank" title="https://zhuanlan.zhihu.com/p/461267517" ref="nofollow noopener noreferrer">深入浅出Bert系列之一：模型原理篇</a></li>
</ul>
<blockquote>
<p>CV大模型系列文章导航（持续更新中）：<br>
🌸<a href="https://juejin.cn/post/7251391372394053691" title="https://juejin.cn/post/7251391372394053691" target="_blank">CV大模型系列之：扩散模型基石DDPM（模型架构篇）</a>🌸<br>
🌸<a href="https://juejin.cn/post/7251399225425494071" title="https://juejin.cn/post/7251399225425494071" target="_blank">CV大模型系列之：扩散模型基石DDPM（人人都能看懂的数学原理篇）</a>🌸<br>
🌸<a href="https://juejin.cn/post/7258069406961352764" title="https://juejin.cn/post/7258069406961352764" target="_blank">CV大模型系列之：扩散模型基石DDPM（源码解读与实操篇）</a>🌸<br>
🌸<a href="https://juejin.cn/post/7254341178258489404" title="https://juejin.cn/post/7254341178258489404" target="_blank">CV大模型系列之：全面解读VIT，它到底给植树人挖了多少坑</a>🌸<br>
🌸<a href="https://juejin.cn/post/7264503343996747830" title="https://juejin.cn/post/7264503343996747830" target="_blank">CV大模型系列之：多模态经典之作CLIP，探索图文结合的奥秘</a>🌸<br>
🌸<a href="https://juejin.cn/post/7267417057438777399" title="https://juejin.cn/post/7267417057438777399" target="_blank">CV大模型系列之：MAE，实现像素级图像重建</a>🌸<br>
🌸<a href="https://juejin.cn/post/7272735633457659938" target="_blank" title="https://juejin.cn/post/7272735633457659938">CV大模型系列之：MoCo v1，利用对比学习在CV任务上做无监督训练</a>🌸 <br>
🌸<a href="https://juejin.cn/post/7275932704533282831" target="_blank" title="https://juejin.cn/post/7275932704533282831">CV大模型系列之：DALLE2，OpenAI文生图代表作解读</a>🌸</p>
</blockquote>
<h1 data-id="heading-0">一、模型架构</h1>
<p>提起一个新模型，我想大家最关心的事就是：它到底长什么样？输入输出是什么？我要怎么用？</p>
<p>所以，我们先来看模型架构。</p>
<h2 data-id="heading-1">1.1 Bert架构</h2>
<p>前面说过，VIT几乎和Bert一致，我们来速扫一下Bert模型：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c37c1df00f454ea78385f77d7a112b36~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<ul>
<li>
<p>input：输入是一条文本。文本中的每个词（token）我们都通过embedding把它表示成了向量的形式。、</p>
</li>
<li>
<p>训练任务：在Bert中，我们同时做2个训练任务：</p>
<ul>
<li><strong>Next Sentence Prediction Model（下一句预测）</strong> ：input中会包含两个句子，这两个句子有50%的概率是真实相连的句子，50%的概率是随机组装在一起的句子。我们在每个input前面增加特殊符<code>&#x3C;cls></code>，这个位置所在的token将会在训练里不断学习整条文本蕴含的信息。最后它将作为“下一句预测”任务的输入向量，该任务是一个二分类模型，输出结果表示两个句子是否真实相连。</li>
<li><strong>Masked Language Model（遮蔽词猜测）</strong> ：在input中，我们会以一定概率随机遮盖掉一些token（<code>&#x3C;mask></code>)，以此来强迫模型通过Bert中的attention结构更好抽取上下文信息，然后在“遮蔽词猜测”任务重，准确地将被覆盖的词猜测出来。</li>
</ul>
</li>
<li>
<p>Bert模型：Transformer的Encoder层。</p>
</li>
</ul>
<p>关于Bert更详细的介绍，可以参考文章开头给出的链接文章。</p>
<h2 data-id="heading-2">1.2 VIT模型架构</h2>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fb49b6e44c104f8197f8fc1eff1f6bb8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>我们先来看左侧部分。</p>
<ul>
<li><strong>Patch</strong>：对于输入图片，首先将它分成几个patch（例如图中分为9个patch），每个patch就类似于NLP中的一个token（具体如何将patch转变为token向量，在下文会细说）。</li>
<li><strong>Position Embedding</strong>：每个patch的位置向量，用于指示对应patch在原始图片中的位置。和Bert一样，这个位置向量是learnable的，而并非原始Transformer中的函数式位置向量。同样，我们会在下文详细讲解这一块。</li>
<li><strong>Input:</strong> 最终传入模型的Input = patching_emebdding + position embedding，同样，在输入最开始，我们也加一个分类符<code>&#x3C;cls></code>，在bert中，这个分类符是作为“下一句预测”中的输入，来判断两个句子是否真实相连。<strong>在VIT中，这个分类符作为分类任务的输入，来判断原始图片中物体的类别</strong>。</li>
</ul>
<p>右侧部分则详细刻画了Transformer Encoder层的架构，它由L块这样的架构组成。图片已刻画得很详细，这里不再赘述。</p>
<p><strong>总结起来，VIT的训练其实就在做一件事</strong>：把图片打成patch，送入Transformer Encoder，然后拿<code>&#x3C;cls></code>对应位置的向量，过一个简单的softmax多分类模型，去预测原始图片中描绘的物体类别即可。</p>
<p>你可能会想：“这个分类任务只用一个简单的softmax，真得能分准吗？”其实，<strong>这就是VIT的精华所在了：VIT的目的不是让这个softmax分类模型强大，而是让这个分类模型的输入强大。这个输入就是Transformer Encoder提炼出来的特征</strong>。分类模型越简单，对特征的要求就越高。</p>
<p><strong>所以为什么说Transformer开启了大一统模型的预训练大门呢？主要原因就在于它对特征的提炼能力——这样我们就可以拿这个特征去做更多有趣的任务了</strong>。这也是VIT能成为后续多模态backbone的主要原因。</p>
<h1 data-id="heading-3">二、从patch到token</h1>
<p>讲完了基本框架，我们现在来看细节。首先我们来看看，图片的patch是怎么变成token embedding的。</p>
<h2 data-id="heading-4">2.1 patch变token的过程</h2>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6304e32b158c4b54b3241ad6bc0997a2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>如图，假设原始图片尺寸大小为：<code>224*224*3</code> (H * W * C)。</p>
<p>现在我们要把它切成小patch，<strong>每个patch的尺寸设为16</strong>（<strong>P=16</strong>），则每个patch下图片的大小为<code>16*16*3</code>。</p>
<p>则容易计算出共有<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>224</mn><mo>∗</mo><mn>224</mn></mrow><mrow><mn>16</mn><mo>∗</mo><mn>16</mn></mrow></mfrac><mtext> </mtext><mo>=</mo><mtext> </mtext><mn>196</mn></mrow><annotation encoding="application/x-tex">\frac{224*224}{16*16} = 196</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span><span class="mbin mtight">∗</span><span class="mord mtight">16</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">224</span><span class="mbin mtight">∗</span><span class="mord mtight">224</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"> </span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord"> 196</span></span></span></span></span>个patch。</p>
<p>不难看出每个patch对应着一个token，将每个patch展平，则得到输入矩阵X，其大小为<code>(196, 768)</code>，也就是每个token是768维。</p>
<p>通过这样的方式，我们成功将图像数据处理成自然语言的向量表达方式。</p>
<p>好，<strong>那么现在问题来了，对于图中每一个</strong> <strong><code>16*16*3</code></strong> <strong>的小方块，我要怎么把它拉平成</strong> <strong><code>1*768</code></strong> <strong>维度的向量呢？</strong></p>
<p>比如说，我先把第一个channel拉成一个向量，然后再往后依次接上第二个channel、第三个channel拉平的向量。但这种办法下，同一个pixel本来是三个channel的值共同表达的，现在变成竖直的向量之后，这三个值的距离反而远了。基于这个原因，你可能会想一些别的拉平方式，但归根究底它们都有一个共同的问题：太规则化，太主观。</p>
<p>所以，<strong>有办法利用模型来做更好的特征提取吗</strong>？当然没问题。VIT中最终采用CNN进行特征提取，具体方案如下：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3008ea3b4de944959d5a6dd5d765a5f9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>采用768个<code>16*16*3</code>尺寸的卷积核，stride=16，padding=0。这样我们就能得到<code>14*14*768</code>大小的特征图。如同所示，特征图中每一个<code>1*1*768</code>大小的子特征图，都是由卷积核对第一块patch做处理而来，因此它就能表示第一块patch的token向量。</p>
<p>【<strong>备注】：</strong></p>
<p>你可能会问，<strong>前面不是说VIT已经摆脱CNN了吗？这里怎么又用卷积了</strong>？由于这一步只是输入预处理阶段，和主体模型没有关系，只要将其试为一致特征提取方法即可，并不影响我们之前的结论。</p>
<h2 data-id="heading-5">2.2 为什么要处理成patch</h2>
<p><strong>你可能想问，为什么一定要先分patch，再从patch转token呢？</strong></p>
<p><strong>第一个原因，是为了减少模型计算量。</strong></p>
<p>在Transformer中，假设输入的序列长度为N，那么经过attention时，计算复杂度就为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^{2})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，因为注意力机制下，每个token都要和包括自己在内的所有token做一次attention score计算。</p>
<p>在VIT中，<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mfrac><mrow><mi>H</mi><mo>∗</mo><mi>W</mi></mrow><msup><mi>P</mi><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">N=\frac{H*W}{P^{2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2173em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>，当patch尺寸P越小时，N越大，此时模型的计算量也就越大。因此，我们需要找到一个合适的P值，来减少计算压力。</p>
<p><strong>第二个原因，是图像数据带有较多的冗余信息。</strong></p>
<p>和语言数据中蕴含的丰富语义不同，像素本身含有大量的冗余信息。比如，相邻的两个像素格子间的取值往往是相似的。因此我们并不需要特别精准的计算粒度（比如把P设为1）。这个特性也是之后MAE，MoCo之类的像素级预测模型能够成功的原因之一。</p>
<h1 data-id="heading-6">三、Emebdding</h1>
<p>如下图，我们知道在Bert（及其它NLP任务中）：</p>
<p>输入 = <strong>token_embedding</strong>(将单个词转变为词向量) + <strong>position_embedding</strong>(位置编码，用于表示token在输入序列中的位置) + <strong>segment_emebdding(</strong> 非必须，在bert中用于表示每个词属于哪个句子)。</p>
<p><strong>在VIT中，同样存在token_embedding和postion_emebedding</strong>。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0ca39caa2d5a480592042c52139e7ccc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<h2 data-id="heading-7">3.1 Token Emebdding</h2>
<p>我们记token emebdding为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>，则</mtext></mrow><annotation encoding="application/x-tex">，则</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">，则</span></span></span></span></span>是一个形状为<code>(768, 768)</code>的矩阵。</p>
<p>由前文知经过patch处理后输入$$$$的形状为<code>(196, 768)</code>，则输入X过toke_embedding后的结果为：</p>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>T</mi><mi>E</mi></mrow></msub><mo>=</mo><mi>X</mi><mo>∗</mo><mi>E</mi><mo>=</mo><mo stretchy="false">(</mo><mn>196</mn><mo separator="true">,</mo><mn>768</mn><mo stretchy="false">)</mo><mo>∗</mo><mo stretchy="false">(</mo><mn>768</mn><mo>∗</mo><mn>768</mn><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>196</mn><mo separator="true">,</mo><mn>768</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">X_{TE} = X * E = (196, 768) * (768 * 768) = (196, 768)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">TE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">196</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">768</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">768</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">768</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">196</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">768</span><span class="mclose">)</span></span></span></span></span></p>
<p><strong>你可能想问，输入X本来就是一个</strong> <strong><code>(196，768)</code></strong> <strong>的矩阵啊，我为什么还要过一次embedding呢？</strong> 这个问题的关键不在于数据的维度，而在于embedding的含义。原始的X仅是由数据预处理而来，和主体模型毫无关系。而token_embedding却参与了主体模型训练中的梯度更新，在使用它之后，能更好地表示出token向量。更进一步，E的维度可以表示成<code>(768, x)</code>的形式，也就是第二维不一定要是768，你可以自由设定词向量的维度。</p>
<h2 data-id="heading-8">3.2 Position Embedding（位置向量）</h2>
<p>在NLP任务中，位置向量的目的是让模型学得token的位置信息。在VIT中也是同理，我们需要让模型知道每个patch的位置信息（参见1.2中架构图）。</p>
<p>我们记位置向量为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">E_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>，则它是一个形状为<code>(196，768)</code>的矩阵，表示196个维度为768的向量，<strong>每个向量表示对应token的位置信息</strong>。</p>
<p>构造位置向量的方法有很多种，在VIT中，作者做了不同的消融实验，来验证不同方案的效果（论文附录D.4）部分，我们来详细看看，作者都曾尝试过哪些方案。</p>
<h3 data-id="heading-9">方案一： 不添加任何位置信息</h3>
<p>将输入视为一堆无序的patch，不往其中添加任何位置向量。</p>
<h3 data-id="heading-10">方案二：使用1-D绝对位置编码</h3>
<p><strong>也就是我们在上文介绍的方案，这也是VIT最终选定的方案。</strong></p>
<p>1-D绝对位置编码又分为<strong>函数式</strong>（Transformer的三角函数编码，详情可参见这篇文章）和<strong>可学习式</strong>（Bert采用编码方式），VIT采用的是后者。之所以被称为“绝对位置编码”，是因为位置向量代表的是token的绝对位置信息（例如第1个token，第2个token之类）。</p>
<h3 data-id="heading-11">方案三：使用2-D绝对位置编码</h3>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f2590652d07b40899f07d9e110244dab~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>如图所示，因为图像数据的特殊性，在2-D位置编码中，认为按全局绝对位置信息来表示一个patch是不足够的（如左侧所示），一个patch在x轴和y轴上具有不同含义的位置信息（如右侧所示）。因此，2-D位置编码将原来的PE向量拆成两部分来分别训练。</p>
<h3 data-id="heading-12">方案四：相对位置编码（relative positional embeddings）</h3>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0eee86026a924bf78ac9a3db6a464a0a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>相对位置编码（RPE）的设计思想是：<strong>我们不应该只关注patch的绝对位置信息，更应该关注patch间的相对位置信息</strong>。如图所示，对于token4，它和其余每一个token间都存在相对位置关系，我们分别用<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mo>−</mo><mn>3</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mo>−</mo><mn>2</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_{-3}, w_{-2}, ... w_{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>这5个向量来表示这种位置关系。那么接下来，<strong>只要在正常计算attention的过程中，将这5个向量当作bias添加到计算过程中（如图公式所示），我们就可以正常训练这些相对位置向量了</strong>。为了减少训练时的参数量，我们还可以做<strong>clip操作</strong>，在制定clip的步数k之后，在k范围之外的w我们都用固定的w表示。例如图中当k=2时，向token4的前方找，我们发现<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mo>−</mo><mn>3</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{-3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span>已经在k=2步之外了，因此就可以用<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mo>−</mo><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{-2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span>来替代<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mo>−</mo><mn>3</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{-3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span>，如果token1之前还有token，那么它们的w都可用<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mo>−</mo><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{-2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span>替代。向token4的后方找，发现大家都在k=2步之内，因此无需做任何替换操作。</p>
<p>关于相对位置编码的更多信息，可以阅读原始论文<a href="https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1803.02155.pdf" target="_blank" title="https://arxiv.org/pdf/1803.02155.pdf" ref="nofollow noopener noreferrer">arxiv.org/pdf/1803.02…</a></p>
<h3 data-id="heading-13">实验结果</h3>
<p>这四种位置编码方案的实验结果如下：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/04d317d554dd49a587e42987876b7c70~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>可以发现除了“不加任何位置编码”的效果显著低之外，其余三种方案的结果都差不多。所以作者们当然选择最快捷省力的1-D位置编码方案啦。当你在阅读VIT的论文中，会发现大量的消融实验细节（例如分类头<code>&#x3C;cls></code>要怎么加），<strong>作者这样做的目的也很明确：“我们的方案是在诸多可行的方法中，逐一做实验比对出来的，是全面考虑后的结果。”</strong> 这也是我一直觉得这篇论文在技术之外值得借鉴和反复读的地方。</p>
<h1 data-id="heading-14">四、模型架构的数学表达</h1>
<p>到这一步位置，我们已基本将VIT的模型架构部分讲完了。结合1.2中的模型架构图，我们来用数学语言简练写一下训练中的计算过程：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/91e5a04120bd46a19e5ada33fc494d0e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>(1）即是我们说的图像预处理过程:</p>
<ul>
<li>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>p</mi><mi>i</mi></msubsup></mrow><annotation encoding="application/x-tex">x_{p}^{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2078em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span></span></span></span></span>：第i块patch</p>
</li>
<li>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo separator="true">,</mo><mtext> </mtext><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">E, E_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"> </span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>：Token Embedding，1-D Positional Embedding</p>
</li>
<li>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{class}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>：和Bert类似，是额外加的一个分类头</p>
</li>
<li>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">z_{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>：最终VIT的输入</p>
</li>
</ul>
<p>（2）即是计算multi-head attention的过程，（3）是计算MLP的过程。</p>
<p>（4）是最终分类任务，LN表示是一个简单的线性分类模型，<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>L</mi><mn>0</mn></msubsup></mrow><annotation encoding="application/x-tex">z_{L}^{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0894em;vertical-align:-0.2753em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4247em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2753em;"><span></span></span></span></span></span></span></span></span></span></span>则是<code>&#x3C;cls></code>对应的向量。</p>
<h1 data-id="heading-15">五、微调（fine-tune）</h1>
<p>目前为止，按照一至五部分所说的内容，通过让模型做分类预测，我们可以<strong>预训练（pretrain）</strong> 好一个VIT了。</p>
<p>前面说过，预训练好的VIT模型是个有力的特征提取器，我们可以用它输出的特征，去做更多有趣的<strong>下游任务（downstream task)</strong> 。例如拿它去做类型更丰富的分类，目标检测等事情。在做这些任务时，我们会喂给预训练模型一堆新的数据，同时尽量保证模型的主体架构不变（例如VIT整体参数不动，只在输出层后接一个新模型，再次训练时只对新模型做参数更新之类）。<strong>这种既利用了已有模型的特征提取能力，又能让模型更好适应不同任务的操作，称为微调（fine-tune）。</strong></p>
<p>在fine-tune的时候，我们用的图像大小可能和预训练时的并不一致，比如：</p>
<ul>
<li>
<p>预训练时用<code>224*224*3</code>大小的图片，fine-tune时为了效果更好，一般选择分辨率更高的图片，例如<code>1024*1024*3</code></p>
</li>
<li>
<p>假设保持patch尺寸P=16不变，则预训练时产生的patch数有196个，fine-tune时产生的patch数有4096个(<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>H</mi><mo>∗</mo><mi>W</mi></mrow><msup><mi>P</mi><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">\frac{H*W}{P^{2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2173em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>)</p>
</li>
<li>
<p>我们知道，Transformer主体架构理论上是可以处理任意长度的输入序列的（相关分析参见<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F445895638%2Fanswer%2F2306274526" target="_blank" title="https://www.zhihu.com/question/445895638/answer/2306274526" ref="nofollow noopener noreferrer">这篇</a>文章）。但是<strong>可学习的（learnable）</strong> 位置编码不是，由于一个位置对应一条位置编码，它和输入序列长度密切相关。</p>
</li>
</ul>
<p>那么多出来的patch，在fine-tune时要怎么给它们位置编码呢？如果统一都赋成0向量，然后在fine-tune的时候再去训练这些向量，看起来可以，但这样粗暴的赋值不仅增加了计算量，也浪费了已有的信息（例如，是否能从已有的位置编码粗略地初始化一些新的位置编码出来？）考虑到这一点，<strong>VIT在fine-tune时，对预训练阶段的位置编码做了2D插值处理。</strong></p>
<h2 data-id="heading-16">5.1 VIT fine-tune: 2D插值位置编码</h2>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/179638ae1a6a4d338387e00d3bf64ca0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>如图绿色部分所示，在fine-tune阶段要处理的patch/token数<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>f</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>t</mi><mi>u</mi><mi>n</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">s_{finetune}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight">in</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>可能比预训练阶段要处理的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">s_{pretrain}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">ain</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>要多。</p>
<p>图中红色部分演示了如何通过插值方法将<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">s_{pretrain}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">ain</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>扩展至<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>f</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>t</mi><mi>u</mi><mi>n</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">s_{finetune}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight">in</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>。其中interpolate部分就是2D插值，这部分是重点，我们直接看下代码中的操作：</p>
<pre><code class="hljs language-ini" lang="ini"><span class="hljs-attr">new_pos_embedding_img</span> = nn.functional.interpolate(
            pos_embedding_img,
            <span class="hljs-attr">size</span>=new_seq_length_1d,
            <span class="hljs-attr">mode</span>=interpolation_mode,
            <span class="hljs-attr">align_corners</span>=<span class="hljs-literal">True</span>,
        )
</code></pre>
<p>可以发现这里用了pytorch内置的interpolate函数，mode表示具体的插值方法，在VIT中采用的是bicubic。<code>align_corners=True</code> 的意思是在固定原矩阵四角的情况下按mode进行插值，可以参加图中，白色圆圈表示原始的矩阵，蓝色点表示做完插值后的矩阵。插值后矩阵的四角保持不变，中间则按设置的方法做插值。关于插值位置编码更详细的讲解，可以参考<a href="https://link.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fqq_44166630%2Farticle%2Fdetails%2F127429697" target="_blank" title="https://blog.csdn.net/qq_44166630/article/details/127429697" ref="nofollow noopener noreferrer">这篇</a>文章。</p>
<h1 data-id="heading-17">六、VIT效果</h1>
<p>到目前为止，我们已讲完了预训练和微调的内容。接下来，我们来看VIT的效果，及一些有趣的实验结果。</p>
<h2 data-id="heading-18">6.1 不同VIT模型的表示符号</h2>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f277adbef31c44efbf3f1057c11bc15e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>VIT预训练了三种不同参数规模的模型，分别是<code>VIT-Base</code> ，<code>VIT-Large</code>和<code>VIT-Huge</code>。其规模可具体见上图。</p>
<p>在论文及实际使用中，我们常用<code>VIT-size/patch_size</code>的形式来表示该模型是在“什么规模”及“多大的patch尺寸”上预训练出来的。例如<code>VIT-H/14</code> 就表示该模型是在Huge规模上，用patch尺寸为14的数据做预训练的。</p>
<h2 data-id="heading-19">6.2 VIT VS 卷积神经网络</h2>
<p>既然VIT的目的是替换卷积神经网络，那么当然要比较一下它和目前SOTA的卷积网络间的性能了。</p>
<p>作者选取了ResNet和Noisy Student这两种经典高性能的卷积神经网络与VIT进行比较，比较内容为“<strong>预测图片类别的准确性</strong>”与“<strong>训练时长</strong>”，结果如下：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/76655dab731c4604af07df12b012d541~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>前三列Ours-JFT(VIT-H/14)，Ours-JFT(VIT-L/16)，Ours-I12K(VIT-L/16)表示三个VIT预训练模型，它们分别在不同规模和不同数据集（JFT, I12K）上预训练而来。后两列表示两个卷积神经网络模型。</p>
<p>纵向的ImageNet，ImageNet Real等表示不同的图像数据集，当我们的VIT模型和卷积模型预训练好后，我们就可以借助这些pretrain模型，在图像数据集上做fine-tune，而表格里给出的就是fine-tune后的准确率。</p>
<p>观察表格，我们发现一个有趣的现象：<strong>VIT和卷积神经网络相比，表现基本一致</strong>。关于这一点，我们会在下文详细分析。</p>
<p>虽然准确率没有突出表现，但是训练时间上VIT的还是有亮点的，表格最后一行表示，假设用单块TPU训练模型，所需要的天数。<strong>我们发现VIT最高也只需要2500核-天（当然其实这个值也不小啦），卷积网络要花至9900核-天以上。所以VIT的一个优势在于，训练没那么贵了。</strong> 关于这点，我的猜想是基于Transformer架构的VIT，和卷积神经网络相比，更适合做<strong>切分均匀</strong>的矩阵计算，这样我们就能把参数均匀切到不同卡上做分布式训练，更好利用GPU算力，平衡整个训练系统了。</p>
<p>现在，我们回到刚才的问题，<strong>为什么VIT相比卷积网络，在准确率上没有突出优势</strong>？为了解答这个问题，我们先来看卷积神经网络的<strong>归纳偏置（inductive biases）</strong></p>
<h4 data-id="heading-20">6.2.1 卷积神经网络的归纳偏置</h4>
<p><strong>归纳偏置用大白话来说，就是一种假设，或者说一种先验知识。有了这种先验，我们就能知道哪一种方法更适合解决哪一类任务。所以归纳偏置是一种统称，不同的任务其归纳偏置下包含的具体内容不一样。</strong></p>
<p>对图像任务来说，它的归纳偏置有以下两点：</p>
<ul>
<li><strong>空间局部性（locality）</strong> ：假设一张图片中，相邻的区域是有相关特征的。比如太阳和天空就经常一起出现。</li>
<li><strong>平移等边性（translation equivariance）</strong> ：<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo>=</mo><mtext>卷积</mtext><mo separator="true">,</mo><mi>g</mi><mo>=</mo><mtext>平</mtext></mrow><annotation encoding="application/x-tex">f(g(x)) = g(f(x)), f=卷积, g=平</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord cjk_fallback">卷积</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">平</span></span></span></span></span>。假设一张图中，左上角有一个太阳，你对这张图正常做卷积得到特征图，则左上角的卷积可表示为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span></span></span></span></span>，做完卷积后，你想把左上角的特征图移动到右上角去，则你这一顿操作可以用<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>来表示。这一系列操作等同于，你先把左上角的太阳移动到右上角去(<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">g(x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">x</span></span></span></span></span>)，然后再做卷积<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(g(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>，这就是图像的平移等边性。不论物体移动到哪里，只要给卷积核的输入不变，那么输出也是一致的。</li>
</ul>
<p>在这两种先验假设下，CNN成为了图像任务最佳的方案之一。<strong>卷积核能最大程度保持空间局部性（保存相关物体的位置信息）和平移等边性</strong>，使得在训练过程中，最大限度学习和保留原始图片信息。</p>
<p>好，那么现在，如果说VIT相比于卷积，在图像任务上没有显著优势，那大概率VIT对这两种先验的维护没有CNN做的好，具体来看：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4820c35273af423fadf62c9d9fb7efcc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>图中箭头所指的两部分都属于同一栋建筑。在卷积中，我们可以用大小适当的卷积核将它们圈在一起。但是在VIT中，它们之间的位置却拉远了，如果我把patch再切分细一些，它们的距离就更远了。虽然attention可以学习到向量间的想关系，但是VIT在<strong>空间局部性</strong>的维护上，确实没有卷积做的好。而在<strong>平移等边性</strong>上，由于VIT需要对patch的位置进行学习，所以对于一个patch，当它位置变幻时，它的输出结果也是不一样的。<strong>所以，VIT的架构没有很好维护图像问题中的归纳偏置假设</strong>。</p>
<p>但是，这就意味着VIT没有翻盘的一天了吗？当然不是，<strong>不要忘了，Transformer架构的模型都有一个广为人知的特性：大力出奇迹</strong>。只要它见过的数据够多，它就能更好地学习像素块之间的关联性，当然也能抹去归纳偏置的问题。</p>
<h4 data-id="heading-21">6.2.2 VIT：大力出奇迹</h4>
<p>作者当然也考虑到了这点，所以采用了不同数量的数据集，对VIT进行训练，效果如下：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c631948acb1b49948594db76dc757f54~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>如图，横轴表示不同量级的数据集（越往右数据集越大），纵轴表示准确率。图中灰色阴影部分表示在相应数据集下，不同架构的卷积神经网络的准确率范围。<strong>可以发现，当数据集较小时，VIT表现明显弱于卷积网络。但当数据量级大于21k时，VIT的能力就上来了</strong>。</p>
<h2 data-id="heading-22">6.3 VIT的Attention到底看到了什么</h2>
<p>讲完了VIT的整体效果，我们来探究下VIT具体学到了什么，才能帮助它达到这样的效果。我们首先来看attention层。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/475e90e0a6bc467ab5541a2ec2424f27~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>这张实验图刻画了VIT的16个multi-head attention学到的像素距离信息。横轴表示网络的深度，纵轴表示“平均注意力距离”，我们设第i个和第j个像素的平均注意力距离为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>，真实像素距离为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">d_{ij}^{\prime}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1467em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span></span>，这两个像素所在patch某一个head上的attention score为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">a_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>，则有：<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∗</mo><msubsup><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">d_{ij} = a_{ij} * d_{ij}^{\prime}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7514em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1467em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span></span>。当<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>越大时，说明VIT的attention机制能让它关注到距离较远的两个像素，类似于CNN中的“扩大感受野”。</p>
<p>图中每一列上，都有16个彩色原点，它们分别表示16个head观测到的平均像素距离。由图可知，在浅层网络中，VIT还只能关注到距离较近的像素点，<strong>随着网络加深，VIT逐渐学会去更远的像素点中寻找相关信息了。这个过程就和用在CNN中用卷积逐层去扩大感受野非常相似</strong>。</p>
<p>下图的左侧表示原始的输入图片，右侧表示VIT最后一层看到的图片信息，可以清楚看见，VIT在最后一层已经学到了将注意力放到关键的物体上了，这是非常有趣的结论：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0c57036c354e42fdb0a8c9bfdd0c2376~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<h2 data-id="heading-23">6.4 VIT的位置编码学到了什么</h2>
<p>我们在上文讨论过图像的<strong>空间局部性（locality）</strong> ，即有相关性的物体（例如太阳和天空）经常一起出现。CNN采用卷积框取特征的方式，极大程度上维护了这种特性。<strong>其实，VIT也有维护这种特性的方法，上面所说的attention是一种，位置编码也是一种。</strong></p>
<p>我们来看看VIT的位置编码学到了什么信息：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2dce17b395fc48f8be7f95a0da800d8f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp" alt="" loading="lazy"></p>
<p>上图是<code>VIT-L/32</code>模型下的位置编码信息，图中每一个方框表示一个patch，图中共有<code>7*7</code>个patch。而每个方框内，也有一个<code>7*7</code>的矩阵，这个矩阵中的每一个值，表示当前patch的position embedding和其余对应位置的position embedding的余弦相似度。<strong>颜色越黄，表示越相似，也即patch和对应位置间的patch密切相关</strong>。</p>
<p>注意到每个方框中，最黄的点总是当前patch所在位置，这个不难理解，因为自己和自己肯定是最相似的。除此以外<strong>颜色较黄的部分都是当前patch所属的行和列，以及以当前patch为中心往外扩散的一小圈。这就说明VIT通过位置编码，已经学到了一定的空间局部性</strong>。</p>
<h1 data-id="heading-24">七、总结：VIT的意义何在</h1>
<p>到此为止，关于VIT模型，我们就介绍完毕了。一顿读下来，你可能有个印象：<strong>如果训练数据量不够多的话，看起来VIT也没比CNN好多少呀，VIT的意义是什么呢？</strong></p>
<p>这是个很好的问题，<strong>因为在工业界，人们的标注数据量和算力都是有限的，因此CNN可能还是首要选择</strong>。</p>
<p>但是，VIT的出现，不仅是用模型效果来考量这么简单，今天再来看这个模型，<strong>发现它的作用，就像是给后续的植树人挖好了坑，等人在上面播种耕植</strong>，具体表现在：</p>
<ul>
<li>
<p>证明了一个统一框架在不同模态任务上的表现能力。在VIT之前，NLP的SOTA范式被认为是Transformer，而图像的SOTA范式依然是CNN。<strong>VIT出现后，证明了用NLP领域的SOTA模型一样能解图像领域的问题，同时在论文中通过丰富的实验，证明了VIT对CNN的替代能力</strong>，同时也论证了<strong>大规模+大模型在图像领域的涌现能力</strong>（论文中没有明确指出这是涌现能力，但通过实验展现了这种趋势）。这也为后续两年多模态任务的发展奠定了基石。</p>
</li>
<li>
<p>虽然VIT只是一个分类任务，但在它提出的几个月之后，立刻就有了用Transformer架构做检测（detection）和分割（segmentation）的模型。而不久之后，GPT式的无监督学习，也在CV届开始火热起来。</p>
</li>
<li>
<p>工业界上，对大部分企业来说，受到训练数据和算力的影响，预训练和微调一个VIT都是困难的，但是这不妨碍直接拿大厂训好的VIT特征做下游任务。同时，低成本的微调方案研究，在今天也层出不穷。长远来看，2年前的这个“庞然大物”，已经在逐步走进千家万户。</p>
</li>
</ul>
<h1 data-id="heading-25">八、参考</h1>
<p>1、<a href="https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2010.11929.pdf" target="_blank" title="https://arxiv.org/pdf/2010.11929.pdf" ref="nofollow noopener noreferrer">arxiv.org/pdf/2010.11…</a></p>
<p>2、<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV15P4y137jb%2F%3Fspm_id_from%3D333.337.search-card.all.click" target="_blank" title="https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.337.search-card.all.click" ref="nofollow noopener noreferrer">www.bilibili.com/video/BV15P…</a></p>
<p>3、<a href="https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1803.02155.pdf" target="_blank" title="https://arxiv.org/pdf/1803.02155.pdf" ref="nofollow noopener noreferrer">arxiv.org/pdf/1803.02…</a></p>
<p>4、<a href="https://link.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fqq_44166630%2Farticle%2Fdetails%2F127429697" target="_blank" title="https://blog.csdn.net/qq_44166630/article/details/127429697" ref="nofollow noopener noreferrer">blog.csdn.net/qq_44166630…</a></p></div></article> <div class="article-end" data-v-5762947c data-v-5b3ac010><div class="tag-list-box" data-v-5762947c data-v-5b3ac010><!----><!----><!----></div></div> <!----><!----><!----><!----><!----></div> <div id="sidebar-container" class="sidebar article-sidebar" data-v-48278d5a data-v-5b3ac010><div class="sidebar-block author-block author-block-container pure" data-v-7e7e812a data-v-3e1bd377 data-v-48278d5a><a href="/user/970190899118414/posts" target="_blank" rel="" class="jj-link user-item item" data-v-04ff4e9e data-v-3e1bd377><div class="avatar jj-avatar avatar" data-v-03256cc6 data-v-3e1bd377><img loading="eager" src="https://p9-passport.byteacctimg.com/img/user-avatar/7b5ddd9a37165c489c861659be04fcc9~200x200.awebp" alt="avatar" class="lazy avatar-img immediate" data-v-5244ef91 data-v-03256cc6> </div> <div class="info-box" style="visibility:hidden;" data-v-04ff4e9e data-v-3e1bd377><span to="[object Object]" blank="true" class="username" data-v-1800aadb data-v-3e1bd377><span class="name" style="max-width:128px;" data-v-1800aadb>
    猛猿
  </span> <span to="" blank="true" class="rank" data-v-23743940 data-v-1800aadb><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADwAAAA8AQMAAAAAMksxAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAA5JREFUKM9jGAWjAAcAAAIcAAE27nY6AAAAAElFTkSuQmCC" alt="创作等级LV.3" title="创作等级LV.3" class="lazy" style="aspect-ratio:NaN;" data-v-5244ef91 data-v-23743940></span> <!----> <!----> </span> <div title="🏆掘金签约作者｜人工智能方向" class="position" data-v-04ff4e9e data-v-3e1bd377>
        🏆掘金签约作者｜人工智能方向
      </div> <div class="extra-container" data-v-04ff4e9e data-v-3e1bd377><!----> </div></div></a> <div class="count-container" data-v-7e7e812a data-v-3e1bd377><a href="/user/970190899118414/posts" target="_blank" rel="" class="jj-link stat-item item" data-v-04ff4e9e data-v-3e1bd377><div class="count" style="display:none;" data-v-04ff4e9e data-v-3e1bd377>
        14
      </div> <div data-v-04ff4e9e data-v-3e1bd377>文章</div></a> <a href="/user/970190899118414/posts" target="_blank" rel="" class="jj-link stat-item item" data-v-04ff4e9e data-v-3e1bd377><div class="count" style="display:none;" data-v-04ff4e9e data-v-3e1bd377>
        22k
      </div> <div data-v-04ff4e9e data-v-3e1bd377>阅读</div></a> <a href="/user/970190899118414/followers" target="_blank" rel="" class="jj-link stat-item item" data-v-04ff4e9e data-v-3e1bd377><div class="count" style="display:none;" data-v-04ff4e9e data-v-3e1bd377>
        89
      </div> <div data-v-04ff4e9e data-v-3e1bd377>粉丝</div></a></div> <div class="operate-btn hidden" style="position:relative;z-index:2;min-height:36px;" data-v-7e7e812a data-v-3e1bd377><!----><!----><!----></div></div> <div class="sticky-block-box" data-v-48278d5a><nav class="article-catalog catalog-block none" data-v-6239701c data-v-16ed86c3 data-v-48278d5a><div class="catalog-title" data-v-6239701c><div data-v-6239701c>目录</div> <div class="direction" data-v-6239701c><div class="word" data-v-6239701c>收起</div> <svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg" class="icon-rotate" data-v-6239701c data-v-6239701c><g id="&amp;#229;&amp;#177;&amp;#149;&amp;#229;&amp;#188;&amp;#128;" data-v-6239701c data-v-6239701c><path id="&amp;#232;&amp;#183;&amp;#175;&amp;#229;&amp;#190;&amp;#132;" fill-rule="evenodd" clip-rule="evenodd" d="M5.99854 7.93206L10.0644 3.86619C10.162 3.76856 10.3203 3.76856 10.418 3.86619L10.7715 4.21975C10.8691 4.31738 10.8691 4.47567 10.7715 4.5733L6.35209 8.99272C6.15683 9.18798 5.84025 9.18798 5.64498 8.99272L1.22557 4.5733C1.12794 4.47567 1.12794 4.31738 1.22557 4.21975L1.57912 3.86619C1.67675 3.76856 1.83504 3.76856 1.93267 3.86619L5.99854 7.93206Z" fill="#8A919F" data-v-6239701c data-v-6239701c></path></g></svg></div></div> <div class="catalog-body unfold" data-v-6239701c><ul class="catalog-list" style="margin-top:0px;" data-v-6239701c><li class="item d1" data-v-6239701c><div class="a-container"><a href="#heading-0" title="一、模型架构" class="catalog-aTag d1-aTag-title">
      一、模型架构
    </a></div> <ul class="sub-list"><li class="item d2"><div class="a-container"><a href="#heading-1" title="1.1 Bert架构" class="catalog-aTag d2-aTag-title">
      1.1 Bert架构
    </a></div> <!----></li><li class="item d2"><div class="a-container"><a href="#heading-2" title="1.2 VIT模型架构" class="catalog-aTag d2-aTag-title">
      1.2 VIT模型架构
    </a></div> <!----></li></ul></li><li class="item d1" data-v-6239701c><div class="a-container"><a href="#heading-3" title="二、从patch到token" class="catalog-aTag d1-aTag-title">
      二、从patch到token
    </a></div> <ul class="sub-list"><li class="item d2"><div class="a-container"><a href="#heading-4" title="2.1 patch变token的过程" class="catalog-aTag d2-aTag-title">
      2.1 patch变token的过程
    </a></div> <!----></li><li class="item d2"><div class="a-container"><a href="#heading-5" title="2.2 为什么要处理成patch" class="catalog-aTag d2-aTag-title">
      2.2 为什么要处理成patch
    </a></div> <!----></li></ul></li><li class="item d1" data-v-6239701c><div class="a-container"><a href="#heading-6" title="三、Emebdding" class="catalog-aTag d1-aTag-title">
      三、Emebdding
    </a></div> <ul class="sub-list"><li class="item d2"><div class="a-container"><a href="#heading-7" title="3.1 Token Emebdding" class="catalog-aTag d2-aTag-title">
      3.1 Token Emebdding
    </a></div> <!----></li><li class="item d2"><div class="a-container"><a href="#heading-8" title="3.2 Position Embedding（位置向量）" class="catalog-aTag d2-aTag-title">
      3.2 Position Embedding（位置向量）
    </a></div> <ul class="sub-list"><li class="item d3"><div class="a-container"><a href="#heading-9" title="方案一： 不添加任何位置信息" class="catalog-aTag d3-aTag-title">
      方案一： 不添加任何位置信息
    </a></div> <!----></li><li class="item d3"><div class="a-container"><a href="#heading-10" title="方案二：使用1-D绝对位置编码" class="catalog-aTag d3-aTag-title">
      方案二：使用1-D绝对位置编码
    </a></div> <!----></li><li class="item d3"><div class="a-container"><a href="#heading-11" title="方案三：使用2-D绝对位置编码" class="catalog-aTag d3-aTag-title">
      方案三：使用2-D绝对位置编码
    </a></div> <!----></li><li class="item d3"><div class="a-container"><a href="#heading-12" title="方案四：相对位置编码（relative positional embeddings）" class="catalog-aTag d3-aTag-title">
      方案四：相对位置编码（relative positional embeddings）
    </a></div> <!----></li><li class="item d3"><div class="a-container"><a href="#heading-13" title="实验结果" class="catalog-aTag d3-aTag-title">
      实验结果
    </a></div> <!----></li></ul></li></ul></li><li class="item d1" data-v-6239701c><div class="a-container"><a href="#heading-14" title="四、模型架构的数学表达" class="catalog-aTag d1-aTag-title">
      四、模型架构的数学表达
    </a></div> <!----></li><li class="item d1" data-v-6239701c><div class="a-container"><a href="#heading-15" title="五、微调（fine-tune）" class="catalog-aTag d1-aTag-title">
      五、微调（fine-tune）
    </a></div> <ul class="sub-list"><li class="item d2"><div class="a-container"><a href="#heading-16" title="5.1 VIT fine-tune: 2D插值位置编码" class="catalog-aTag d2-aTag-title">
      5.1 VIT fine-tune: 2D插值位置编码
    </a></div> <!----></li></ul></li><li class="item d1" data-v-6239701c><div class="a-container"><a href="#heading-17" title="六、VIT效果" class="catalog-aTag d1-aTag-title">
      六、VIT效果
    </a></div> <ul class="sub-list"><li class="item d2"><div class="a-container"><a href="#heading-18" title="6.1 不同VIT模型的表示符号" class="catalog-aTag d2-aTag-title">
      6.1 不同VIT模型的表示符号
    </a></div> <!----></li><li class="item d2"><div class="a-container"><a href="#heading-19" title="6.2 VIT VS 卷积神经网络" class="catalog-aTag d2-aTag-title">
      6.2 VIT VS 卷积神经网络
    </a></div> <ul class="sub-list"><li class="item d3"><div class="a-container"><a href="#heading-20" title="6.2.1 卷积神经网络的归纳偏置" class="catalog-aTag d3-aTag-title">
      6.2.1 卷积神经网络的归纳偏置
    </a></div> <!----></li><li class="item d3"><div class="a-container"><a href="#heading-21" title="6.2.2 VIT：大力出奇迹" class="catalog-aTag d3-aTag-title">
      6.2.2 VIT：大力出奇迹
    </a></div> <!----></li></ul></li><li class="item d2"><div class="a-container"><a href="#heading-22" title="6.3 VIT的Attention到底看到了什么" class="catalog-aTag d2-aTag-title">
      6.3 VIT的Attention到底看到了什么
    </a></div> <!----></li><li class="item d2"><div class="a-container"><a href="#heading-23" title="6.4 VIT的位置编码学到了什么" class="catalog-aTag d2-aTag-title">
      6.4 VIT的位置编码学到了什么
    </a></div> <!----></li></ul></li><li class="item d1" data-v-6239701c><div class="a-container"><a href="#heading-24" title="七、总结：VIT的意义何在" class="catalog-aTag d1-aTag-title">
      七、总结：VIT的意义何在
    </a></div> <!----></li><li class="item d1" data-v-6239701c><div class="a-container"><a href="#heading-25" title="八、参考" class="catalog-aTag d1-aTag-title">
      八、参考
    </a></div> <!----></li></ul></div></nav> <!----> <div class="sidebar-block shadow" data-v-7e7e812a data-v-79d288d1 data-v-48278d5a><div class="block-title" data-v-7e7e812a>相关推荐</div> <div class="block-body" data-v-7e7e812a><div class="entry-list" data-v-7e7e812a data-v-79d288d1><a href="/post/7267417057438777399" target="_blank" rel="" title="CV大模型系列之：MAE，实现像素级图像重建" class="jj-link item" data-v-04ff4e9e data-v-79d288d1><div class="entry-title" data-v-04ff4e9e data-v-79d288d1>CV大模型系列之：MAE，实现像素级图像重建</div> <div class="entry-meta-box" data-v-04ff4e9e data-v-79d288d1><!----> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>877阅读</div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1> · </div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>11点赞</div></div></a><a href="/post/7282962960213098557" target="_blank" rel="" title="CV大模型系列之：打败VIT？Swin Transformer是怎么做到的" class="jj-link item" data-v-04ff4e9e data-v-79d288d1><div class="entry-title" data-v-04ff4e9e data-v-79d288d1>CV大模型系列之：打败VIT？Swin Transformer是怎么做到的</div> <div class="entry-meta-box" data-v-04ff4e9e data-v-79d288d1><!----> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>1.1k阅读</div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1> · </div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>4点赞</div></div></a><a href="/post/7241859817563258917" target="_blank" rel="" title="图解Transformer系列一：Positional Encoding（位置编码）" class="jj-link item" data-v-04ff4e9e data-v-79d288d1><div class="entry-title" data-v-04ff4e9e data-v-79d288d1>图解Transformer系列一：Positional Encoding（位置编码）</div> <div class="entry-meta-box" data-v-04ff4e9e data-v-79d288d1><!----> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>1.0k阅读</div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1> · </div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>6点赞</div></div></a><a href="/post/7241859817563389989" target="_blank" rel="" title="图解Transformer系列二：Self-Attention（自注意力机制）" class="jj-link item" data-v-04ff4e9e data-v-79d288d1><div class="entry-title" data-v-04ff4e9e data-v-79d288d1>图解Transformer系列二：Self-Attention（自注意力机制）</div> <div class="entry-meta-box" data-v-04ff4e9e data-v-79d288d1><!----> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>976阅读</div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1> · </div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>3点赞</div></div></a><a href="/post/7275932704533282831" target="_blank" rel="" title="CV大模型系列之：DALLE2，OpenAI文生图代表作解读" class="jj-link item" data-v-04ff4e9e data-v-79d288d1><div class="entry-title" data-v-04ff4e9e data-v-79d288d1>CV大模型系列之：DALLE2，OpenAI文生图代表作解读</div> <div class="entry-meta-box" data-v-04ff4e9e data-v-79d288d1><!----> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>1.6k阅读</div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1> · </div> <div class="entry-meta" data-v-04ff4e9e data-v-79d288d1>5点赞</div></div></a></div></div></div> <!----></div> <!----> <div id="wechat-assets__post-bottom" data-v-48278d5a></div></div> <!----> <div class="recommended-links main-area" data-v-65e9deb8 data-v-5b3ac010><p data-v-65e9deb8>友情链接：</p> <ul data-v-65e9deb8><li data-v-65e9deb8><a href="https://frontend.devrank.cn/traffic-aggregation/189503" title="js jsp session" target="_blank" rel="noopener noreferrer" data-v-65e9deb8>
        js jsp session
      </a></li></ul></div> <div id="article-suspended-panel" data-v-5762947c data-v-5b3ac010></div></div> <!----><!----><!----></main> <!----></div> <!----> <div class="global-component-box"><!----></div> <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----> <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div></div></div><script>window.__NUXT__=(function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O){t.loading=a;t.skeleton=d;t.cursor=f;t.data=[];t.total=b;t.hasMore=d;F.id=v;F.self_description=l;F.followed=a;F.viewerIsFollowing=l;F.community=l;F.subscribedTagCount=b;F.wroteBookCount=b;F.boughtBookCount=b;F.isBindedPhone=a;F.level=p;F.user_id=v;F.user_name=D;F.company=e;F.job_title="🏆掘金签约作者｜人工智能方向";F.avatar_large="https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image";F.description="填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。";F.followee_count=10;F.follower_count=89;F.post_article_count=14;F.digg_article_count=34;F.got_digg_count=132;F.got_view_count=22007;F.post_shortmsg_count=b;F.digg_shortmsg_count=b;F.isfollowed=a;F.favorable_author=b;F.power=E;F.study_point=b;F.university={university_id:f,name:e,logo:e};F.major={major_id:f,parent_id:f,name:e};F.student_status=b;F.select_event_count=b;F.select_online_course_count=b;F.identity=b;F.is_select_annual=a;F.select_annual_rank=b;F.annual_list_type=b;F.extraMap={};F.is_logout=b;F.annual_info=[];F.account_amount=b;F.user_growth_info={user_id:970190899118414,jpower:E,jscore:538.6,jpower_level:p,jscore_level:5,jscore_title:"先锋掘友",author_achievement_list:[],vip_level:p,vip_title:"渐入佳境",jscore_next_level_score:2000,jscore_this_level_mini_score:500,vip_score:270};F.is_vip=a;F.become_author_days=b;F.collection_set_article_count=b;F.recommend_article_count_daily=b;F.article_collect_count_daily=b;F.juejinPower=E;F.jobTitle="🏆掘金签约作者｜人工智能方向";F.roles={isBookAuthor:a,isFavorableAuthor:a,isCobuilder:a,isAdmin:a};F.username=D;F.blogAddress=l;F.selfDescription="填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。";F.beLikedCount=132;F.beReadCount=22007;F.followerCount=89;F.followingCount=10;F.collectionCount=b;F.createdCollectionCount=b;F.followingCollectionCount=b;F.postedPostsCount=14;F.pinCount=b;F.likedArticleCount=34;F.likedPinCount=b;F.avatar="https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image";F.latestLoginedInAt=c;F.createdAt=c;F.updatedAt=c;F.phoneNumber=e;F.titleDescription=e;F.followeesCount=10;F.applyEventCount=b;F.need_lead=b;F.followTopicCnt=b;return {layout:"default",data:[{renderPost:d}],fetch:[{queryString:e,isShowUserDropdownList:a,isShowAddMoreList:a,isFocus:a,isPhoneMenuShow:a,visibleBadge:a,placeholder:e,hiddenProperty:"hidden",searchHistoryVisible:a,searchHistoryItems:[],tabBadge:c,isChangePlaceholder:d,showMallBridge:a,removeSearchInputKeyupListener:c,logoImg:"\u002F\u002Flf3-cdn-tos.bytescm.com\u002Fobj\u002Fstatic\u002Fxitu_juejin_web\u002Fe08da34488b114bd4c665ba2fa520a31.svg"}],error:c,state:{view:{activityIndex:{activityList:[],pageInfo:{hasNextPage:a,endCursor:e},afterPosition:e,activityListIsLoading:d,activityListIsError:a,userActivityList:[],placeholder:e,actionType:{FETCH:"@\u002Fview\u002Factivity-index\u002FFETCH",FETCH_RECOMMEND_LIST:"@\u002Fview\u002Factivity-index\u002FFETCH_RECOMMEND_LIST",RESET_ACTIVITY_LIST:"@\u002Fview\u002Factivity-index\u002FRESET_ACTIVITY_LIST",FETCH_USER_ACTIVITY_LIST:"@\u002Fview\u002Factivity-index\u002FFETCH_USER_ACTIVITY_LIST",FETCH_NEW_COUNT:"@\u002Fview\u002Factivity-index\u002FFETCH_NEW_COUNT",DELETE_ACTIVITY:"@\u002Fview\u002Factivity-index\u002FDELETE_ACTIVITY",TOGGLE_FOLLOW_USER:"@\u002Fview\u002Factivity-index\u002FTOGGLE_FOLLOW_USER",FETCH_ENTRY_COMMENT_LIST:"@\u002Fview\u002Factivity-index\u002FFETCH_ENTRY_COMMENT_LIST",UPDATE_LIST_LOADING:"@\u002Fview\u002Factivity-index\u002FUPDATE_LIST_LOADING",RESET:"@\u002Fview\u002Factivity-index\u002FRESET"},hotList:{list:[],after:e,loading:a,hasNextPage:a,actionType:{UPDATE_STATE:"@\u002Fview\u002Factivity-index\u002Fhot-list\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Factivity-index\u002Fhot-list\u002FFETCH_MORE",FETCH:"@\u002Fview\u002Factivity-index\u002Fhot-list\u002FFETCH",RESET:"@\u002Fview\u002Factivity-index\u002Fhot-list\u002FRESET"}},sidebar:{bannerList:[],actionType:{RESET:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002FRESET",UPDATE_STATE:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002FUPDATE_STATE",FETCH_BANNER:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002FFETCH_BANNER"},recommend:{pageSize:h,page:b,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Frecommend-topic-list\u002FUPDATE",FETCH:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Frecommend-topic-list\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Frecommend-topic-list\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Frecommend-topic-list\u002FFETCH_MORE",RESET:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Frecommend-topic-list\u002FRESET"},after:b},followed:{pageSize:h,page:b,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Ffollowed-topic-list\u002FUPDATE",FETCH:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Ffollowed-topic-list\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Ffollowed-topic-list\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Ffollowed-topic-list\u002FFETCH_MORE",RESET:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Ffollowed-topic-list\u002FRESET"},after:b},recommendPin:{list:[],after:e,loading:a,hasNextPage:d,actionType:{UPDATE_STATE:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Frecommend-pin-list\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Frecommend-pin-list\u002FFETCH_MORE",FETCH:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Frecommend-pin-list\u002FFETCH",RESET:"@\u002Fview\u002Factivity-index\u002Fsidebar\u002Frecommend-pin-list\u002FRESET"}}},topicPinList:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Factivity-index\u002Ftopic-pin-list\u002FUPDATE",FETCH:"@\u002Fview\u002Factivity-index\u002Ftopic-pin-list\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Factivity-index\u002Ftopic-pin-list\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Factivity-index\u002Ftopic-pin-list\u002FFETCH_MORE",RESET:"@\u002Fview\u002Factivity-index\u002Ftopic-pin-list\u002FRESET"},topicId:e,navList:[{type:k,name:k,title:"推荐 ",id:k},{type:m,name:m,title:"热门 ",id:m},{type:q,name:q,title:"关注 ",id:q},{type:i,name:"opensource",title:"开源推荐 ",id:"5c09ea2b092dcb42c740fe73"},{type:i,name:"recruitment",title:"内推招聘",id:"5abb61e1092dcb4620ca3322"},{type:i,name:"dating",title:"掘金相亲",id:"5abcaa67092dcb4620ca335c"},{type:i,name:"slacking",title:"上班摸鱼",id:"5c106be9092dcb2cc5de7257"},{type:i,name:"app",title:"应用安利",id:"5b514af1092dcb61bd72800d"},{type:i,name:"tool",title:"开发工具",id:"5abb67d2092dcb4620ca3324"},{type:i,name:"news",title:"New资讯",id:"5c46a17f092dcb4737217152"}],sortType:n}},search:{search_result_from:b,query:e,list:[],linkList:[],loading:a,skeleton:d,actionType:{FETCH:"@\u002Fview\u002Fsearch\u002FFETCH",FETCH_MORE:"@\u002Fview\u002Fsearch\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fsearch\u002FRESET"}},columnIndex:{list:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002FcolumnIndex\u002Flist\u002FUPDATE",FETCH:"@\u002Fview\u002FcolumnIndex\u002Flist\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002FcolumnIndex\u002Flist\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002FcolumnIndex\u002Flist\u002FFETCH_MORE",RESET:"@\u002Fview\u002FcolumnIndex\u002Flist\u002FRESET"},sort:o,category:"all"},hotList:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002FcolumnIndex\u002FhotList\u002FUPDATE",FETCH:"@\u002Fview\u002FcolumnIndex\u002FhotList\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002FcolumnIndex\u002FhotList\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002FcolumnIndex\u002FhotList\u002FFETCH_MORE",RESET:"@\u002Fview\u002FcolumnIndex\u002FhotList\u002FRESET"}}},timelineIndex:{tdkTemplates:[],categoryNavList:[],tagNavList:[],splitTagList:[],timelineAdList:[],list:[],sort:n,category:k,categoryId:e,tagId:e,tag:"全部",actionType:{FETCH_TIMELINE_LIST:"@\u002Fview\u002FtimelineIndex\u002FFETCH_TIMELINE_LIST",FETCH_CATEGORY_LIST:"@\u002Fview\u002FtimelineIndex\u002FFETCH_CATEGORY_LIST",FETCH_TAG_LIST:"@\u002Fview\u002FtimelineIndex\u002FFETCH_TAG_LIST",DELETE_ENTRY:"@\u002Fview\u002FtimelineIndex\u002FDELETE_ENTRY",DELETE_USER_ENTRIES:"@\u002Fview\u002FtimelineIndex\u002FDELETE_USER_ENTRIES",DELETE_TAG_ENTRIES:"@\u002Fview\u002FtimelineIndex\u002FDELETE_TAG_ENTRIES",FETCH_MORE:"@\u002Fview\u002FtimelineIndex\u002FFETCH_MORE",FETCH:"@\u002Fview\u002FtimelineIndex\u002FFETCH",RESET:"@\u002Fview\u002FtimelineIndex\u002FRESET"},serverRenderTimelineList:a,timelineList:{list:[],cursor:f,skeleton:d,loading:a,hasMore:d,categoryId:e,tagId:e,sort:e,actionType:{UPDATE_STATE:"timeline-list\u002FUPDATE_STATE",FETCH_MORE:"timeline-list\u002FFETCH_MORE",FETCH:"timeline-list\u002FFETCH",RESET:"timeline-list\u002FRESET"}},recommendList:{list:[],cursor:f,loading:a,skeleton:d,hasMore:d,actionType:{UPDATE_STATE:"recommend-list\u002FUPDATE_STATE",FETCH_MORE:"recommend-list\u002FFETCH_MORE",FETCH:"recommend-list\u002FFETCH",RESET:"recommend-list\u002FRESET"}},followingList:{list:[],cursor:f,skeleton:d,loading:a,hasMore:d,actionType:{UPDATE_STATE:"following-list\u002FUPDATE_STATE",FETCH_MORE:"following-list\u002FFETCH_MORE",FETCH:"following-list\u002FFETCH",RESET:"following-list\u002FRESET"}}},subscribe:{subscribed:{list:[],cursor:f,skeleton:d,loading:a,hasMore:a,actionType:{UPDATE_STATE:"view\u002Fsubscribe\u002Fsubscribed\u002Flist\u002FUPDATE_STATE",FETCH_MORE:"view\u002Fsubscribe\u002Fsubscribed\u002Flist\u002FFETCH_MORE",FETCH:"view\u002Fsubscribe\u002Fsubscribed\u002Flist\u002FFETCH",RESET:"view\u002Fsubscribe\u002Fsubscribed\u002Flist\u002FRESET"}},all:{list:[],cursor:f,loading:a,skeleton:d,hasMore:a,linkList:e,actionType:{UPDATE_STATE:"view\u002Fsubscribe\u002Fall\u002Flist\u002FUPDATE_STATE",FETCH_MORE:"view\u002Fsubscribe\u002Fall\u002Flist\u002FFETCH_MORE",FETCH:"view\u002Fsubscribe\u002Fall\u002Flist\u002FFETCH",RESET:"view\u002Fsubscribe\u002Fall\u002Flist\u002FRESET"}}},entryPublic:{entry:{user:{}},relatedEntryList:[],relatedCollectionList:[],actionType:{FETCH:"@\u002Fview\u002FentryPublic\u002FFETCH",RESET:"@\u002Fview\u002FentryPublic\u002FRESET"}},user:{user:{},serverRendered:a,userAnnuals:[],actionType:{FETCH:"@\u002Fview\u002Fuser\u002FFETCH",RESET:"@\u002Fview\u002Fuser\u002FRESET",UPDATE:"@\u002Fview\u002Fuser\u002FUPDATE",FETCH_ANNUALS:"@\u002Fview\u002Fuser\u002FFETCH_ANNUALS"},detailList:{actionType:{RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FRESET"},likeList:{list:[],cursor:f,hasMore:a,loading:a,skeleton:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FlikePostList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FlikePostList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FlikePostList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FlikePostList\u002FRESET"}},postList:{list:[],hasMore:a,skeleton:a,loading:a,sort:o,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FpostList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FpostList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FpostList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FpostList\u002FRESET"}},searchList:{list:[],hasMore:a,skeleton:a,loading:a,key_word:e,search_type:b,cursor:f,isPostSearch:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FsearchList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FsearchList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FsearchList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FsearchList\u002FRESET"}},tagList:{list:[],loading:a,skeleton:a,hasMore:a,cursor:f,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FtagList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FtagList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FtagList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FtagList\u002FRESET"}},collectionList:{list:[],userId:e,skeleton:a,hasMore:a,cursor:f,type:"created",loading:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FRESET",TOGGLE_FOLLOW_COLLECTION:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FTOGGLE_FOLLOW_COLLECTION",FOLLOW_COLLECTION:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FFOLLOW_COLLECTION",UNFOLLOW_COLLECTION:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FUNFOLLOW_COLLECTION",DELELTE_COLLECTION:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FDELELTE_COLLECTION",ADD_COLLECTION:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FADD_COLLECTION",EDIT_COLLECTION:"@\u002Fview\u002Fuser\u002FdetailList\u002FcollectionList\u002FEDIT_COLLECTION"}},followerList:{list:[],cursor:f,hasMore:a,loading:a,skeleton:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowerList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowerList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowerList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowerList\u002FRESET"}},followingList:{list:[],cursor:f,hasMore:a,skeleton:a,loading:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowingList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowingList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowingList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowingList\u002FRESET"}},followingTeamsList:{list:[],cursor:f,hasMore:a,skeleton:a,loading:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowingTeamsList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowingTeamsList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowingTeamsList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FfollowingTeamsList\u002FRESET"}},activityList:{list:[],cursor:f,hasMore:a,loading:a,skeleton:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FactivityList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FactivityList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FactivityList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FactivityList\u002FRESET"}},bookList:{list:[],cursor:f,skeleton:a,hasMore:a,loading:a,type:"wrote",actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FbookList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FbookList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FbookList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FbookList\u002FRESET"}},pinList:{list:[],hasMore:a,loading:a,skeleton:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FpinList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FpinList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FpinList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FpinList\u002FRESET"}},courseList:{list:[],hasMore:a,loading:a,skeleton:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FcourseList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FcourseList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FcourseList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FcourseList\u002FRESET"}},pinPraisedList:{list:[],cursor:f,hasMore:a,loading:a,skeleton:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FpinPraisedList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FpinPraisedList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FpinPraisedList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FpinPraisedList\u002FRESET"}},eventList:{list:[],cursor:f,hasMore:a,loading:a,skeleton:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FeventList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FeventList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FeventList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FeventList\u002FRESET"}},selfColumnList:{list:[],hasMore:a,skeleton:a,loading:a,cursor:f,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FcolumnList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FcolumnList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FcolumnList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FcolumnList\u002FRESET"}},columnFollowedList:{list:[],hasMore:a,skeleton:a,loading:a,cursor:f,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002FcolumnFollowedList\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002FcolumnFollowedList\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002FcolumnFollowedList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002FcolumnFollowedList\u002FRESET",FILTER:"@\u002Fview\u002Fuser\u002FdetailList\u002FcolumnFollowedList\u002FFILTER"}},realtimes:{list:[],cursor:f,hasMore:a,loading:a,skeleton:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimes\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimes\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimes\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimes\u002FRESET",DELETE:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimes\u002FDELETE"}},realtimeliked:{list:[],cursor:f,hasMore:a,loading:a,skeleton:a,actionType:{FETCH:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimeliked\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimeliked\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimeliked\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimeliked\u002FRESET",DELETE:"@\u002Fview\u002Fuser\u002FdetailList\u002Frealtimeliked\u002FDELETE"}}}},tag:{tag:{},actionType:{FETCH:"@\u002Fview\u002Ftag\u002FFETCH",FETCH_LIST:"@\u002Fview\u002Ftag\u002FFETCH_LIST",RESET:"@\u002Fview\u002Ftag\u002FRESET"},list:{list:[],cursor:f,loading:a,skeleton:a,hasMore:a,actionType:{UPDATE_STATE:"@\u002Fview\u002Ftag\u002Flist\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Ftag\u002Flist\u002FFETCH_MORE",FETCH:"@\u002Fview\u002Ftag\u002Flist\u002FFETCH",RESET:"@\u002Fview\u002Ftag\u002Flist\u002FRESET"}}},notification:{user:{actionType:{RESET:"@\u002Fview\u002Fnotification\u002Fuser\u002FRESET"},listState:{list:[],cursor:f,hasMore:a,isLoading:a,messageType:p,msgTotal:b,msgSubMap:{"1":b,"2":b,"3":b,"4":b,"7":b}},list:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Fnotification\u002Fuser\u002Flist\u002FUPDATE",FETCH:"@\u002Fview\u002Fnotification\u002Fuser\u002Flist\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Fnotification\u002Fuser\u002Flist\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Fnotification\u002Fuser\u002Flist\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fnotification\u002Fuser\u002Flist\u002FRESET"}}},system:{actionType:{RESET:"@\u002Fview\u002Fnotification\u002Fsystem\u002FRESET"},list:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Fnotification\u002Fsystem\u002Flist\u002FUPDATE",FETCH:"@\u002Fview\u002Fnotification\u002Fsystem\u002Flist\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Fnotification\u002Fsystem\u002Flist\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Fnotification\u002Fsystem\u002Flist\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fnotification\u002Fsystem\u002Flist\u002FRESET"}}}},column:{serverRenderList:a,column:{id:j},entry:{id:j,screenshot:l,liked:a,article_id:j,article_info:{article_id:j,user_id:v,category_id:"6809637773935378440",tag_ids:[6809640642101117000,6809640711177110000,7197380506562871000],visible_level:b,link_url:e,cover_image:"https:\u002F\u002Fp9-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F71b5ff03cf154a65b1906fee95a35e70~tplv-k3u1fbpfcp-watermark.image?",is_gfw:b,title:w,brief_content:x,is_english:b,is_original:g,user_index:9.81510264217437,original_type:b,original_author:e,content:e,ctime:"1689051788",mtime:"1694066497",rtime:"1689145631",draft_id:y,view_count:z,collect_count:A,digg_count:r,comment_count:B,hot_index:142,is_hot:b,rank_index:.01271866,status:s,verify_status:g,audit_status:s,mark_content:e,display_count:b,is_markdown:g,app_html_content:e,version:C,web_html_content:"\u003Cstyle\u003E.markdown-body{word-break:break-word;line-height:1.75;font-weight:400;font-size:15px;overflow-x:hidden;color:#333}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{line-height:1.5;margin:30px 0 10px;color:#cca152;position:relative;padding-left:50px;border-bottom:2px solid rgba(209,163,78,.6);padding-bottom:0}.markdown-body h1{font-size:30px}.markdown-body h1:before{content:\"\";width:50px;height:42px;display:block;position:absolute;background:url(data:image\u002Fpng;base64,iVBORw0KGgoAAAANSUhEUgAAACcAAAAhBAMAAACo1K8bAAAABGdBTUEAALGPC\u002FxhBQAAAAFzUkdCAK7OHOkAAAFZaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIj4KICAgICAgICAgPHRpZmY6T3JpZW50YXRpb24+MTwvdGlmZjpPcmllbnRhdGlvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkzCJ1kAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAnUExURUdwTMyhUsqkUMyhUsyiUcyhUsyhUsyhUcyhUsyhUs2hUtytWNWoVX44si8AAAAKdFJOUwD8Bfcge95QncCWSSDAAAABh0lEQVQoz2WSPUvDQBjHjyPSOB7drh2OUOs36NA6pKAFwaEo4tDFIUWxGYLgKw4BEYUuHaQg7XJ0kccsUmrT2qUUQZp8KO\u002Fy1hSfIYEf\u002F5eHu0MoGU1+sIbSw5Bypd92S+dWWrd34b15vuOPY4QxuvtYZp0lIVk3Zgjt+5zkh12AvBHnbW85BOjXySPwhc5CYcY0OdDh5dEUqIEj4cN8Dty3a1MqYJS4MaWU5wzVLwMMxqGSNQgA\u002FVFa4gf50yhxSUW+db8ACa2gBxfnABVDFYHCPYrc7TLwCepJM8\u002FZoVsRwpxdHEozHUXd6idwVzFFM5BFmIhQVQjrNdnCvdd48wblI2VHtsAZioSsTYVwLoXvjMXH1icuNgPhQE+Ot1+xi8HeA3d1Df1f1JPVUOmsYLujBjuSySqSXYt+yTwbJ0pcyIhqTrxkn0BazRLizJosxRBued+z0jPD6VewOXl5utHRGmMNK3k0iTkzxpq2\u002FoIQO6gLprF12kT\u002FR20eyzlcg7iwK0dPsz\u002FibpdsCHweWwAAAABJRU5ErkJggg==) 0 0 no-repeat;background-size:80%;bottom:-10px;left:-2px}.markdown-body h2{font-size:24px}.markdown-body h2:before{content:\"\";width:50px;height:42px;display:block;position:absolute;background:url(data:image\u002Fpng;base64,iVBORw0KGgoAAAANSUhEUgAAACcAAAAhBAMAAACo1K8bAAAABGdBTUEAALGPC\u002FxhBQAAAAFzUkdCAK7OHOkAAAFZaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIj4KICAgICAgICAgPHRpZmY6T3JpZW50YXRpb24+MTwvdGlmZjpPcmllbnRhdGlvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkzCJ1kAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAnUExURUdwTMyhUsqkUMyhUsyiUcyhUsyhUsyhUcyhUsyhUs2hUtytWNWoVX44si8AAAAKdFJOUwD8Bfcge95QncCWSSDAAAABh0lEQVQoz2WSPUvDQBjHjyPSOB7drh2OUOs36NA6pKAFwaEo4tDFIUWxGYLgKw4BEYUuHaQg7XJ0kccsUmrT2qUUQZp8KO\u002Fy1hSfIYEf\u002F5eHu0MoGU1+sIbSw5Bypd92S+dWWrd34b15vuOPY4QxuvtYZp0lIVk3Zgjt+5zkh12AvBHnbW85BOjXySPwhc5CYcY0OdDh5dEUqIEj4cN8Dty3a1MqYJS4MaWU5wzVLwMMxqGSNQgA\u002FVFa4gf50yhxSUW+db8ACa2gBxfnABVDFYHCPYrc7TLwCepJM8\u002FZoVsRwpxdHEozHUXd6idwVzFFM5BFmIhQVQjrNdnCvdd48wblI2VHtsAZioSsTYVwLoXvjMXH1icuNgPhQE+Ot1+xi8HeA3d1Df1f1JPVUOmsYLujBjuSySqSXYt+yTwbJ0pcyIhqTrxkn0BazRLizJosxRBued+z0jPD6VewOXl5utHRGmMNK3k0iTkzxpq2\u002FoIQO6gLprF12kT\u002FR20eyzlcg7iwK0dPsz\u002FibpdsCHweWwAAAABJRU5ErkJggg==) 0 0 no-repeat;background-size:70%;bottom:-15px;left:-1px}.markdown-body h3{font-size:18px}.markdown-body h3:before{content:\"\";width:50px;height:42px;display:block;position:absolute;background:url(data:image\u002Fpng;base64,iVBORw0KGgoAAAANSUhEUgAAACcAAAAhBAMAAACo1K8bAAAABGdBTUEAALGPC\u002FxhBQAAAAFzUkdCAK7OHOkAAAFZaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIj4KICAgICAgICAgPHRpZmY6T3JpZW50YXRpb24+MTwvdGlmZjpPcmllbnRhdGlvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkzCJ1kAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAnUExURUdwTMyhUsqkUMyhUsyiUcyhUsyhUsyhUcyhUsyhUs2hUtytWNWoVX44si8AAAAKdFJOUwD8Bfcge95QncCWSSDAAAABh0lEQVQoz2WSPUvDQBjHjyPSOB7drh2OUOs36NA6pKAFwaEo4tDFIUWxGYLgKw4BEYUuHaQg7XJ0kccsUmrT2qUUQZp8KO\u002Fy1hSfIYEf\u002F5eHu0MoGU1+sIbSw5Bypd92S+dWWrd34b15vuOPY4QxuvtYZp0lIVk3Zgjt+5zkh12AvBHnbW85BOjXySPwhc5CYcY0OdDh5dEUqIEj4cN8Dty3a1MqYJS4MaWU5wzVLwMMxqGSNQgA\u002FVFa4gf50yhxSUW+db8ACa2gBxfnABVDFYHCPYrc7TLwCepJM8\u002FZoVsRwpxdHEozHUXd6idwVzFFM5BFmIhQVQjrNdnCvdd48wblI2VHtsAZioSsTYVwLoXvjMXH1icuNgPhQE+Ot1+xi8HeA3d1Df1f1JPVUOmsYLujBjuSySqSXYt+yTwbJ0pcyIhqTrxkn0BazRLizJosxRBued+z0jPD6VewOXl5utHRGmMNK3k0iTkzxpq2\u002FoIQO6gLprF12kT\u002FR20eyzlcg7iwK0dPsz\u002FibpdsCHweWwAAAABJRU5ErkJggg==) 0 0 no-repeat;background-size:60%;bottom:-19px;left:-2px}.markdown-body h4{font-size:16px;padding-left:0;border-bottom:1px solid rgba(209,163,78,.6)}.markdown-body h5{font-size:15px;padding-left:0}.markdown-body em{color:#cca152}.markdown-body del{text-decoration-color:#cca152;text-decoration-thickness:2px}.markdown-body p{line-height:inherit;margin-top:22px;margin-bottom:22px}.markdown-body img{max-width:80%;margin:6px auto;box-shadow:0 6px 15px #8e8e8e;display:block;margin:20px auto!important;object-fit:contain;border-radius:8px}.markdown-body hr{border:none;border-top:2px solid #e0c9a1;margin-top:32px 0}.markdown-body code{word-break:break-word;border-radius:2px;overflow-x:auto;background:#f6efde;color:#b69454;font-size:.87em;padding:.065em .4em}.markdown-body code,.markdown-body pre{font-family:Mono,Menlo,Monaco,Consolas,Courier New,monospace}.markdown-body pre{overflow:auto;position:relative;line-height:1.75;background:#fef6e1;border-radius:4px;box-shadow:0 0 8px hsla(0,0%,47%,.45)}.markdown-body pre:before{content:\"\";display:block;height:30px;width:100%;margin-bottom:-7px;background:url(data:image\u002Fpng;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAOCAMAAABaWb9VAAAABGdBTUEAALGPC\u002FxhBQAAAAFzUkdCAK7OHOkAAAFZaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIj4KICAgICAgICAgPHRpZmY6T3JpZW50YXRpb24+MTwvdGlmZjpPcmllbnRhdGlvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkzCJ1kAAAAJcEhZcwAADsQAAA7EAZUrDhsAAACHUExURUdwTMxCKdc8NvdYT\u002F9hVyLJPABBAGubKNiIOv+\u002FK+ytJBakH9aZG+5QR5VZDOBDPhmtJ8+VGuGjH\u002FCwJR26MR6+NBq0LPW1KBmwKetNRfJUTONHP92fHuSmIeFEPhu3LR29M\u002FBTS+mqIuqsI5qdHyLKPP++Kv9gVf9lW\u002F\u002FKLSXXQCTQP\u002F\u002FFLMVm5KQAAAAldFJOUwAlPPz7+woBBPu8Mzu+FlRRKmvnweeG+Wqg6mVNXmuc1OCbmjZJWF\u002F9AAABKklEQVQoz3WS6XaCMBCFRzAM++KGsqhtDwES3\u002F\u002F5OtmA2uP9A9zwzRoAgBC0QgQrdA68OZsDr229YGHoIEj7Pl0ZOgjKa5lYJ4Rd1vijn3nuD4Qurjmv48o6RFyfbGDnS6DeEXZfk5ZfuBhdPb9I87ECNMh1EFJKIU6agWzaa01NbmLkxznSmmObJBkkUxrERf3i+ftRiZi7ShPCYY64UhTx1AR5CDYoMXkOKEY7GmTcTzeD\u002FFiER68DfSLgSRqEIBoB3P8h3x8RZhBXGJGusJdD6rfCBl0YqvZNkrV9zej2cWlfJWG6fRpyM41qYH+GTPPi2yFLQYC0Qybm5o96lW77kMbcrBKtgeWTsthVamNXFF4I6x0DTPuugsVRFyYpyxzWqNvHJwed8ws7QyP1UwjNjwAAAABJRU5ErkJggg==) 10px 10px no-repeat;background-size:40px}.markdown-body pre\u003Ecode{font-size:12px;padding:15px 12px;margin:0;word-break:normal;display:block;overflow-x:auto;color:#333;background:#fef6e1}.markdown-body a{text-decoration:none;color:#d8ac5a;border-bottom:1px solid #d8ac5a}.markdown-body a:active,.markdown-body a:hover{color:#93753f;border-color:#93753f}.markdown-body table{display:inline-block!important;font-size:12px;width:auto;max-width:100%;overflow:auto;border:1px solid #f4e8c7;border-collapse:collapse}.markdown-body thead{background:rgba(255,227,176,.6588235294);text-align:left;display:table-header-group;border-bottom:1px solid rgba(255,227,176,.6588235294)}.markdown-body tbody{background:rgba(255,247,229,.3882352941)}.markdown-body td,.markdown-body th{padding:7px;line-height:24px;min-width:100px}.markdown-body blockquote{color:#bd954f;padding:1px 23px;margin:22px 0;border-left:4px solid #dcb267;background-color:#fff7e5}.markdown-body blockquote:after{display:block;content:\"\"}.markdown-body blockquote\u003Ep{margin:10px 0}.markdown-body ol,.markdown-body ul{padding-left:28px}.markdown-body ol .task-list-item,.markdown-body ul .task-list-item{list-style:none}.markdown-body ol li{padding-left:6px}.markdown-body::marker{color:#dcb267}.markdown-body .contains-task-list{padding-left:0}.markdown-body .contains-task-list .task-list-item{list-style:none;position:relative}.markdown-body .contains-task-list .task-list-item input[type=checkbox]{position:relative;top:2px}.markdown-body .contains-task-list .task-list-item input[type=checkbox]:before{content:\"\";display:inline-block;height:12px;width:12px;position:absolute;left:-2px;top:-2px;border:2px solid #cda152;border-radius:5px}.markdown-body .contains-task-list .task-list-item input[type=checkbox]:checked{position:relative;top:2px}.markdown-body .contains-task-list .task-list-item input[type=checkbox]:checked:before{border:none;content:\"\";display:inline-block;height:17px;width:17px;position:absolute;left:-2px;top:-2px;background:url(data:image\u002Fpng;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAFo9M\u002F3AAAAAXNSR0IArs4c6QAAAYhJREFUOBFjYACC0\u002FMDGsDEiyvr\u002FzOCeUwMJxn\u002FA8HZRUEMYBFGJsZ6kFoQYALiAyAGCgDpA+sFijKeWRj4H1kWpIWBW1SDQcm+BCzOAiK\u002Fvr7BcO\u002FgDbAAI4hE1waWgRBnmWCGIwkyKFjnwow0BhsJk9QLncvw5dV1oPE9MCEGmBWrgCKhcFEowyR+PSNYwemFAZ4M\u002FxjMkRWYJm5oAPFB\u002FjoDpI1BHHQANgGPDxj+\u002F\u002FvfCA4IdJ3GcevgQnAFhlHLwYIgSVA0wABcwY3tFQzokiBFGIEP0wmigW5wZAK5FFkQib0a6NUDcEmgb7AGFpIGZOZqoMFhIAFQknEAJpn9yLLEssFOBCp2IFaDkl0xg6C8FbJyB3gowUS5RdQYBORQYpVB1iwVHIIMwJh\u002F\u002F\u002FAYTCmYRklNIBFmNi4GeYt0BhnjeIa3dw8wSBlEgDUhxw2yCRgGfHp2geHiqiSwUwUVrFAiFVkjjA1LrjgTHEwhFvosMCZM4NEIUoAtWWNoBGZ70\u002FgN22HiAD2uhAzsYNBZAAAAAElFTkSuQmCC) 0 0 no-repeat;background-size:100%}@media (max-width:720px){.markdown-body h1{font-size:24px}.markdown-body h2{font-size:20px}.markdown-body h3{font-size:18px}}\u003C\u002Fstyle\u003E\u003Cstyle data-highlight\u003E.markdown-body pre,.markdown-body pre\u003Ecode.hljs{color:#333;background:#f8f8f8}.hljs-comment,.hljs-quote{color:#998;font-style:italic}.hljs-keyword,.hljs-selector-tag,.hljs-subst{color:#333;font-weight:700}.hljs-literal,.hljs-number,.hljs-tag .hljs-attr,.hljs-template-variable,.hljs-variable{color:teal}.hljs-doctag,.hljs-string{color:#d14}.hljs-section,.hljs-selector-id,.hljs-title{color:#900;font-weight:700}.hljs-subst{font-weight:400}.hljs-class .hljs-title,.hljs-type{color:#458;font-weight:700}.hljs-attribute,.hljs-name,.hljs-tag{color:navy;font-weight:400}.hljs-link,.hljs-regexp{color:#009926}.hljs-bullet,.hljs-symbol{color:#990073}.hljs-built_in,.hljs-builtin-name{color:#0086b3}.hljs-meta{color:#999;font-weight:700}.hljs-deletion{background:#fdd}.hljs-addition{background:#dfd}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}\u003C\u002Fstyle\u003E\u003Cblockquote\u003E\n\u003Cp\u003E\u003Cstrong\u003E⚠️⚠️⚠️本文为稀土掘金技术社区首发签约文章，30天内禁止转载，30天后未获授权禁止转载，侵权必究！\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003E大家好，最近越演越热的AIGC浪潮，将Transformer这个模型带进了大家的视野。如果你从事NLP相关的职业，你一定知道Bert（来自Transformer的Encoder）在5年前卷起的NLP预训练的热潮。而在当时因为受到训练数据和算力的限制，以GPT（来自Transformer的Decoder）为代表的生成式自然语言模型，还没有展现出惊人的涌现能力，但它却启发了人们对“自监督训练”的不懈研究，以此来解决“自然语言训练中标注数据不足”的问题。时间来到2020年，此时还是Bert独占鳌头，在Transformer Encoder架构的启发下，CV算法工程师们开始思考一个问题：”当CNN的架构快做到极致时，我们能否换一个新方向？”于是这一年，Google推出了VIT（Vision Transformer）：\u003Cstrong\u003E一个和Bert几乎一致，同时不添加任何卷积结构的图像分类模型\u003C\u002Fstrong\u003E。VIT在Transformer上的成功，\u003Cstrong\u003E证明了可以用统一的模型，来处理不同领域（语言\u002F图像\u002F视频）的任务\u003C\u002Fstrong\u003E，进而开启了多模态模型研究的新篇章。VIT作为众多大模型的backbone（骨架结构），是我们在研究AIGC时绕不过的话题。\u003C\u002Fp\u003E\n\u003Cp\u003E今天这篇文章，就和大家一起来全面解读VIT。如果大家对Transformer和Bert不了解，强烈建议大家在阅读本文前速读以下2篇文章：\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7241859817563389989\" target=\"_blank\" title=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7241859817563389989\"\u003E深入浅出Transformer系列之二：Self-attention（attention是Transformer的核心模块）\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Flink.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F461267517\" target=\"_blank\" title=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F461267517\" ref=\"nofollow noopener noreferrer\"\u003E深入浅出Bert系列之一：模型原理篇\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003ECV大模型系列文章导航（持续更新中）：\u003Cbr\u003E\n🌸\u003Ca href=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7251391372394053691\" title=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7251391372394053691\" target=\"_blank\"\u003ECV大模型系列之：扩散模型基石DDPM（模型架构篇）\u003C\u002Fa\u003E🌸\u003Cbr\u003E\n🌸\u003Ca href=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7251399225425494071\" title=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7251399225425494071\" target=\"_blank\"\u003ECV大模型系列之：扩散模型基石DDPM（人人都能看懂的数学原理篇）\u003C\u002Fa\u003E🌸\u003Cbr\u003E\n🌸\u003Ca href=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7258069406961352764\" title=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7258069406961352764\" target=\"_blank\"\u003ECV大模型系列之：扩散模型基石DDPM（源码解读与实操篇）\u003C\u002Fa\u003E🌸\u003Cbr\u003E\n🌸\u003Ca href=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7254341178258489404\" title=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7254341178258489404\" target=\"_blank\"\u003ECV大模型系列之：全面解读VIT，它到底给植树人挖了多少坑\u003C\u002Fa\u003E🌸\u003Cbr\u003E\n🌸\u003Ca href=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7264503343996747830\" title=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7264503343996747830\" target=\"_blank\"\u003ECV大模型系列之：多模态经典之作CLIP，探索图文结合的奥秘\u003C\u002Fa\u003E🌸\u003Cbr\u003E\n🌸\u003Ca href=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7267417057438777399\" title=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7267417057438777399\" target=\"_blank\"\u003ECV大模型系列之：MAE，实现像素级图像重建\u003C\u002Fa\u003E🌸\u003Cbr\u003E\n🌸\u003Ca href=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7272735633457659938\" target=\"_blank\" title=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7272735633457659938\"\u003ECV大模型系列之：MoCo v1，利用对比学习在CV任务上做无监督训练\u003C\u002Fa\u003E🌸 \u003Cbr\u003E\n🌸\u003Ca href=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7275932704533282831\" target=\"_blank\" title=\"https:\u002F\u002Fjuejin.cn\u002Fpost\u002F7275932704533282831\"\u003ECV大模型系列之：DALLE2，OpenAI文生图代表作解读\u003C\u002Fa\u003E🌸\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Ch1 data-id=\"heading-0\"\u003E一、模型架构\u003C\u002Fh1\u003E\n\u003Cp\u003E提起一个新模型，我想大家最关心的事就是：它到底长什么样？输入输出是什么？我要怎么用？\u003C\u002Fp\u003E\n\u003Cp\u003E所以，我们先来看模型架构。\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-1\"\u003E1.1 Bert架构\u003C\u002Fh2\u003E\n\u003Cp\u003E前面说过，VIT几乎和Bert一致，我们来速扫一下Bert模型：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002Fc37c1df00f454ea78385f77d7a112b36~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003Einput：输入是一条文本。文本中的每个词（token）我们都通过embedding把它表示成了向量的形式。、\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003E训练任务：在Bert中，我们同时做2个训练任务：\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003ENext Sentence Prediction Model（下一句预测）\u003C\u002Fstrong\u003E ：input中会包含两个句子，这两个句子有50%的概率是真实相连的句子，50%的概率是随机组装在一起的句子。我们在每个input前面增加特殊符\u003Ccode\u003E&#x3C;cls\u003E\u003C\u002Fcode\u003E，这个位置所在的token将会在训练里不断学习整条文本蕴含的信息。最后它将作为“下一句预测”任务的输入向量，该任务是一个二分类模型，输出结果表示两个句子是否真实相连。\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EMasked Language Model（遮蔽词猜测）\u003C\u002Fstrong\u003E ：在input中，我们会以一定概率随机遮盖掉一些token（\u003Ccode\u003E&#x3C;mask\u003E\u003C\u002Fcode\u003E)，以此来强迫模型通过Bert中的attention结构更好抽取上下文信息，然后在“遮蔽词猜测”任务重，准确地将被覆盖的词猜测出来。\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EBert模型：Transformer的Encoder层。\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E关于Bert更详细的介绍，可以参考文章开头给出的链接文章。\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-2\"\u003E1.2 VIT模型架构\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002Ffb49b6e44c104f8197f8fc1eff1f6bb8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E我们先来看左侧部分。\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EPatch\u003C\u002Fstrong\u003E：对于输入图片，首先将它分成几个patch（例如图中分为9个patch），每个patch就类似于NLP中的一个token（具体如何将patch转变为token向量，在下文会细说）。\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EPosition Embedding\u003C\u002Fstrong\u003E：每个patch的位置向量，用于指示对应patch在原始图片中的位置。和Bert一样，这个位置向量是learnable的，而并非原始Transformer中的函数式位置向量。同样，我们会在下文详细讲解这一块。\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EInput:\u003C\u002Fstrong\u003E 最终传入模型的Input = patching_emebdding + position embedding，同样，在输入最开始，我们也加一个分类符\u003Ccode\u003E&#x3C;cls\u003E\u003C\u002Fcode\u003E，在bert中，这个分类符是作为“下一句预测”中的输入，来判断两个句子是否真实相连。\u003Cstrong\u003E在VIT中，这个分类符作为分类任务的输入，来判断原始图片中物体的类别\u003C\u002Fstrong\u003E。\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E右侧部分则详细刻画了Transformer Encoder层的架构，它由L块这样的架构组成。图片已刻画得很详细，这里不再赘述。\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E总结起来，VIT的训练其实就在做一件事\u003C\u002Fstrong\u003E：把图片打成patch，送入Transformer Encoder，然后拿\u003Ccode\u003E&#x3C;cls\u003E\u003C\u002Fcode\u003E对应位置的向量，过一个简单的softmax多分类模型，去预测原始图片中描绘的物体类别即可。\u003C\u002Fp\u003E\n\u003Cp\u003E你可能会想：“这个分类任务只用一个简单的softmax，真得能分准吗？”其实，\u003Cstrong\u003E这就是VIT的精华所在了：VIT的目的不是让这个softmax分类模型强大，而是让这个分类模型的输入强大。这个输入就是Transformer Encoder提炼出来的特征\u003C\u002Fstrong\u003E。分类模型越简单，对特征的要求就越高。\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E所以为什么说Transformer开启了大一统模型的预训练大门呢？主要原因就在于它对特征的提炼能力——这样我们就可以拿这个特征去做更多有趣的任务了\u003C\u002Fstrong\u003E。这也是VIT能成为后续多模态backbone的主要原因。\u003C\u002Fp\u003E\n\u003Ch1 data-id=\"heading-3\"\u003E二、从patch到token\u003C\u002Fh1\u003E\n\u003Cp\u003E讲完了基本框架，我们现在来看细节。首先我们来看看，图片的patch是怎么变成token embedding的。\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-4\"\u003E2.1 patch变token的过程\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F6304e32b158c4b54b3241ad6bc0997a2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E如图，假设原始图片尺寸大小为：\u003Ccode\u003E224*224*3\u003C\u002Fcode\u003E (H * W * C)。\u003C\u002Fp\u003E\n\u003Cp\u003E现在我们要把它切成小patch，\u003Cstrong\u003E每个patch的尺寸设为16\u003C\u002Fstrong\u003E（\u003Cstrong\u003EP=16\u003C\u002Fstrong\u003E），则每个patch下图片的大小为\u003Ccode\u003E16*16*3\u003C\u002Fcode\u003E。\u003C\u002Fp\u003E\n\u003Cp\u003E则容易计算出共有\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmfrac\u003E\u003Cmrow\u003E\u003Cmn\u003E224\u003C\u002Fmn\u003E\u003Cmo\u003E∗\u003C\u002Fmo\u003E\u003Cmn\u003E224\u003C\u002Fmn\u003E\u003C\u002Fmrow\u003E\u003Cmrow\u003E\u003Cmn\u003E16\u003C\u002Fmn\u003E\u003Cmo\u003E∗\u003C\u002Fmo\u003E\u003Cmn\u003E16\u003C\u002Fmn\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmfrac\u003E\u003Cmtext\u003E \u003C\u002Fmtext\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmtext\u003E \u003C\u002Fmtext\u003E\u003Cmn\u003E196\u003C\u002Fmn\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003E\\frac{224*224}{16*16} = 196\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1.1901em;vertical-align:-0.345em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mopen nulldelimiter\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mfrac\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.8451em;\"\u003E\u003Cspan style=\"top:-2.655em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:3em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E16\u003C\u002Fspan\u003E\u003Cspan class=\"mbin mtight\"\u003E∗\u003C\u002Fspan\u003E\u003Cspan class=\"mord mtight\"\u003E16\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.23em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:3em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.394em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:3em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E224\u003C\u002Fspan\u003E\u003Cspan class=\"mbin mtight\"\u003E∗\u003C\u002Fspan\u003E\u003Cspan class=\"mord mtight\"\u003E224\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.345em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mclose nulldelimiter\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E \u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6444em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E 196\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E个patch。\u003C\u002Fp\u003E\n\u003Cp\u003E不难看出每个patch对应着一个token，将每个patch展平，则得到输入矩阵X，其大小为\u003Ccode\u003E(196, 768)\u003C\u002Fcode\u003E，也就是每个token是768维。\u003C\u002Fp\u003E\n\u003Cp\u003E通过这样的方式，我们成功将图像数据处理成自然语言的向量表达方式。\u003C\u002Fp\u003E\n\u003Cp\u003E好，\u003Cstrong\u003E那么现在问题来了，对于图中每一个\u003C\u002Fstrong\u003E \u003Cstrong\u003E\u003Ccode\u003E16*16*3\u003C\u002Fcode\u003E\u003C\u002Fstrong\u003E \u003Cstrong\u003E的小方块，我要怎么把它拉平成\u003C\u002Fstrong\u003E \u003Cstrong\u003E\u003Ccode\u003E1*768\u003C\u002Fcode\u003E\u003C\u002Fstrong\u003E \u003Cstrong\u003E维度的向量呢？\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E比如说，我先把第一个channel拉成一个向量，然后再往后依次接上第二个channel、第三个channel拉平的向量。但这种办法下，同一个pixel本来是三个channel的值共同表达的，现在变成竖直的向量之后，这三个值的距离反而远了。基于这个原因，你可能会想一些别的拉平方式，但归根究底它们都有一个共同的问题：太规则化，太主观。\u003C\u002Fp\u003E\n\u003Cp\u003E所以，\u003Cstrong\u003E有办法利用模型来做更好的特征提取吗\u003C\u002Fstrong\u003E？当然没问题。VIT中最终采用CNN进行特征提取，具体方案如下：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F3008ea3b4de944959d5a6dd5d765a5f9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E采用768个\u003Ccode\u003E16*16*3\u003C\u002Fcode\u003E尺寸的卷积核，stride=16，padding=0。这样我们就能得到\u003Ccode\u003E14*14*768\u003C\u002Fcode\u003E大小的特征图。如同所示，特征图中每一个\u003Ccode\u003E1*1*768\u003C\u002Fcode\u003E大小的子特征图，都是由卷积核对第一块patch做处理而来，因此它就能表示第一块patch的token向量。\u003C\u002Fp\u003E\n\u003Cp\u003E【\u003Cstrong\u003E备注】：\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E你可能会问，\u003Cstrong\u003E前面不是说VIT已经摆脱CNN了吗？这里怎么又用卷积了\u003C\u002Fstrong\u003E？由于这一步只是输入预处理阶段，和主体模型没有关系，只要将其试为一致特征提取方法即可，并不影响我们之前的结论。\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-5\"\u003E2.2 为什么要处理成patch\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cstrong\u003E你可能想问，为什么一定要先分patch，再从patch转token呢？\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E第一个原因，是为了减少模型计算量。\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E在Transformer中，假设输入的序列长度为N，那么经过attention时，计算复杂度就为\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003EO\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmsup\u003E\u003Cmi\u003EN\u003C\u002Fmi\u003E\u003Cmn\u003E2\u003C\u002Fmn\u003E\u003C\u002Fmsup\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003EO(N^{2})\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003EO\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003EN\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.8141em;\"\u003E\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E2\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E，因为注意力机制下，每个token都要和包括自己在内的所有token做一次attention score计算。\u003C\u002Fp\u003E\n\u003Cp\u003E在VIT中，\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003EN\u003C\u002Fmi\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmfrac\u003E\u003Cmrow\u003E\u003Cmi\u003EH\u003C\u002Fmi\u003E\u003Cmo\u003E∗\u003C\u002Fmo\u003E\u003Cmi\u003EW\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cmsup\u003E\u003Cmi\u003EP\u003C\u002Fmi\u003E\u003Cmn\u003E2\u003C\u002Fmn\u003E\u003C\u002Fmsup\u003E\u003C\u002Fmfrac\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003EN=\\frac{H*W}{P^{2}}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6833em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003EN\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1.2173em;vertical-align:-0.345em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mopen nulldelimiter\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mfrac\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.8723em;\"\u003E\u003Cspan style=\"top:-2.655em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:3em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003EP\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.7463em;\"\u003E\u003Cspan style=\"top:-2.786em;margin-right:0.0714em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.5em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size3 size1 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E2\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.23em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:3em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.394em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:3em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\"\u003EH\u003C\u002Fspan\u003E\u003Cspan class=\"mbin mtight\"\u003E∗\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003EW\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.345em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mclose nulldelimiter\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E，当patch尺寸P越小时，N越大，此时模型的计算量也就越大。因此，我们需要找到一个合适的P值，来减少计算压力。\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E第二个原因，是图像数据带有较多的冗余信息。\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E和语言数据中蕴含的丰富语义不同，像素本身含有大量的冗余信息。比如，相邻的两个像素格子间的取值往往是相似的。因此我们并不需要特别精准的计算粒度（比如把P设为1）。这个特性也是之后MAE，MoCo之类的像素级预测模型能够成功的原因之一。\u003C\u002Fp\u003E\n\u003Ch1 data-id=\"heading-6\"\u003E三、Emebdding\u003C\u002Fh1\u003E\n\u003Cp\u003E如下图，我们知道在Bert（及其它NLP任务中）：\u003C\u002Fp\u003E\n\u003Cp\u003E输入 = \u003Cstrong\u003Etoken_embedding\u003C\u002Fstrong\u003E(将单个词转变为词向量) + \u003Cstrong\u003Eposition_embedding\u003C\u002Fstrong\u003E(位置编码，用于表示token在输入序列中的位置) + \u003Cstrong\u003Esegment_emebdding(\u003C\u002Fstrong\u003E 非必须，在bert中用于表示每个词属于哪个句子)。\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E在VIT中，同样存在token_embedding和postion_emebedding\u003C\u002Fstrong\u003E。\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F0ca39caa2d5a480592042c52139e7ccc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-7\"\u003E3.1 Token Emebdding\u003C\u002Fh2\u003E\n\u003Cp\u003E我们记token emebdding为\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmtext\u003E，则\u003C\u002Fmtext\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003E，则\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6833em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord cjk_fallback\"\u003E，则\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E是一个形状为\u003Ccode\u003E(768, 768)\u003C\u002Fcode\u003E的矩阵。\u003C\u002Fp\u003E\n\u003Cp\u003E由前文知经过patch处理后输入$$$$的形状为\u003Ccode\u003E(196, 768)\u003C\u002Fcode\u003E，则输入X过toke_embedding后的结果为：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003EX\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003ET\u003C\u002Fmi\u003E\u003Cmi\u003EE\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmi\u003EX\u003C\u002Fmi\u003E\u003Cmo\u003E∗\u003C\u002Fmo\u003E\u003Cmi\u003EE\u003C\u002Fmi\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmn\u003E196\u003C\u002Fmn\u003E\u003Cmo separator=\"true\"\u003E,\u003C\u002Fmo\u003E\u003Cmn\u003E768\u003C\u002Fmn\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo\u003E∗\u003C\u002Fmo\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmn\u003E768\u003C\u002Fmn\u003E\u003Cmo\u003E∗\u003C\u002Fmo\u003E\u003Cmn\u003E768\u003C\u002Fmn\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmn\u003E196\u003C\u002Fmn\u003E\u003Cmo separator=\"true\"\u003E,\u003C\u002Fmo\u003E\u003Cmn\u003E768\u003C\u002Fmn\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003EX_{TE} = X * E = (196, 768) * (768 * 768) = (196, 768)\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003EX\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3283em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\"\u003ETE\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.15em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6833em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003EX\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mbin\"\u003E∗\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6833em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\"\u003EE\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E196\u003C\u002Fspan\u003E\u003Cspan class=\"mpunct\"\u003E,\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E768\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mbin\"\u003E∗\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E768\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mbin\"\u003E∗\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E768\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E196\u003C\u002Fspan\u003E\u003Cspan class=\"mpunct\"\u003E,\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E768\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E你可能想问，输入X本来就是一个\u003C\u002Fstrong\u003E \u003Cstrong\u003E\u003Ccode\u003E(196，768)\u003C\u002Fcode\u003E\u003C\u002Fstrong\u003E \u003Cstrong\u003E的矩阵啊，我为什么还要过一次embedding呢？\u003C\u002Fstrong\u003E 这个问题的关键不在于数据的维度，而在于embedding的含义。原始的X仅是由数据预处理而来，和主体模型毫无关系。而token_embedding却参与了主体模型训练中的梯度更新，在使用它之后，能更好地表示出token向量。更进一步，E的维度可以表示成\u003Ccode\u003E(768, x)\u003C\u002Fcode\u003E的形式，也就是第二维不一定要是768，你可以自由设定词向量的维度。\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-8\"\u003E3.2 Position Embedding（位置向量）\u003C\u002Fh2\u003E\n\u003Cp\u003E在NLP任务中，位置向量的目的是让模型学得token的位置信息。在VIT中也是同理，我们需要让模型知道每个patch的位置信息（参见1.2中架构图）。\u003C\u002Fp\u003E\n\u003Cp\u003E我们记位置向量为\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003EE\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ep\u003C\u002Fmi\u003E\u003Cmi\u003Eo\u003C\u002Fmi\u003E\u003Cmi\u003Es\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003EE_{pos}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\"\u003EE\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.1514em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ep\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Eos\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E，则它是一个形状为\u003Ccode\u003E(196，768)\u003C\u002Fcode\u003E的矩阵，表示196个维度为768的向量，\u003Cstrong\u003E每个向量表示对应token的位置信息\u003C\u002Fstrong\u003E。\u003C\u002Fp\u003E\n\u003Cp\u003E构造位置向量的方法有很多种，在VIT中，作者做了不同的消融实验，来验证不同方案的效果（论文附录D.4）部分，我们来详细看看，作者都曾尝试过哪些方案。\u003C\u002Fp\u003E\n\u003Ch3 data-id=\"heading-9\"\u003E方案一： 不添加任何位置信息\u003C\u002Fh3\u003E\n\u003Cp\u003E将输入视为一堆无序的patch，不往其中添加任何位置向量。\u003C\u002Fp\u003E\n\u003Ch3 data-id=\"heading-10\"\u003E方案二：使用1-D绝对位置编码\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003E也就是我们在上文介绍的方案，这也是VIT最终选定的方案。\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E1-D绝对位置编码又分为\u003Cstrong\u003E函数式\u003C\u002Fstrong\u003E（Transformer的三角函数编码，详情可参见这篇文章）和\u003Cstrong\u003E可学习式\u003C\u002Fstrong\u003E（Bert采用编码方式），VIT采用的是后者。之所以被称为“绝对位置编码”，是因为位置向量代表的是token的绝对位置信息（例如第1个token，第2个token之类）。\u003C\u002Fp\u003E\n\u003Ch3 data-id=\"heading-11\"\u003E方案三：使用2-D绝对位置编码\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002Ff2590652d07b40899f07d9e110244dab~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E如图所示，因为图像数据的特殊性，在2-D位置编码中，认为按全局绝对位置信息来表示一个patch是不足够的（如左侧所示），一个patch在x轴和y轴上具有不同含义的位置信息（如右侧所示）。因此，2-D位置编码将原来的PE向量拆成两部分来分别训练。\u003C\u002Fp\u003E\n\u003Ch3 data-id=\"heading-12\"\u003E方案四：相对位置编码（relative positional embeddings）\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F0eee86026a924bf78ac9a3db6a464a0a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E相对位置编码（RPE）的设计思想是：\u003Cstrong\u003E我们不应该只关注patch的绝对位置信息，更应该关注patch间的相对位置信息\u003C\u002Fstrong\u003E。如图所示，对于token4，它和其余每一个token间都存在相对位置关系，我们分别用\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ew\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmo\u003E−\u003C\u002Fmo\u003E\u003Cmn\u003E3\u003C\u002Fmn\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003Cmo separator=\"true\"\u003E,\u003C\u002Fmo\u003E\u003Cmsub\u003E\u003Cmi\u003Ew\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmo\u003E−\u003C\u002Fmo\u003E\u003Cmn\u003E2\u003C\u002Fmn\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003Cmo separator=\"true\"\u003E,\u003C\u002Fmo\u003E\u003Cmi mathvariant=\"normal\"\u003E.\u003C\u002Fmi\u003E\u003Cmi mathvariant=\"normal\"\u003E.\u003C\u002Fmi\u003E\u003Cmi mathvariant=\"normal\"\u003E.\u003C\u002Fmi\u003E\u003Cmsub\u003E\u003Cmi\u003Ew\u003C\u002Fmi\u003E\u003Cmn\u003E1\u003C\u002Fmn\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ew_{-3}, w_{-2}, ... w_{1}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003Ew\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3011em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E−\u003C\u002Fspan\u003E\u003Cspan class=\"mord mtight\"\u003E3\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2083em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mpunct\"\u003E,\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003Ew\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3011em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E−\u003C\u002Fspan\u003E\u003Cspan class=\"mord mtight\"\u003E2\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2083em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mpunct\"\u003E,\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E...\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003Ew\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3011em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E1\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.15em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E这5个向量来表示这种位置关系。那么接下来，\u003Cstrong\u003E只要在正常计算attention的过程中，将这5个向量当作bias添加到计算过程中（如图公式所示），我们就可以正常训练这些相对位置向量了\u003C\u002Fstrong\u003E。为了减少训练时的参数量，我们还可以做\u003Cstrong\u003Eclip操作\u003C\u002Fstrong\u003E，在制定clip的步数k之后，在k范围之外的w我们都用固定的w表示。例如图中当k=2时，向token4的前方找，我们发现\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ew\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmo\u003E−\u003C\u002Fmo\u003E\u003Cmn\u003E3\u003C\u002Fmn\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ew_{-3}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003Ew\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3011em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E−\u003C\u002Fspan\u003E\u003Cspan class=\"mord mtight\"\u003E3\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2083em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E已经在k=2步之外了，因此就可以用\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ew\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmo\u003E−\u003C\u002Fmo\u003E\u003Cmn\u003E2\u003C\u002Fmn\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ew_{-2}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003Ew\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3011em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E−\u003C\u002Fspan\u003E\u003Cspan class=\"mord mtight\"\u003E2\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2083em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E来替代\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ew\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmo\u003E−\u003C\u002Fmo\u003E\u003Cmn\u003E3\u003C\u002Fmn\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ew_{-3}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003Ew\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3011em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E−\u003C\u002Fspan\u003E\u003Cspan class=\"mord mtight\"\u003E3\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2083em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E，如果token1之前还有token，那么它们的w都可用\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ew\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmo\u003E−\u003C\u002Fmo\u003E\u003Cmn\u003E2\u003C\u002Fmn\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ew_{-2}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003Ew\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3011em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E−\u003C\u002Fspan\u003E\u003Cspan class=\"mord mtight\"\u003E2\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2083em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E替代。向token4的后方找，发现大家都在k=2步之内，因此无需做任何替换操作。\u003C\u002Fp\u003E\n\u003Cp\u003E关于相对位置编码的更多信息，可以阅读原始论文\u003Ca href=\"https:\u002F\u002Flink.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1803.02155.pdf\" target=\"_blank\" title=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.02155.pdf\" ref=\"nofollow noopener noreferrer\"\u003Earxiv.org\u002Fpdf\u002F1803.02…\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch3 data-id=\"heading-13\"\u003E实验结果\u003C\u002Fh3\u003E\n\u003Cp\u003E这四种位置编码方案的实验结果如下：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F04d317d554dd49a587e42987876b7c70~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E可以发现除了“不加任何位置编码”的效果显著低之外，其余三种方案的结果都差不多。所以作者们当然选择最快捷省力的1-D位置编码方案啦。当你在阅读VIT的论文中，会发现大量的消融实验细节（例如分类头\u003Ccode\u003E&#x3C;cls\u003E\u003C\u002Fcode\u003E要怎么加），\u003Cstrong\u003E作者这样做的目的也很明确：“我们的方案是在诸多可行的方法中，逐一做实验比对出来的，是全面考虑后的结果。”\u003C\u002Fstrong\u003E 这也是我一直觉得这篇论文在技术之外值得借鉴和反复读的地方。\u003C\u002Fp\u003E\n\u003Ch1 data-id=\"heading-14\"\u003E四、模型架构的数学表达\u003C\u002Fh1\u003E\n\u003Cp\u003E到这一步位置，我们已基本将VIT的模型架构部分讲完了。结合1.2中的模型架构图，我们来用数学语言简练写一下训练中的计算过程：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F91e5a04120bd46a19e5ada33fc494d0e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E(1）即是我们说的图像预处理过程:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003E\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsubsup\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmi\u003Ep\u003C\u002Fmi\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003C\u002Fmsubsup\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ex_{p}^{i}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1.2078em;vertical-align:-0.3831em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.8247em;\"\u003E\u003Cspan style=\"top:-2.453em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ep\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ei\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3831em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E：第i块patch\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003E\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003EE\u003C\u002Fmi\u003E\u003Cmo separator=\"true\"\u003E,\u003C\u002Fmo\u003E\u003Cmtext\u003E \u003C\u002Fmtext\u003E\u003Cmsub\u003E\u003Cmi\u003EE\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ep\u003C\u002Fmi\u003E\u003Cmi\u003Eo\u003C\u002Fmi\u003E\u003Cmi\u003Es\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003EE, E_{pos}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\"\u003EE\u003C\u002Fspan\u003E\u003Cspan class=\"mpunct\"\u003E,\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E \u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\"\u003EE\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.1514em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ep\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Eos\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E：Token Embedding，1-D Positional Embedding\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003E\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ec\u003C\u002Fmi\u003E\u003Cmi\u003El\u003C\u002Fmi\u003E\u003Cmi\u003Ea\u003C\u002Fmi\u003E\u003Cmi\u003Es\u003C\u002Fmi\u003E\u003Cmi\u003Es\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ex_{class}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3361em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ec\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\"\u003El\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ea\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ess\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.15em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E：和Bert类似，是额外加的一个分类头\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003E\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ez\u003C\u002Fmi\u003E\u003Cmn\u003E0\u003C\u002Fmn\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ez_{0}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.04398em;\"\u003Ez\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3011em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:-0.044em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E0\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.15em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E：最终VIT的输入\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E（2）即是计算multi-head attention的过程，（3）是计算MLP的过程。\u003C\u002Fp\u003E\n\u003Cp\u003E（4）是最终分类任务，LN表示是一个简单的线性分类模型，\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsubsup\u003E\u003Cmi\u003Ez\u003C\u002Fmi\u003E\u003Cmi\u003EL\u003C\u002Fmi\u003E\u003Cmn\u003E0\u003C\u002Fmn\u003E\u003C\u002Fmsubsup\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ez_{L}^{0}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1.0894em;vertical-align:-0.2753em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.04398em;\"\u003Ez\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.8141em;\"\u003E\u003Cspan style=\"top:-2.4247em;margin-left:-0.044em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003EL\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E0\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2753em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E则是\u003Ccode\u003E&#x3C;cls\u003E\u003C\u002Fcode\u003E对应的向量。\u003C\u002Fp\u003E\n\u003Ch1 data-id=\"heading-15\"\u003E五、微调（fine-tune）\u003C\u002Fh1\u003E\n\u003Cp\u003E目前为止，按照一至五部分所说的内容，通过让模型做分类预测，我们可以\u003Cstrong\u003E预训练（pretrain）\u003C\u002Fstrong\u003E 好一个VIT了。\u003C\u002Fp\u003E\n\u003Cp\u003E前面说过，预训练好的VIT模型是个有力的特征提取器，我们可以用它输出的特征，去做更多有趣的\u003Cstrong\u003E下游任务（downstream task)\u003C\u002Fstrong\u003E 。例如拿它去做类型更丰富的分类，目标检测等事情。在做这些任务时，我们会喂给预训练模型一堆新的数据，同时尽量保证模型的主体架构不变（例如VIT整体参数不动，只在输出层后接一个新模型，再次训练时只对新模型做参数更新之类）。\u003Cstrong\u003E这种既利用了已有模型的特征提取能力，又能让模型更好适应不同任务的操作，称为微调（fine-tune）。\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E在fine-tune的时候，我们用的图像大小可能和预训练时的并不一致，比如：\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003E预训练时用\u003Ccode\u003E224*224*3\u003C\u002Fcode\u003E大小的图片，fine-tune时为了效果更好，一般选择分辨率更高的图片，例如\u003Ccode\u003E1024*1024*3\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003E假设保持patch尺寸P=16不变，则预训练时产生的patch数有196个，fine-tune时产生的patch数有4096个(\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmfrac\u003E\u003Cmrow\u003E\u003Cmi\u003EH\u003C\u002Fmi\u003E\u003Cmo\u003E∗\u003C\u002Fmo\u003E\u003Cmi\u003EW\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cmsup\u003E\u003Cmi\u003EP\u003C\u002Fmi\u003E\u003Cmn\u003E2\u003C\u002Fmn\u003E\u003C\u002Fmsup\u003E\u003C\u002Fmfrac\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003E\\frac{H*W}{P^{2}}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1.2173em;vertical-align:-0.345em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mopen nulldelimiter\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mfrac\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.8723em;\"\u003E\u003Cspan style=\"top:-2.655em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:3em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003EP\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.7463em;\"\u003E\u003Cspan style=\"top:-2.786em;margin-right:0.0714em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.5em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size3 size1 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E2\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.23em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:3em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.394em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:3em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\"\u003EH\u003C\u002Fspan\u003E\u003Cspan class=\"mbin mtight\"\u003E∗\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003EW\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.345em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mclose nulldelimiter\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003E我们知道，Transformer主体架构理论上是可以处理任意长度的输入序列的（相关分析参见\u003Ca href=\"https:\u002F\u002Flink.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F445895638%2Fanswer%2F2306274526\" target=\"_blank\" title=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F445895638\u002Fanswer\u002F2306274526\" ref=\"nofollow noopener noreferrer\"\u003E这篇\u003C\u002Fa\u003E文章）。但是\u003Cstrong\u003E可学习的（learnable）\u003C\u002Fstrong\u003E 位置编码不是，由于一个位置对应一条位置编码，它和输入序列长度密切相关。\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E那么多出来的patch，在fine-tune时要怎么给它们位置编码呢？如果统一都赋成0向量，然后在fine-tune的时候再去训练这些向量，看起来可以，但这样粗暴的赋值不仅增加了计算量，也浪费了已有的信息（例如，是否能从已有的位置编码粗略地初始化一些新的位置编码出来？）考虑到这一点，\u003Cstrong\u003EVIT在fine-tune时，对预训练阶段的位置编码做了2D插值处理。\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-16\"\u003E5.1 VIT fine-tune: 2D插值位置编码\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F179638ae1a6a4d338387e00d3bf64ca0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E如图绿色部分所示，在fine-tune阶段要处理的patch\u002Ftoken数\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Es\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003En\u003C\u002Fmi\u003E\u003Cmi\u003Ee\u003C\u002Fmi\u003E\u003Cmi\u003Et\u003C\u002Fmi\u003E\u003Cmi\u003Eu\u003C\u002Fmi\u003E\u003Cmi\u003En\u003C\u002Fmi\u003E\u003Cmi\u003Ee\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Es_{finetune}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Es\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3361em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ein\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ee\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Et\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Eu\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003En\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ee\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E可能比预训练阶段要处理的\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Es\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ep\u003C\u002Fmi\u003E\u003Cmi\u003Er\u003C\u002Fmi\u003E\u003Cmi\u003Ee\u003C\u002Fmi\u003E\u003Cmi\u003Et\u003C\u002Fmi\u003E\u003Cmi\u003Er\u003C\u002Fmi\u003E\u003Cmi\u003Ea\u003C\u002Fmi\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003En\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Es_{pretrain}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Es\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3117em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ep\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ere\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Et\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\"\u003Er\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Eain\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E要多。\u003C\u002Fp\u003E\n\u003Cp\u003E图中红色部分演示了如何通过插值方法将\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Es\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ep\u003C\u002Fmi\u003E\u003Cmi\u003Er\u003C\u002Fmi\u003E\u003Cmi\u003Ee\u003C\u002Fmi\u003E\u003Cmi\u003Et\u003C\u002Fmi\u003E\u003Cmi\u003Er\u003C\u002Fmi\u003E\u003Cmi\u003Ea\u003C\u002Fmi\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003En\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Es_{pretrain}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Es\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3117em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ep\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ere\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Et\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\"\u003Er\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Eain\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E扩展至\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Es\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003En\u003C\u002Fmi\u003E\u003Cmi\u003Ee\u003C\u002Fmi\u003E\u003Cmi\u003Et\u003C\u002Fmi\u003E\u003Cmi\u003Eu\u003C\u002Fmi\u003E\u003Cmi\u003En\u003C\u002Fmi\u003E\u003Cmi\u003Ee\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Es_{finetune}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Es\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3361em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ein\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ee\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Et\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Eu\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003En\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal mtight\"\u003Ee\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E。其中interpolate部分就是2D插值，这部分是重点，我们直接看下代码中的操作：\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"hljs language-ini\" lang=\"ini\"\u003E\u003Cspan class=\"hljs-attr\"\u003Enew_pos_embedding_img\u003C\u002Fspan\u003E = nn.functional.interpolate(\n            pos_embedding_img,\n            \u003Cspan class=\"hljs-attr\"\u003Esize\u003C\u002Fspan\u003E=new_seq_length_1d,\n            \u003Cspan class=\"hljs-attr\"\u003Emode\u003C\u002Fspan\u003E=interpolation_mode,\n            \u003Cspan class=\"hljs-attr\"\u003Ealign_corners\u003C\u002Fspan\u003E=\u003Cspan class=\"hljs-literal\"\u003ETrue\u003C\u002Fspan\u003E,\n        )\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E可以发现这里用了pytorch内置的interpolate函数，mode表示具体的插值方法，在VIT中采用的是bicubic。\u003Ccode\u003Ealign_corners=True\u003C\u002Fcode\u003E 的意思是在固定原矩阵四角的情况下按mode进行插值，可以参加图中，白色圆圈表示原始的矩阵，蓝色点表示做完插值后的矩阵。插值后矩阵的四角保持不变，中间则按设置的方法做插值。关于插值位置编码更详细的讲解，可以参考\u003Ca href=\"https:\u002F\u002Flink.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fqq_44166630%2Farticle%2Fdetails%2F127429697\" target=\"_blank\" title=\"https:\u002F\u002Fblog.csdn.net\u002Fqq_44166630\u002Farticle\u002Fdetails\u002F127429697\" ref=\"nofollow noopener noreferrer\"\u003E这篇\u003C\u002Fa\u003E文章。\u003C\u002Fp\u003E\n\u003Ch1 data-id=\"heading-17\"\u003E六、VIT效果\u003C\u002Fh1\u003E\n\u003Cp\u003E到目前为止，我们已讲完了预训练和微调的内容。接下来，我们来看VIT的效果，及一些有趣的实验结果。\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-18\"\u003E6.1 不同VIT模型的表示符号\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002Ff277adbef31c44efbf3f1057c11bc15e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EVIT预训练了三种不同参数规模的模型，分别是\u003Ccode\u003EVIT-Base\u003C\u002Fcode\u003E ，\u003Ccode\u003EVIT-Large\u003C\u002Fcode\u003E和\u003Ccode\u003EVIT-Huge\u003C\u002Fcode\u003E。其规模可具体见上图。\u003C\u002Fp\u003E\n\u003Cp\u003E在论文及实际使用中，我们常用\u003Ccode\u003EVIT-size\u002Fpatch_size\u003C\u002Fcode\u003E的形式来表示该模型是在“什么规模”及“多大的patch尺寸”上预训练出来的。例如\u003Ccode\u003EVIT-H\u002F14\u003C\u002Fcode\u003E 就表示该模型是在Huge规模上，用patch尺寸为14的数据做预训练的。\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-19\"\u003E6.2 VIT VS 卷积神经网络\u003C\u002Fh2\u003E\n\u003Cp\u003E既然VIT的目的是替换卷积神经网络，那么当然要比较一下它和目前SOTA的卷积网络间的性能了。\u003C\u002Fp\u003E\n\u003Cp\u003E作者选取了ResNet和Noisy Student这两种经典高性能的卷积神经网络与VIT进行比较，比较内容为“\u003Cstrong\u003E预测图片类别的准确性\u003C\u002Fstrong\u003E”与“\u003Cstrong\u003E训练时长\u003C\u002Fstrong\u003E”，结果如下：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F76655dab731c4604af07df12b012d541~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E前三列Ours-JFT(VIT-H\u002F14)，Ours-JFT(VIT-L\u002F16)，Ours-I12K(VIT-L\u002F16)表示三个VIT预训练模型，它们分别在不同规模和不同数据集（JFT, I12K）上预训练而来。后两列表示两个卷积神经网络模型。\u003C\u002Fp\u003E\n\u003Cp\u003E纵向的ImageNet，ImageNet Real等表示不同的图像数据集，当我们的VIT模型和卷积模型预训练好后，我们就可以借助这些pretrain模型，在图像数据集上做fine-tune，而表格里给出的就是fine-tune后的准确率。\u003C\u002Fp\u003E\n\u003Cp\u003E观察表格，我们发现一个有趣的现象：\u003Cstrong\u003EVIT和卷积神经网络相比，表现基本一致\u003C\u002Fstrong\u003E。关于这一点，我们会在下文详细分析。\u003C\u002Fp\u003E\n\u003Cp\u003E虽然准确率没有突出表现，但是训练时间上VIT的还是有亮点的，表格最后一行表示，假设用单块TPU训练模型，所需要的天数。\u003Cstrong\u003E我们发现VIT最高也只需要2500核-天（当然其实这个值也不小啦），卷积网络要花至9900核-天以上。所以VIT的一个优势在于，训练没那么贵了。\u003C\u002Fstrong\u003E 关于这点，我的猜想是基于Transformer架构的VIT，和卷积神经网络相比，更适合做\u003Cstrong\u003E切分均匀\u003C\u002Fstrong\u003E的矩阵计算，这样我们就能把参数均匀切到不同卡上做分布式训练，更好利用GPU算力，平衡整个训练系统了。\u003C\u002Fp\u003E\n\u003Cp\u003E现在，我们回到刚才的问题，\u003Cstrong\u003E为什么VIT相比卷积网络，在准确率上没有突出优势\u003C\u002Fstrong\u003E？为了解答这个问题，我们先来看卷积神经网络的\u003Cstrong\u003E归纳偏置（inductive biases）\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Ch4 data-id=\"heading-20\"\u003E6.2.1 卷积神经网络的归纳偏置\u003C\u002Fh4\u003E\n\u003Cp\u003E\u003Cstrong\u003E归纳偏置用大白话来说，就是一种假设，或者说一种先验知识。有了这种先验，我们就能知道哪一种方法更适合解决哪一类任务。所以归纳偏置是一种统称，不同的任务其归纳偏置下包含的具体内容不一样。\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E对图像任务来说，它的归纳偏置有以下两点：\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003E空间局部性（locality）\u003C\u002Fstrong\u003E ：假设一张图片中，相邻的区域是有相关特征的。比如太阳和天空就经常一起出现。\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003E平移等边性（translation equivariance）\u003C\u002Fstrong\u003E ：\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Eg\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmi\u003Eg\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003Cmo separator=\"true\"\u003E,\u003C\u002Fmo\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmtext\u003E卷积\u003C\u002Fmtext\u003E\u003Cmo separator=\"true\"\u003E,\u003C\u002Fmo\u003E\u003Cmi\u003Eg\u003C\u002Fmi\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmtext\u003E平\u003C\u002Fmtext\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ef(g(x)) = g(f(x)), f=卷积, g=平\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003Eg\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E))\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003Eg\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E))\u003C\u002Fspan\u003E\u003Cspan class=\"mpunct\"\u003E,\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord cjk_fallback\"\u003E卷积\u003C\u002Fspan\u003E\u003Cspan class=\"mpunct\"\u003E,\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003Eg\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.6833em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord cjk_fallback\"\u003E平\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E。假设一张图中，左上角有一个太阳，你对这张图正常做卷积得到特征图，则左上角的卷积可表示为\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ef(x\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E，做完卷积后，你想把左上角的特征图移动到右上角去，则你这一顿操作可以用\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Eg\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Eg(f(x)\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003Eg\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E来表示。这一系列操作等同于，你先把左上角的太阳移动到右上角去(\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Eg\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Eg(x\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003Eg\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E)，然后再做卷积\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmi\u003Ef\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Eg\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E(\u003C\u002Fmo\u003E\u003Cmi\u003Ex\u003C\u002Fmi\u003E\u003Cmo stretchy=\"false\"\u003E)\u003C\u002Fmo\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ef(g(x)\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003Ef\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003Eg\u003C\u002Fspan\u003E\u003Cspan class=\"mopen\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"mord mathnormal\"\u003Ex\u003C\u002Fspan\u003E\u003Cspan class=\"mclose\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E，这就是图像的平移等边性。不论物体移动到哪里，只要给卷积核的输入不变，那么输出也是一致的。\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E在这两种先验假设下，CNN成为了图像任务最佳的方案之一。\u003Cstrong\u003E卷积核能最大程度保持空间局部性（保存相关物体的位置信息）和平移等边性\u003C\u002Fstrong\u003E，使得在训练过程中，最大限度学习和保留原始图片信息。\u003C\u002Fp\u003E\n\u003Cp\u003E好，那么现在，如果说VIT相比于卷积，在图像任务上没有显著优势，那大概率VIT对这两种先验的维护没有CNN做的好，具体来看：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F4820c35273af423fadf62c9d9fb7efcc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E图中箭头所指的两部分都属于同一栋建筑。在卷积中，我们可以用大小适当的卷积核将它们圈在一起。但是在VIT中，它们之间的位置却拉远了，如果我把patch再切分细一些，它们的距离就更远了。虽然attention可以学习到向量间的想关系，但是VIT在\u003Cstrong\u003E空间局部性\u003C\u002Fstrong\u003E的维护上，确实没有卷积做的好。而在\u003Cstrong\u003E平移等边性\u003C\u002Fstrong\u003E上，由于VIT需要对patch的位置进行学习，所以对于一个patch，当它位置变幻时，它的输出结果也是不一样的。\u003Cstrong\u003E所以，VIT的架构没有很好维护图像问题中的归纳偏置假设\u003C\u002Fstrong\u003E。\u003C\u002Fp\u003E\n\u003Cp\u003E但是，这就意味着VIT没有翻盘的一天了吗？当然不是，\u003Cstrong\u003E不要忘了，Transformer架构的模型都有一个广为人知的特性：大力出奇迹\u003C\u002Fstrong\u003E。只要它见过的数据够多，它就能更好地学习像素块之间的关联性，当然也能抹去归纳偏置的问题。\u003C\u002Fp\u003E\n\u003Ch4 data-id=\"heading-21\"\u003E6.2.2 VIT：大力出奇迹\u003C\u002Fh4\u003E\n\u003Cp\u003E作者当然也考虑到了这点，所以采用了不同数量的数据集，对VIT进行训练，效果如下：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002Fc631948acb1b49948594db76dc757f54~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E如图，横轴表示不同量级的数据集（越往右数据集越大），纵轴表示准确率。图中灰色阴影部分表示在相应数据集下，不同架构的卷积神经网络的准确率范围。\u003Cstrong\u003E可以发现，当数据集较小时，VIT表现明显弱于卷积网络。但当数据量级大于21k时，VIT的能力就上来了\u003C\u002Fstrong\u003E。\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-22\"\u003E6.3 VIT的Attention到底看到了什么\u003C\u002Fh2\u003E\n\u003Cp\u003E讲完了VIT的整体效果，我们来探究下VIT具体学到了什么，才能帮助它达到这样的效果。我们首先来看attention层。\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F475e90e0a6bc467ab5541a2ec2424f27~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E这张实验图刻画了VIT的16个multi-head attention学到的像素距离信息。横轴表示网络的深度，纵轴表示“平均注意力距离”，我们设第i个和第j个像素的平均注意力距离为\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ed\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003Ej\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ed_{ij}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.9805em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Ed\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3117em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\"\u003Eij\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E，真实像素距离为\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsubsup\u003E\u003Cmi\u003Ed\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003Ej\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003E′\u003C\u002Fmo\u003E\u003C\u002Fmsubsup\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ed_{ij}^{\\prime}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1.1467em;vertical-align:-0.3948em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Ed\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.7519em;\"\u003E\u003Cspan style=\"top:-2.4413em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\"\u003Eij\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E′\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3948em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E，这两个像素所在patch某一个head上的attention score为\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ea\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003Ej\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ea_{ij}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Ea\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3117em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\"\u003Eij\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E，则有：\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ed\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003Ej\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003Cmo\u003E=\u003C\u002Fmo\u003E\u003Cmsub\u003E\u003Cmi\u003Ea\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003Ej\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003Cmo\u003E∗\u003C\u002Fmo\u003E\u003Cmsubsup\u003E\u003Cmi\u003Ed\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003Ej\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003Cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003E′\u003C\u002Fmo\u003E\u003C\u002Fmsubsup\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ed_{ij} = a_{ij} * d_{ij}^{\\prime}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.9805em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Ed\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3117em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\"\u003Eij\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mrel\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.7514em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Ea\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3117em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\"\u003Eij\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mbin\"\u003E∗\u003C\u002Fspan\u003E\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:1.1467em;vertical-align:-0.3948em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Ed\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.7519em;\"\u003E\u003Cspan style=\"top:-2.4413em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\"\u003Eij\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E′\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3948em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E。当\u003Cspan class=\"math math-inline\"\u003E\u003Cspan class=\"katex\"\u003E\u003Cspan class=\"katex-mathml\"\u003E\u003Cmath xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1998\u002FMath\u002FMathML\"\u003E\u003Csemantics\u003E\u003Cmrow\u003E\u003Cmsub\u003E\u003Cmi\u003Ed\u003C\u002Fmi\u003E\u003Cmrow\u003E\u003Cmi\u003Ei\u003C\u002Fmi\u003E\u003Cmi\u003Ej\u003C\u002Fmi\u003E\u003C\u002Fmrow\u003E\u003C\u002Fmsub\u003E\u003C\u002Fmrow\u003E\u003Cannotation encoding=\"application\u002Fx-tex\"\u003Ed_{ij}\u003C\u002Fannotation\u003E\u003C\u002Fsemantics\u003E\u003C\u002Fmath\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"katex-html\" aria-hidden=\"true\"\u003E\u003Cspan class=\"base\"\u003E\u003Cspan class=\"strut\" style=\"height:0.9805em;vertical-align:-0.2861em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"mord\"\u003E\u003Cspan class=\"mord mathnormal\"\u003Ed\u003C\u002Fspan\u003E\u003Cspan class=\"msupsub\"\u003E\u003Cspan class=\"vlist-t vlist-t2\"\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.3117em;\"\u003E\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003E\u003Cspan class=\"pstrut\" style=\"height:2.7em;\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"sizing reset-size6 size3 mtight\"\u003E\u003Cspan class=\"mord mtight\"\u003E\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\"\u003Eij\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-s\"\u003E​\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"vlist-r\"\u003E\u003Cspan class=\"vlist\" style=\"height:0.2861em;\"\u003E\u003Cspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E越大时，说明VIT的attention机制能让它关注到距离较远的两个像素，类似于CNN中的“扩大感受野”。\u003C\u002Fp\u003E\n\u003Cp\u003E图中每一列上，都有16个彩色原点，它们分别表示16个head观测到的平均像素距离。由图可知，在浅层网络中，VIT还只能关注到距离较近的像素点，\u003Cstrong\u003E随着网络加深，VIT逐渐学会去更远的像素点中寻找相关信息了。这个过程就和用在CNN中用卷积逐层去扩大感受野非常相似\u003C\u002Fstrong\u003E。\u003C\u002Fp\u003E\n\u003Cp\u003E下图的左侧表示原始的输入图片，右侧表示VIT最后一层看到的图片信息，可以清楚看见，VIT在最后一层已经学到了将注意力放到关键的物体上了，这是非常有趣的结论：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F0c57036c354e42fdb0a8c9bfdd0c2376~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Ch2 data-id=\"heading-23\"\u003E6.4 VIT的位置编码学到了什么\u003C\u002Fh2\u003E\n\u003Cp\u003E我们在上文讨论过图像的\u003Cstrong\u003E空间局部性（locality）\u003C\u002Fstrong\u003E ，即有相关性的物体（例如太阳和天空）经常一起出现。CNN采用卷积框取特征的方式，极大程度上维护了这种特性。\u003Cstrong\u003E其实，VIT也有维护这种特性的方法，上面所说的attention是一种，位置编码也是一种。\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E我们来看看VIT的位置编码学到了什么信息：\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F2dce17b395fc48f8be7f95a0da800d8f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E上图是\u003Ccode\u003EVIT-L\u002F32\u003C\u002Fcode\u003E模型下的位置编码信息，图中每一个方框表示一个patch，图中共有\u003Ccode\u003E7*7\u003C\u002Fcode\u003E个patch。而每个方框内，也有一个\u003Ccode\u003E7*7\u003C\u002Fcode\u003E的矩阵，这个矩阵中的每一个值，表示当前patch的position embedding和其余对应位置的position embedding的余弦相似度。\u003Cstrong\u003E颜色越黄，表示越相似，也即patch和对应位置间的patch密切相关\u003C\u002Fstrong\u003E。\u003C\u002Fp\u003E\n\u003Cp\u003E注意到每个方框中，最黄的点总是当前patch所在位置，这个不难理解，因为自己和自己肯定是最相似的。除此以外\u003Cstrong\u003E颜色较黄的部分都是当前patch所属的行和列，以及以当前patch为中心往外扩散的一小圈。这就说明VIT通过位置编码，已经学到了一定的空间局部性\u003C\u002Fstrong\u003E。\u003C\u002Fp\u003E\n\u003Ch1 data-id=\"heading-24\"\u003E七、总结：VIT的意义何在\u003C\u002Fh1\u003E\n\u003Cp\u003E到此为止，关于VIT模型，我们就介绍完毕了。一顿读下来，你可能有个印象：\u003Cstrong\u003E如果训练数据量不够多的话，看起来VIT也没比CNN好多少呀，VIT的意义是什么呢？\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E这是个很好的问题，\u003Cstrong\u003E因为在工业界，人们的标注数据量和算力都是有限的，因此CNN可能还是首要选择\u003C\u002Fstrong\u003E。\u003C\u002Fp\u003E\n\u003Cp\u003E但是，VIT的出现，不仅是用模型效果来考量这么简单，今天再来看这个模型，\u003Cstrong\u003E发现它的作用，就像是给后续的植树人挖好了坑，等人在上面播种耕植\u003C\u002Fstrong\u003E，具体表现在：\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003E证明了一个统一框架在不同模态任务上的表现能力。在VIT之前，NLP的SOTA范式被认为是Transformer，而图像的SOTA范式依然是CNN。\u003Cstrong\u003EVIT出现后，证明了用NLP领域的SOTA模型一样能解图像领域的问题，同时在论文中通过丰富的实验，证明了VIT对CNN的替代能力\u003C\u002Fstrong\u003E，同时也论证了\u003Cstrong\u003E大规模+大模型在图像领域的涌现能力\u003C\u002Fstrong\u003E（论文中没有明确指出这是涌现能力，但通过实验展现了这种趋势）。这也为后续两年多模态任务的发展奠定了基石。\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003E虽然VIT只是一个分类任务，但在它提出的几个月之后，立刻就有了用Transformer架构做检测（detection）和分割（segmentation）的模型。而不久之后，GPT式的无监督学习，也在CV届开始火热起来。\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003E工业界上，对大部分企业来说，受到训练数据和算力的影响，预训练和微调一个VIT都是困难的，但是这不妨碍直接拿大厂训好的VIT特征做下游任务。同时，低成本的微调方案研究，在今天也层出不穷。长远来看，2年前的这个“庞然大物”，已经在逐步走进千家万户。\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch1 data-id=\"heading-25\"\u003E八、参考\u003C\u002Fh1\u003E\n\u003Cp\u003E1、\u003Ca href=\"https:\u002F\u002Flink.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2010.11929.pdf\" target=\"_blank\" title=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2010.11929.pdf\" ref=\"nofollow noopener noreferrer\"\u003Earxiv.org\u002Fpdf\u002F2010.11…\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E2、\u003Ca href=\"https:\u002F\u002Flink.juejin.cn?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV15P4y137jb%2F%3Fspm_id_from%3D333.337.search-card.all.click\" target=\"_blank\" title=\"https:\u002F\u002Fwww.bilibili.com\u002Fvideo\u002FBV15P4y137jb\u002F?spm_id_from=333.337.search-card.all.click\" ref=\"nofollow noopener noreferrer\"\u003Ewww.bilibili.com\u002Fvideo\u002FBV15P…\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E3、\u003Ca href=\"https:\u002F\u002Flink.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1803.02155.pdf\" target=\"_blank\" title=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.02155.pdf\" ref=\"nofollow noopener noreferrer\"\u003Earxiv.org\u002Fpdf\u002F1803.02…\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E4、\u003Ca href=\"https:\u002F\u002Flink.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fqq_44166630%2Farticle%2Fdetails%2F127429697\" target=\"_blank\" title=\"https:\u002F\u002Fblog.csdn.net\u002Fqq_44166630\u002Farticle\u002Fdetails\u002F127429697\" ref=\"nofollow noopener noreferrer\"\u003Eblog.csdn.net\u002Fqq_44166630…\u003C\u002Fa\u003E\u003C\u002Fp\u003E",meta_info:"{\"plugins\":[3,6,5]}",catalog:"[{\"title\":\"一、模型架构\",\"hash\":\"#heading-0\",\"deep\":1,\"children\":[{\"title\":\"1.1 Bert架构\",\"hash\":\"#heading-1\",\"deep\":2,\"children\":[],\"visible\":true},{\"title\":\"1.2 VIT模型架构\",\"hash\":\"#heading-2\",\"deep\":2,\"children\":[],\"visible\":true}],\"visible\":true},{\"title\":\"二、从patch到token\",\"hash\":\"#heading-3\",\"deep\":1,\"children\":[{\"title\":\"2.1 patch变token的过程\",\"hash\":\"#heading-4\",\"deep\":2,\"children\":[],\"visible\":true},{\"title\":\"2.2 为什么要处理成patch\",\"hash\":\"#heading-5\",\"deep\":2,\"children\":[],\"visible\":true}],\"visible\":true},{\"title\":\"三、Emebdding\",\"hash\":\"#heading-6\",\"deep\":1,\"children\":[{\"title\":\"3.1 Token Emebdding\",\"hash\":\"#heading-7\",\"deep\":2,\"children\":[],\"visible\":true},{\"title\":\"3.2 Position Embedding（位置向量）\",\"hash\":\"#heading-8\",\"deep\":2,\"children\":[{\"title\":\"方案一： 不添加任何位置信息\",\"hash\":\"#heading-9\",\"deep\":3,\"children\":[],\"visible\":true},{\"title\":\"方案二：使用1-D绝对位置编码\",\"hash\":\"#heading-10\",\"deep\":3,\"children\":[],\"visible\":true},{\"title\":\"方案三：使用2-D绝对位置编码\",\"hash\":\"#heading-11\",\"deep\":3,\"children\":[],\"visible\":true},{\"title\":\"方案四：相对位置编码（relative positional embeddings）\",\"hash\":\"#heading-12\",\"deep\":3,\"children\":[],\"visible\":true},{\"title\":\"实验结果\",\"hash\":\"#heading-13\",\"deep\":3,\"children\":[],\"visible\":true}],\"visible\":true}],\"visible\":true},{\"title\":\"四、模型架构的数学表达\",\"hash\":\"#heading-14\",\"deep\":1,\"children\":[],\"visible\":true},{\"title\":\"五、微调（fine-tune）\",\"hash\":\"#heading-15\",\"deep\":1,\"children\":[{\"title\":\"5.1 VIT fine-tune: 2D插值位置编码\",\"hash\":\"#heading-16\",\"deep\":2,\"children\":[],\"visible\":true}],\"visible\":true},{\"title\":\"六、VIT效果\",\"hash\":\"#heading-17\",\"deep\":1,\"children\":[{\"title\":\"6.1 不同VIT模型的表示符号\",\"hash\":\"#heading-18\",\"deep\":2,\"children\":[],\"visible\":true},{\"title\":\"6.2 VIT VS 卷积神经网络\",\"hash\":\"#heading-19\",\"deep\":2,\"children\":[{\"title\":\"6.2.1 卷积神经网络的归纳偏置\",\"hash\":\"#heading-20\",\"deep\":3,\"children\":[],\"visible\":true},{\"title\":\"6.2.2 VIT：大力出奇迹\",\"hash\":\"#heading-21\",\"deep\":3,\"children\":[],\"visible\":true}],\"visible\":true},{\"title\":\"6.3 VIT的Attention到底看到了什么\",\"hash\":\"#heading-22\",\"deep\":2,\"children\":[],\"visible\":true},{\"title\":\"6.4 VIT的位置编码学到了什么\",\"hash\":\"#heading-23\",\"deep\":2,\"children\":[],\"visible\":true}],\"visible\":true},{\"title\":\"七、总结：VIT的意义何在\",\"hash\":\"#heading-24\",\"deep\":1,\"children\":[],\"visible\":true},{\"title\":\"八、参考\",\"hash\":\"#heading-25\",\"deep\":1,\"children\":[],\"visible\":true}]",homepage_top_time:-62135596800,homepage_top_status:b},author_user_info:{user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",level:p,description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:10,follower_count:89,post_article_count:14,digg_article_count:34,got_digg_count:132,got_view_count:22007,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:E,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:970190899118414,jpower:E,jscore:538.6,jpower_level:p,jscore_level:5,jscore_title:"先锋掘友",author_achievement_list:[],vip_level:p,vip_title:"渐入佳境",jscore_next_level_score:2000,jscore_this_level_mini_score:500,vip_score:270},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b},category:{category_id:"6809637773935378440",category_name:"人工智能",category_url:"ai",rank:5,back_ground:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002Ff7cecf8806e8621ef35e.jpg",icon:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002F9b525117507d7a76c4ac.png",ctime:1500876664,mtime:1500876667,show_type:p,item_type:s,promote_tag_cap:4,promote_priority:5,id:"6809637773935378440",name:"人工智能",title:"人工智能",alias:"ai"},tags:[{entriesCount:l,subscribed:a,id:"6809640642101116936",tag_id:"6809640642101116936",tag_name:"人工智能",color:"#2e6cff",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F9b525117507d7a76c4ac.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:g,ctime:1472072600,mtime:1694066336,id_type:9,tag_alias:e,post_article_count:27894,concern_user_count:240522,title:"人工智能",tagId:"6809640642101116936",articleCount:27894,subscribersCount:240522,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"6809640711177109517",tag_id:"6809640711177109517",tag_name:"计算机视觉",color:"#000000",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F8d1b7f5a309434d082fe.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:b,ctime:1481241701,mtime:1694066336,id_type:9,tag_alias:e,post_article_count:2309,concern_user_count:30841,title:"计算机视觉",tagId:"6809640711177109517",articleCount:2309,subscribersCount:30841,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"7197380506562871333",tag_id:"7197380506562871333",tag_name:"AIGC",color:e,icon:"https:\u002F\u002Fp6-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F61a92ed3e7c04ef7bcabcaad9cb75e60~tplv-k3u1fbpfcp-watermark.image?",back_ground:e,show_navi:b,ctime:1675771209,mtime:1694064505,id_type:9,tag_alias:"AI Generated Content,AIgc",post_article_count:1070,concern_user_count:2760,title:"AIGC",tagId:"7197380506562871333",articleCount:1070,subscribersCount:2760,createdAt:c,updatedAt:c}],user_interact:{id:7254341178258489000,omitempty:s,user_id:b,is_digg:a,is_follow:a,is_collect:a,collect_set_count:b},org:{org_info:c,is_followed:a},req_id:"021694066501553fdbddc02001306720000000000000224f5e508",status:{push_status:b},theme_list:[],title:w,user:F,viewCount:l,commentsCount:B,isEvent:l,abstract:x,latestCommentAt:c,createdAt:new Date(1689051788000),updatedAt:c,isTopicEvent:a,likedCount:r,likeCount:r,content:e,originalUrl:e,type:"post",collected:a,viewsCount:z,username:D,viewerHasLiked:a,draftId:y,collectionCount:A,isCache:d},entryView:{},author:F,adEntryList:[],relatedEntryList:[],selectedList:[],linkVotingList:[{keyword:"js jsp session",url:"https:\u002F\u002Ffrontend.devrank.cn\u002Ftraffic-aggregation\u002F189503",type:"前端"}],userAnnuals:[],columnList:[],hitArticleCache:d,authorCard:{},relatedLoaded:a,dynamicDataReady:a,followAuthorPopupTime:b,followAuthorPopupType:b,authorBlockVisible:b,compVisible:{},version:C,actionType:{FETCH:"view\u002Fcolumn\u002FFETCH",FETCH_RELATED:"view\u002Fcolumn\u002FFETCH_RELATED",FETCH_RECOMMEND:"view\u002Fcolumn\u002FFETCH_RECOMMEND",FETCH_SELECTED:"view\u002Fcolumn\u002FFETCH_SELECTED",FETCH_ADDITIONAL:"view\u002Fcolumn\u002FFETCH_ADDITIONAL",FETCH_SIDEBAR_ADENTRY:"view\u002Fcolumn\u002FFETCH_SIDEBAR_ADENTRY",FETCH_AUTHOR_EXTRA:"view\u002Fcolumn\u002FFETCH_AUTHOR_EXTRA",MD_FALLBACK_RENDER:"view\u002Fcolumn\u002FMD_FALLBACK_RENDER",RESET:"view\u002Fcolumn\u002FRESET"},recommendedArticleList:{list:[{id:"7267417057438777399",screenshot:l,liked:a,article_id:"7267417057438777399",article_info:{article_id:"7267417057438777399",user_id:v,category_id:"6809637773935378440",tag_ids:[6809640642101117000,6809640711177110000,7257794499869573000],visible_level:b,link_url:e,cover_image:"https:\u002F\u002Fp1-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F29ab4c200482477ea0522fae0a355b83~tplv-k3u1fbpfcp-watermark.image?",is_gfw:b,title:"CV大模型系列之：MAE，实现像素级图像重建",brief_content:"本文将介绍，如何基于Transformer架构，借鉴Bert的基本思想，做基于像素级别的图像重建任务。",is_english:b,is_original:g,user_index:10.00006961079378,original_type:b,original_author:e,content:e,ctime:"1692077582",mtime:"1694066436",rtime:"1692254416",draft_id:"7267177063034535973",view_count:877,collect_count:A,digg_count:11,comment_count:C,hot_index:60,is_hot:b,rank_index:.00227991,status:s,verify_status:g,audit_status:s,mark_content:e,display_count:b,is_markdown:g,app_html_content:e,version:p,web_html_content:c,meta_info:c,catalog:c,homepage_top_time:-62135596800,homepage_top_status:b,content_count:3409,read_time:"11分钟"},author_user_info:{user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",level:b,description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b}},category:{category_id:"6809637773935378440",category_name:"人工智能",category_url:"ai",rank:5,back_ground:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002Ff7cecf8806e8621ef35e.jpg",icon:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002F9b525117507d7a76c4ac.png",ctime:1500876664,mtime:1500876667,show_type:p,item_type:s,promote_tag_cap:4,promote_priority:5,id:"6809637773935378440",name:"人工智能",title:"人工智能",alias:"ai"},tags:[{entriesCount:l,subscribed:a,id:"6809640642101116936",tag_id:"6809640642101116936",tag_name:"人工智能",color:"#2e6cff",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F9b525117507d7a76c4ac.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:g,ctime:1472072600,mtime:1702476143,id_type:9,tag_alias:e,post_article_count:47374,concern_user_count:245342,title:"人工智能",tagId:"6809640642101116936",articleCount:47374,subscribersCount:245342,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"6809640711177109517",tag_id:"6809640711177109517",tag_name:"计算机视觉",color:"#000000",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F8d1b7f5a309434d082fe.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:b,ctime:1481241701,mtime:1702444680,id_type:9,tag_alias:e,post_article_count:2452,concern_user_count:31708,title:"计算机视觉",tagId:"6809640711177109517",articleCount:2452,subscribersCount:31708,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"7257794499869573175",tag_id:"7257794499869573175",tag_name:"LLM",color:e,icon:"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F461a12434205464287418fa250495073~tplv-k3u1fbpfcp-watermark.image?",back_ground:e,show_navi:b,ctime:1689836965,mtime:1702471684,id_type:9,tag_alias:"大型语言模型,Large Language Model",post_article_count:855,concern_user_count:1161,title:"LLM",tagId:"7257794499869573175",articleCount:855,subscribersCount:1161,createdAt:c,updatedAt:c}],user_interact:{id:7267417057438777000,omitempty:s,user_id:b,is_digg:a,is_follow:a,is_collect:a,collect_set_count:b},org:{org_info:c,is_followed:a},req_id:"20231213221218455CB3D6F6857CE33129",status:{push_status:b},theme_list:[],extra:{extra:e},title:"CV大模型系列之：MAE，实现像素级图像重建",user:{id:v,self_description:l,followed:a,viewerIsFollowing:l,community:l,subscribedTagCount:b,wroteBookCount:b,boughtBookCount:b,isBindedPhone:a,level:b,user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b},juejinPower:b,jobTitle:"🏆掘金签约作者｜人工智能方向",roles:{isBookAuthor:a,isFavorableAuthor:a,isCobuilder:a,isAdmin:a},username:D,blogAddress:l,selfDescription:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",beLikedCount:b,beReadCount:b,followerCount:b,followingCount:b,collectionCount:b,createdCollectionCount:b,followingCollectionCount:b,postedPostsCount:b,pinCount:b,likedArticleCount:b,likedPinCount:b,avatar:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",latestLoginedInAt:c,createdAt:c,updatedAt:c,phoneNumber:e,titleDescription:e,followeesCount:b,applyEventCount:b,need_lead:b,followTopicCnt:b},viewCount:l,commentsCount:C,isEvent:l,abstract:"本文将介绍，如何基于Transformer架构，借鉴Bert的基本思想，做基于像素级别的图像重建任务。",latestCommentAt:c,createdAt:new Date(1692077582000),updatedAt:c,isTopicEvent:a,likedCount:11,likeCount:11,content:e,originalUrl:e,type:"post",collected:a,viewsCount:877,username:D,viewerHasLiked:a,draftId:"7267177063034535973",collectionCount:A},{id:"7282962960213098557",screenshot:l,liked:a,article_id:"7282962960213098557",article_info:{article_id:"7282962960213098557",user_id:v,category_id:"6809637773935378440",tag_ids:[6809640642101117000,6809640711177110000,7257794499869573000],visible_level:b,link_url:e,cover_image:"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002Fee0a3cee04d64fc78e3969fbe4ab84b1~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=2560&h=1441&s=344250&e=jpg&b=414141",is_gfw:b,title:"CV大模型系列之：打败VIT？Swin Transformer是怎么做到的",brief_content:"一文详细图解Swin Transformer架构，探索移动窗口attention、patch merging、patch partition等技术细节",is_english:b,is_original:g,user_index:10.00006961079378,original_type:b,original_author:e,content:e,ctime:"1695730314",mtime:"1696757488",rtime:"1696757488",draft_id:"7282994347988975675",view_count:1052,collect_count:g,digg_count:4,comment_count:b,hot_index:56,is_hot:b,rank_index:.00459348,status:s,verify_status:g,audit_status:s,mark_content:e,display_count:b,is_markdown:g,app_html_content:e,version:g,web_html_content:c,meta_info:c,catalog:c,homepage_top_time:-62135596800,homepage_top_status:b,content_count:6175,read_time:"21分钟"},author_user_info:{user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",level:b,description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b}},category:{category_id:"6809637773935378440",category_name:"人工智能",category_url:"ai",rank:5,back_ground:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002Ff7cecf8806e8621ef35e.jpg",icon:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002F9b525117507d7a76c4ac.png",ctime:1500876664,mtime:1500876667,show_type:p,item_type:s,promote_tag_cap:4,promote_priority:5,id:"6809637773935378440",name:"人工智能",title:"人工智能",alias:"ai"},tags:[{entriesCount:l,subscribed:a,id:"6809640642101116936",tag_id:"6809640642101116936",tag_name:"人工智能",color:"#2e6cff",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F9b525117507d7a76c4ac.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:g,ctime:1472072600,mtime:1702476143,id_type:9,tag_alias:e,post_article_count:47374,concern_user_count:245342,title:"人工智能",tagId:"6809640642101116936",articleCount:47374,subscribersCount:245342,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"6809640711177109517",tag_id:"6809640711177109517",tag_name:"计算机视觉",color:"#000000",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F8d1b7f5a309434d082fe.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:b,ctime:1481241701,mtime:1702444680,id_type:9,tag_alias:e,post_article_count:2452,concern_user_count:31708,title:"计算机视觉",tagId:"6809640711177109517",articleCount:2452,subscribersCount:31708,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"7257794499869573175",tag_id:"7257794499869573175",tag_name:"LLM",color:e,icon:"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F461a12434205464287418fa250495073~tplv-k3u1fbpfcp-watermark.image?",back_ground:e,show_navi:b,ctime:1689836965,mtime:1702471684,id_type:9,tag_alias:"大型语言模型,Large Language Model",post_article_count:855,concern_user_count:1161,title:"LLM",tagId:"7257794499869573175",articleCount:855,subscribersCount:1161,createdAt:c,updatedAt:c}],user_interact:{id:7282962960213098000,omitempty:s,user_id:b,is_digg:a,is_follow:a,is_collect:a,collect_set_count:b},org:{org_info:c,is_followed:a},req_id:"20231213221218455CB3D6F6857CE33129",status:{push_status:b},theme_list:[],extra:{extra:e},title:"CV大模型系列之：打败VIT？Swin Transformer是怎么做到的",user:{id:v,self_description:l,followed:a,viewerIsFollowing:l,community:l,subscribedTagCount:b,wroteBookCount:b,boughtBookCount:b,isBindedPhone:a,level:b,user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b},juejinPower:b,jobTitle:"🏆掘金签约作者｜人工智能方向",roles:{isBookAuthor:a,isFavorableAuthor:a,isCobuilder:a,isAdmin:a},username:D,blogAddress:l,selfDescription:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",beLikedCount:b,beReadCount:b,followerCount:b,followingCount:b,collectionCount:b,createdCollectionCount:b,followingCollectionCount:b,postedPostsCount:b,pinCount:b,likedArticleCount:b,likedPinCount:b,avatar:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",latestLoginedInAt:c,createdAt:c,updatedAt:c,phoneNumber:e,titleDescription:e,followeesCount:b,applyEventCount:b,need_lead:b,followTopicCnt:b},viewCount:l,commentsCount:b,isEvent:l,abstract:"一文详细图解Swin Transformer架构，探索移动窗口attention、patch merging、patch partition等技术细节",latestCommentAt:c,createdAt:new Date(1695730314000),updatedAt:c,isTopicEvent:a,likedCount:4,likeCount:4,content:e,originalUrl:e,type:"post",collected:a,viewsCount:1052,username:D,viewerHasLiked:a,draftId:"7282994347988975675",collectionCount:g},{id:"7241859817563258917",screenshot:l,liked:a,article_id:"7241859817563258917",article_info:{article_id:"7241859817563258917",user_id:v,category_id:"6809637773935378440",tag_ids:[6809640499062768000,6809640642101117000],visible_level:b,link_url:e,cover_image:"https:\u002F\u002Fp9-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F17c52961290e4b29adfbbf8a2cfa5f65~tplv-k3u1fbpfcp-watermark.image?",is_gfw:b,title:"图解Transformer系列一：Positional Encoding（位置编码）",brief_content:"作为最近热门的生成式大模型的基石，Transformer自2017年诞生以来，就成为nlp研究者必须掌握的基本模型。本系列将通过图解方式，全方面展示Transformer的细节。",is_english:b,is_original:g,user_index:10.244188849587678,original_type:b,original_author:e,content:e,ctime:"1686135894",mtime:"1686208796",rtime:"1686208796",draft_id:"7241854869224128571",view_count:1045,collect_count:9,digg_count:C,comment_count:b,hot_index:58,is_hot:b,rank_index:.00121081,status:s,verify_status:g,audit_status:s,mark_content:e,display_count:b,is_markdown:g,app_html_content:e,version:s,web_html_content:c,meta_info:c,catalog:c,homepage_top_time:-62135596800,homepage_top_status:b,content_count:3022,read_time:"10分钟"},author_user_info:{user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",level:b,description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b}},category:{category_id:"6809637773935378440",category_name:"人工智能",category_url:"ai",rank:5,back_ground:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002Ff7cecf8806e8621ef35e.jpg",icon:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002F9b525117507d7a76c4ac.png",ctime:1500876664,mtime:1500876667,show_type:p,item_type:s,promote_tag_cap:4,promote_priority:5,id:"6809637773935378440",name:"人工智能",title:"人工智能",alias:"ai"},tags:[{entriesCount:l,subscribed:a,id:"6809640499062767624",tag_id:"6809640499062767624",tag_name:"算法",color:"#60ADFF",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F68a1097944c7fa1d7961.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:b,ctime:1439503293,mtime:1702476376,id_type:9,tag_alias:e,post_article_count:68679,concern_user_count:400538,title:"算法",tagId:"6809640499062767624",articleCount:68679,subscribersCount:400538,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"6809640642101116936",tag_id:"6809640642101116936",tag_name:"人工智能",color:"#2e6cff",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F9b525117507d7a76c4ac.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:g,ctime:1472072600,mtime:1702476143,id_type:9,tag_alias:e,post_article_count:47374,concern_user_count:245342,title:"人工智能",tagId:"6809640642101116936",articleCount:47374,subscribersCount:245342,createdAt:c,updatedAt:c}],user_interact:{id:7241859817563259000,omitempty:s,user_id:b,is_digg:a,is_follow:a,is_collect:a,collect_set_count:b},org:{org_info:c,is_followed:a},req_id:"20231213221218455CB3D6F6857CE33129",status:{push_status:b},theme_list:[],extra:{extra:e},title:"图解Transformer系列一：Positional Encoding（位置编码）",user:{id:v,self_description:l,followed:a,viewerIsFollowing:l,community:l,subscribedTagCount:b,wroteBookCount:b,boughtBookCount:b,isBindedPhone:a,level:b,user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b},juejinPower:b,jobTitle:"🏆掘金签约作者｜人工智能方向",roles:{isBookAuthor:a,isFavorableAuthor:a,isCobuilder:a,isAdmin:a},username:D,blogAddress:l,selfDescription:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",beLikedCount:b,beReadCount:b,followerCount:b,followingCount:b,collectionCount:b,createdCollectionCount:b,followingCollectionCount:b,postedPostsCount:b,pinCount:b,likedArticleCount:b,likedPinCount:b,avatar:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",latestLoginedInAt:c,createdAt:c,updatedAt:c,phoneNumber:e,titleDescription:e,followeesCount:b,applyEventCount:b,need_lead:b,followTopicCnt:b},viewCount:l,commentsCount:b,isEvent:l,abstract:"作为最近热门的生成式大模型的基石，Transformer自2017年诞生以来，就成为nlp研究者必须掌握的基本模型。本系列将通过图解方式，全方面展示Transformer的细节。",latestCommentAt:c,createdAt:new Date(1686135894000),updatedAt:c,isTopicEvent:a,likedCount:C,likeCount:C,content:e,originalUrl:e,type:"post",collected:a,viewsCount:1045,username:D,viewerHasLiked:a,draftId:"7241854869224128571",collectionCount:9},{id:"7241859817563389989",screenshot:l,liked:a,article_id:"7241859817563389989",article_info:{article_id:"7241859817563389989",user_id:v,category_id:"6809637773935378440",tag_ids:[6809640499062768000,6809640642101117000],visible_level:b,link_url:e,cover_image:"https:\u002F\u002Fp1-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F630524ca0259403ab41e1eebddd47567~tplv-k3u1fbpfcp-watermark.image?",is_gfw:b,title:"图解Transformer系列二：Self-Attention（自注意力机制）",brief_content:"作为最近热门的生成式大模型的基石，Transformer自2017年诞生以来，就成为nlp研究者必须掌握的基本模型。本系列将通过图解方式，全方面展示Transformer的细节。",is_english:b,is_original:g,user_index:9.534677558236224,original_type:b,original_author:e,content:e,ctime:"1686136780",mtime:"1689130802",rtime:"1686208720",draft_id:"7241859817563275301",view_count:976,collect_count:4,digg_count:p,comment_count:B,hot_index:58,is_hot:b,rank_index:.00120096,status:s,verify_status:g,audit_status:s,mark_content:e,display_count:b,is_markdown:g,app_html_content:e,version:s,web_html_content:c,meta_info:c,catalog:c,homepage_top_time:-62135596800,homepage_top_status:b,content_count:3053,read_time:"10分钟"},author_user_info:{user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",level:b,description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b}},category:{category_id:"6809637773935378440",category_name:"人工智能",category_url:"ai",rank:5,back_ground:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002Ff7cecf8806e8621ef35e.jpg",icon:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002F9b525117507d7a76c4ac.png",ctime:1500876664,mtime:1500876667,show_type:p,item_type:s,promote_tag_cap:4,promote_priority:5,id:"6809637773935378440",name:"人工智能",title:"人工智能",alias:"ai"},tags:[{entriesCount:l,subscribed:a,id:"6809640499062767624",tag_id:"6809640499062767624",tag_name:"算法",color:"#60ADFF",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F68a1097944c7fa1d7961.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:b,ctime:1439503293,mtime:1702476376,id_type:9,tag_alias:e,post_article_count:68679,concern_user_count:400538,title:"算法",tagId:"6809640499062767624",articleCount:68679,subscribersCount:400538,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"6809640642101116936",tag_id:"6809640642101116936",tag_name:"人工智能",color:"#2e6cff",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F9b525117507d7a76c4ac.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:g,ctime:1472072600,mtime:1702476143,id_type:9,tag_alias:e,post_article_count:47374,concern_user_count:245342,title:"人工智能",tagId:"6809640642101116936",articleCount:47374,subscribersCount:245342,createdAt:c,updatedAt:c}],user_interact:{id:7241859817563390000,omitempty:s,user_id:b,is_digg:a,is_follow:a,is_collect:a,collect_set_count:b},org:{org_info:c,is_followed:a},req_id:"20231213221218455CB3D6F6857CE33129",status:{push_status:b},theme_list:[],extra:{extra:e},title:"图解Transformer系列二：Self-Attention（自注意力机制）",user:{id:v,self_description:l,followed:a,viewerIsFollowing:l,community:l,subscribedTagCount:b,wroteBookCount:b,boughtBookCount:b,isBindedPhone:a,level:b,user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b},juejinPower:b,jobTitle:"🏆掘金签约作者｜人工智能方向",roles:{isBookAuthor:a,isFavorableAuthor:a,isCobuilder:a,isAdmin:a},username:D,blogAddress:l,selfDescription:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",beLikedCount:b,beReadCount:b,followerCount:b,followingCount:b,collectionCount:b,createdCollectionCount:b,followingCollectionCount:b,postedPostsCount:b,pinCount:b,likedArticleCount:b,likedPinCount:b,avatar:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",latestLoginedInAt:c,createdAt:c,updatedAt:c,phoneNumber:e,titleDescription:e,followeesCount:b,applyEventCount:b,need_lead:b,followTopicCnt:b},viewCount:l,commentsCount:B,isEvent:l,abstract:"作为最近热门的生成式大模型的基石，Transformer自2017年诞生以来，就成为nlp研究者必须掌握的基本模型。本系列将通过图解方式，全方面展示Transformer的细节。",latestCommentAt:c,createdAt:new Date(1686136780000),updatedAt:c,isTopicEvent:a,likedCount:p,likeCount:p,content:e,originalUrl:e,type:"post",collected:a,viewsCount:976,username:D,viewerHasLiked:a,draftId:"7241859817563275301",collectionCount:4},{id:"7275932704533282831",screenshot:l,liked:a,article_id:"7275932704533282831",article_info:{article_id:"7275932704533282831",user_id:v,category_id:"6809637773935378440",tag_ids:[6809640642101117000,6809640711177110000,7257794499869573000],visible_level:b,link_url:e,cover_image:"https:\u002F\u002Fp1-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002Fc167b60dfa874677b548cf8a83b553d3~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=2000&h=1041&s=2561639&e=png&b=a063e6",is_gfw:b,title:"CV大模型系列之：DALLE2，OpenAI文生图代表作解读",brief_content:"在CV大模型系列中，我们介绍了扩散模型的运作原理介绍了Transformer架构下做CV任务的方法，也介绍了多模态大模型CLIP。有了这么多的前置知识，今天，我们终于可以来一探文生图模型的奥秘了。",is_english:b,is_original:g,user_index:10.00006961079378,original_type:b,original_author:e,content:e,ctime:"1694066336",mtime:"1694157699",rtime:"1694157699",draft_id:"7275943125821718562",view_count:1603,collect_count:p,digg_count:5,comment_count:b,hot_index:85,is_hot:b,rank_index:.00405268,status:s,verify_status:g,audit_status:s,mark_content:e,display_count:b,is_markdown:g,app_html_content:e,version:g,web_html_content:c,meta_info:c,catalog:c,homepage_top_time:-62135596800,homepage_top_status:b,content_count:4116,read_time:"14分钟"},author_user_info:{user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",level:b,description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b}},category:{category_id:"6809637773935378440",category_name:"人工智能",category_url:"ai",rank:5,back_ground:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002Ff7cecf8806e8621ef35e.jpg",icon:"https:\u002F\u002Flc-mhke0kuv.cn-n1.lcfile.com\u002F9b525117507d7a76c4ac.png",ctime:1500876664,mtime:1500876667,show_type:p,item_type:s,promote_tag_cap:4,promote_priority:5,id:"6809637773935378440",name:"人工智能",title:"人工智能",alias:"ai"},tags:[{entriesCount:l,subscribed:a,id:"6809640642101116936",tag_id:"6809640642101116936",tag_name:"人工智能",color:"#2e6cff",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F9b525117507d7a76c4ac.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:g,ctime:1472072600,mtime:1702476143,id_type:9,tag_alias:e,post_article_count:47374,concern_user_count:245342,title:"人工智能",tagId:"6809640642101116936",articleCount:47374,subscribersCount:245342,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"6809640711177109517",tag_id:"6809640711177109517",tag_name:"计算机视觉",color:"#000000",icon:"https:\u002F\u002Fp1-jj.byteimg.com\u002Ftos-cn-i-t2oaga2asx\u002Fleancloud-assets\u002F8d1b7f5a309434d082fe.png~tplv-t2oaga2asx-image.image",back_ground:e,show_navi:b,ctime:1481241701,mtime:1702444680,id_type:9,tag_alias:e,post_article_count:2452,concern_user_count:31708,title:"计算机视觉",tagId:"6809640711177109517",articleCount:2452,subscribersCount:31708,createdAt:c,updatedAt:c},{entriesCount:l,subscribed:a,id:"7257794499869573175",tag_id:"7257794499869573175",tag_name:"LLM",color:e,icon:"https:\u002F\u002Fp3-juejin.byteimg.com\u002Ftos-cn-i-k3u1fbpfcp\u002F461a12434205464287418fa250495073~tplv-k3u1fbpfcp-watermark.image?",back_ground:e,show_navi:b,ctime:1689836965,mtime:1702471684,id_type:9,tag_alias:"大型语言模型,Large Language Model",post_article_count:855,concern_user_count:1161,title:"LLM",tagId:"7257794499869573175",articleCount:855,subscribersCount:1161,createdAt:c,updatedAt:c}],user_interact:{id:7275932704533283000,omitempty:s,user_id:b,is_digg:a,is_follow:a,is_collect:a,collect_set_count:b},org:{org_info:c,is_followed:a},req_id:"20231213221218455CB3D6F6857CE33129",status:{push_status:b},theme_list:[],extra:{extra:e},title:"CV大模型系列之：DALLE2，OpenAI文生图代表作解读",user:{id:v,self_description:l,followed:a,viewerIsFollowing:l,community:l,subscribedTagCount:b,wroteBookCount:b,boughtBookCount:b,isBindedPhone:a,level:b,user_id:v,user_name:D,company:e,job_title:"🏆掘金签约作者｜人工智能方向",avatar_large:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",description:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",followee_count:b,follower_count:b,post_article_count:b,digg_article_count:b,got_digg_count:b,got_view_count:b,post_shortmsg_count:b,digg_shortmsg_count:b,isfollowed:a,favorable_author:b,power:b,study_point:b,university:{university_id:f,name:e,logo:e},major:{major_id:f,parent_id:f,name:e},student_status:b,select_event_count:b,select_online_course_count:b,identity:b,is_select_annual:a,select_annual_rank:b,annual_list_type:b,extraMap:{},is_logout:b,annual_info:[],account_amount:b,user_growth_info:{user_id:b,jpower:b,jscore:b,jpower_level:b,jscore_level:b,jscore_title:e,author_achievement_list:[],vip_level:b,vip_title:e,jscore_next_level_score:b,jscore_this_level_mini_score:b,vip_score:b},is_vip:a,become_author_days:b,collection_set_article_count:b,recommend_article_count_daily:b,article_collect_count_daily:b,user_priv_info:{administrator:b,builder:b,favorable_author:b,book_author:b,forbidden_words:b,can_tag_cnt:b,auto_recommend:b,signed_author:b,popular_author:b,can_add_video:b},juejinPower:b,jobTitle:"🏆掘金签约作者｜人工智能方向",roles:{isBookAuthor:a,isFavorableAuthor:a,isCobuilder:a,isAdmin:a},username:D,blogAddress:l,selfDescription:"填坑工程师。\n分享技术笔记，也分享转行故事。希望朋友们在学习和写码的道路上不孤单。",beLikedCount:b,beReadCount:b,followerCount:b,followingCount:b,collectionCount:b,createdCollectionCount:b,followingCollectionCount:b,postedPostsCount:b,pinCount:b,likedArticleCount:b,likedPinCount:b,avatar:"https:\u002F\u002Fp9-passport.byteacctimg.com\u002Fimg\u002Fuser-avatar\u002F7b5ddd9a37165c489c861659be04fcc9~300x300.image",latestLoginedInAt:c,createdAt:c,updatedAt:c,phoneNumber:e,titleDescription:e,followeesCount:b,applyEventCount:b,need_lead:b,followTopicCnt:b},viewCount:l,commentsCount:b,isEvent:l,abstract:"在CV大模型系列中，我们介绍了扩散模型的运作原理介绍了Transformer架构下做CV任务的方法，也介绍了多模态大模型CLIP。有了这么多的前置知识，今天，我们终于可以来一探文生图模型的奥秘了。",latestCommentAt:c,createdAt:new Date(1694066336000),updatedAt:c,isTopicEvent:a,likedCount:5,likeCount:5,content:e,originalUrl:e,type:"post",collected:a,viewsCount:1603,username:D,viewerHasLiked:a,draftId:"7275943125821718562",collectionCount:p}],cursor:f,loading:a,skeleton:a,hasMore:a,articleId:j,actionType:{UPDATE_STATE:"view\u002Fcolumn\u002Frecommend-List\u002FUPDATE_STATE",FETCH_MORE:"view\u002Fcolumn\u002Frecommend-List\u002FFETCH_MORE",FETCH:"view\u002Fcolumn\u002Frecommend-List\u002FFETCH",RESET:"view\u002Fcolumn\u002Frecommend-List\u002FRESET"},sort:n}},collection:{collection:{author:{}},actionType:{FETCH:"@\u002Fview\u002Fcollection\u002FFETCH",REFRESH:"@\u002Fview\u002Fcollection\u002FREFRESH",RESET:"@\u002Fview\u002Fcollection\u002FRESET"},list:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Fcollection\u002Flist\u002FUPDATE",FETCH:"@\u002Fview\u002Fcollection\u002Flist\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Fcollection\u002Flist\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Fcollection\u002Flist\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fcollection\u002Flist\u002FRESET"},id:e,sort:o}},gettingStarted:{category:{},actionType:{UPDATE_STATE:"@\u002Fview\u002FgettingStarted\u002FUPDATE_STATE",FOLLOW:"@\u002Fview\u002FgettingStarted\u002FFOLLOW",RESET:"@\u002Fview\u002FgettingStarted\u002FRESET",UPDATE_CATEGORY:"@\u002Fview\u002FgettingStarted\u002FUPDATE_CATEGORY"}},pin:{pin:{user:{},imageUrlList:[]},pinList:[],actionType:{FETCH:"@\u002Fview\u002Fpin\u002FFETCH",RESET:"@\u002Fview\u002Fpin\u002FRESET"},sidebar:{list:[],after:e,loading:a,isRecommend:a,hasNextPage:d,actionType:{UPDATE_STATE:"@\u002Fview\u002Fpin\u002Fsidebar\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fpin\u002Fsidebar\u002FFETCH_MORE",FETCH:"@\u002Fview\u002Fpin\u002Fsidebar\u002FFETCH",RESET:"@\u002Fview\u002Fpin\u002Fsidebar\u002FRESET"}},commentList:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Fpin\u002FcommentList\u002FUPDATE",FETCH:"@\u002Fview\u002Fpin\u002FcommentList\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Fpin\u002FcommentList\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Fpin\u002FcommentList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fpin\u002FcommentList\u002FRESET"},pinId:c},subCommentList:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Fpin\u002FsubCommentList\u002FUPDATE",FETCH:"@\u002Fview\u002Fpin\u002FsubCommentList\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Fpin\u002FsubCommentList\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Fpin\u002FsubCommentList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fpin\u002FsubCommentList\u002FRESET"},commentId:c}},topic:{topic:e,followedTopicList:[],actionType:{FETCH:"@\u002Fview\u002Ftopic\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Ftopic\u002FUPDATE_STATE",RESET:"@\u002Fview\u002Ftopic\u002FRESET"},allTopicList:{pageSize:G,page:b,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Ftopic\u002FallTopicList\u002FUPDATE",FETCH:"@\u002Fview\u002Ftopic\u002FallTopicList\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Ftopic\u002FallTopicList\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Ftopic\u002FallTopicList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Ftopic\u002FallTopicList\u002FRESET"},sortType:m},pinlist:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Ftopic\u002FpinList\u002FUPDATE",FETCH:"@\u002Fview\u002Ftopic\u002FpinList\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Ftopic\u002FpinList\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Ftopic\u002FpinList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Ftopic\u002FpinList\u002FRESET"},sortType:n},sidebar:{actionType:{RESET:"@\u002Fview\u002Ftopic\u002Fsidebar\u002FRESET",UPDATE_STATE:"@\u002Fview\u002Ftopic\u002Fsidebar\u002FUPDATE_STATE"},attender:{pageSize:h,page:g,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Ftopic\u002Fsidebar\u002Fattender\u002FUPDATE",FETCH:"@\u002Fview\u002Ftopic\u002Fsidebar\u002Fattender\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Ftopic\u002Fsidebar\u002Fattender\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Ftopic\u002Fsidebar\u002Fattender\u002FFETCH_MORE",RESET:"@\u002Fview\u002Ftopic\u002Fsidebar\u002Fattender\u002FRESET"},topicId:c}},followedList:{pageSize:G,page:b,total:b,pointer:c,lastPointer:c,list:[],loading:a,error:c,canPrev:d,canNext:d,linkList:[],lastFetchOnServer:a,actionType:{UPDATE:"@\u002Fview\u002Ftopic\u002FfollowedList\u002FUPDATE",FETCH:"@\u002Fview\u002Ftopic\u002FfollowedList\u002FFETCH",FORCE_FETCH:"@\u002Fview\u002Ftopic\u002FfollowedList\u002FFORCE_FETCH",FETCH_MORE:"@\u002Fview\u002Ftopic\u002FfollowedList\u002FFETCH_MORE",RESET:"@\u002Fview\u002Ftopic\u002FfollowedList\u002FRESET"},after:b}},recommendationIndex:{actionType:{FETCH_USER:"@\u002Fview\u002Frecommendation\u002FFETCH_USER",FETCH_MORE:"@\u002Fview\u002Frecommendation\u002FFETCH_MORE",RESET:"@\u002Fview\u002Frecommendation\u002FRESET",FETCH:"@\u002Fview\u002Frecommendation\u002FFETCH"},cursor:e,hasMore:e,userList:[],loading:a,skeleton:d,category:k,categoryNavList:[],serverRenderUserList:a},event:{event:{},loading:a,user:{},actionType:{FETCH:"view\u002Fevent\u002FFETCH",RESET:"view\u002Fevent\u002FRESET"}},coursesIndex:{loading:a,list:[],sort:"online",actionType:{FETCH:"view\u002Fcourses\u002FFETCH",RESET:"view\u002Fcourses\u002FRESET",FETCH_MORE:"view\u002Fcourses\u002FFETCH_MORE"}},team:{team:{},loading:d,actionType:{FETCH:"@\u002Fview\u002Fteam\u002FFETCH",RESET:"@\u002Fview\u002Fteam\u002FRESET",UPDATE:"@\u002Fview\u002Fteam\u002FUPDATE",FOLLOW:"@\u002Fview\u002Fteam\u002FFOLLOW"},detailList:{actionType:{RESET:"@\u002Fview\u002Fteam\u002FdetailList\u002FRESET"},posts:{list:[],hasMore:a,skeleton:a,loading:a,sort:o,actionType:{FETCH:"@\u002Fview\u002Fteam\u002FdetailList\u002Fposts\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fteam\u002FdetailList\u002Fposts\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fteam\u002FdetailList\u002Fposts\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fteam\u002FdetailList\u002Fposts\u002FRESET"}},pins:{list:[],hasMore:a,loading:a,skeleton:d,actionType:{FETCH:"@\u002Fview\u002Fteam\u002FdetailList\u002Fpins\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fteam\u002FdetailList\u002Fpins\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fteam\u002FdetailList\u002Fpins\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fteam\u002FdetailList\u002Fpins\u002FRESET"}},hire:{list:[],hasMore:a,cursor:f,loading:a,skeleton:d,actionType:{FETCH:"@\u002Fview\u002Fteam\u002FdetailList\u002Fhire\u002FFETCH",UPDATE_STATE:"@\u002Fview\u002Fteam\u002FdetailList\u002Fhire\u002FUPDATE_STATE",FETCH_MORE:"@\u002Fview\u002Fteam\u002FdetailList\u002Fhire\u002FFETCH_MORE",RESET:"@\u002Fview\u002Fteam\u002FdetailList\u002Fhire\u002FRESET"}}}},couponList:{list:{"0":t,"1":t,"2":t},showTooltip:a},payment:{selectedDiscount:{},bookletDetail:{},coupons:{availables:[],unavailables:[]},discountList:[]},activityVip:{selectedVipSku:{}}},component:{indexAside:{bannerList:[],userList:[],actionType:{FETCH_BANNER:"@\u002Fcomponent\u002Faside\u002FFETCH_BANNER",FETCH_USER:"@\u002Fcomponent\u002Faside\u002FFETCH_USER",CLOSE_BANNER:"@\u002Fcomponent\u002Faside\u002FCLOSE_BANNER"}}},dislike:{whiteList:["1398234521548542",H,I,J,K,L,M],officialList:[H,I,"3984285868490807",J,"3562073405009031",K,"4433674252325966","53218623894222","2110693287406632","2498524693925623","430664288836334",L,M,"2832783991648407","3386151545092589"]},ore:{oreCount:b},avatarMenuInfo:{user_basic:{},user_counter:{},user_growth_info:{}},common:{theme:"light",isFollowSystem:a},env:{ua:"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F120.0.0.0 Safari\u002F537.36 Edg\u002F120.0.0.0",serverEnv:"production",logId:"20231213221218455CB3D6F6857CE33129"},auth:{user:c,clientId:c,token:c,qrCode:c,qrCodeStatus:c,qrCodeToken:c,userInitiated:a,loginTeaParams:l},tag:{subscribedTagList:[]},entry:{isLikeLoading:a},collection:{},comment:{},bookComment:{},repoComment:{},category:{list:[]},user:{subscribedTagList:[]},notification:{unreadCount:{user:b,system:b,total:b}},error:{location:c,errorView:c,statusCode:200,needRiskModal:a,riskAppealUrl:e},abTest:{info:{}},suspensionPanel:{needSuspension:d},pinComment:{},pin:{deleteDialogVisible:a,reportDialogVisible:a,targetPin:c,isOnFocus:a},topic:{visible:a},activity:{"2020":{},offer:{is_show:b,start_time:b},voteData:l},header:{leadStep:b,isPopupZlink:a},tcc:{tccConfig:c},adAssets:{adverts:[]},route:{name:N,path:u,hash:e,query:{},params:{id:j},fullPath:u,meta:{isAvailableDarkMode:d},from:{name:c,path:O,hash:e,query:{},params:{},fullPath:O,meta:{}}}},serverRendered:d,routePath:u,config:{API_HOST:"api.juejin.cn",CAPTCHA_HOST:"verify.snssdk.com",PLATFORM_APPID:{wechat:1277,weibo:1276,github:1045,wechatApp:1070},SCM_VERSION:"1.0.0.143",REGISTERED_ROUTES:[N,"selfPost","noCDNPost","SeoSearch"],http:{}},globalRefs:{}}}(false,0,null,true,"","0",1,20,"topic","7254341178258489404","recommended",void 0,"hot","popular","newest",3,"following",13,2,{},"\u002Fpost\u002F7254341178258489404","970190899118414","CV大模型系列之：全面解读VIT，它到底给植树人挖了多少坑","全面图解CV大模型开山之作: Vision Transformer（ViT），带领大家阅读VIT中的细节和VIT超越图像分类本身的意义。","7254361990075400229",2446,12,7,6,"猛猿",799,{},100,"1556564194374926","940837680722589","2780007432717400","1204720443866983","852876722177533","3227821828225517","column","\u002F"));</script><script src="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/e28e478.js" defer></script><script src="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/a99f4f3.js" defer></script><script src="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/2f4bee9.js" defer></script><script src="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/6a32973.js" defer></script><script src="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/60514e1.js" defer></script><script src="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/643346e.js" defer></script><script src="//lf3-cdn-tos.bytescm.com/obj/static/xitu_juejin_web/adfe0b1.js" defer></script>
  </body>
</html>
