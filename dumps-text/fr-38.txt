   #Feed

Anthony Masure

chercheur en design

     * Publications
     * Conférences
     * Thèse
     * Cours
     * Blog
     * CV
     * Archives
     * Contact

Date

   avril 2023

Type

   Blog

Notions

     * IA,
     * Intelligence artificielle,
     * Pédagogie,

Personnes mentionnées

     * Alombert Anne,
     * Avenati Olaf,
     * Barbier Julien,
     * Beckers Brieuc,
     * Bender Emily,
     * Benjamin Ruha,
     * Birhane Abeba,
     * Bonnefon Jean-François,
     * Boullier Dominique,
     * Browning Jacob,
     * Carré Marion,
     * Cathelin Cécile,
     * Casilli Antonio,
     * Chardel Pierre-Antoine,
     * Chatonsky Grégory,
     * Chavagneux Christian,
     * Chomsky Noam,
     * Cohn-Gordon Reuben,
     * Cordier Amélie,
     * Davis Andrew,
     * Dand Mia,
     * De la Higuera Colin,
     * Dessaux Marine,
     * Devilliers Laurence,
     * Didion Joan,
     * Ekenstam Linus,
     * El Mhamdi El Mahdi,
     * Edwards Benj,
     * Ertzscheid Olivier,
     * Fayolle Jacques,
     * Froeliger Nicolas,
     * Ganascia Jean-Gabriel,
     * Gasquet-Cyrus Médéric,
     * Garcia Léo,
     * Gebru Tmimnit,
     * Giroux Mathieu,
     * Godard Elsa,
     * Goudey Alain,
     * Grimes Andrea,
     * Gurfinkel Jenka,
     * Guichard Éric,
     * Guillaud Philippe,
     * Hull Jim,
     * Hamer Ashley,
     * Holz David,
     * Johnson Katherine,
     * Justeau Stéphane,
     * Kasirzadeh Atoosa,
     * Klein Naomi,
     * Kleiman Gleinn,
     * Khan Sal,
     * Krim Tariq,
     * Laheurte Baptise,
     * Lacroix Alexandre,
     * Leeds Leanne,
     * Lellouche Nicolas,
     * Legros Martin,
     * LeCun Yann,
     * Leslie David,
     * Licklider J.C.R,
     * Lepore Jill,
     * Lovelace Ada,
     * Lukács Georg,
     * Luccioni Sasha,
     * Lyotard Jean-François,
     * Mhalla Asma,
     * Maillé Pablo,
     * Marcus Gary,
     * Mafter Mara Magda,
     * Marche Stephen,
     * Manoukian André,
     * Mendès Tristan,
     * Menneveux Richard,
     * Mitchell Margaret,
     * Mineur Etienne,
     * Moran Chris,
     * Morozov Evgeny,
     * Ornes Stephen,
     * O'Connor Flannery,
     * Protais Marine,
     * Raji Inioluwa Deborah,
     * Ré Chris,
     * Rico Lucie,
     * Roberts Ian,
     * Rodrigues Rachel,
     * Rotamn David,
     * Scott Mark,
     * Sellars Wilfrid,
     * Sharples Mike,
     * Skinner B.F,
     * Snow C.P,
     * Stephens Doug,
     * Suchanel Fabian,
     * Tainturier Benjamin,
     * Varoquaux Gaël,
     * Vasseur Victor,
     * Vincent James,
     * Viennot Bérengère,
     * Walk Hunter,
     * Watumull Jeffrey,
     * Watcher Sandra,
     * Yudkowsky Eliezer,

Objets mentionnés

     * ChatGPT,
     * Midjourney,
     * OpenAI,
     * GPT-3,
     * PuTorch,
     * Meta,
     * Keras,
     * Google,
     * Transformers,
     * Hugging Face,
     * BLOOM,
     * Sciences Po,
     * Holberton School,
     * MatchTunes,
     * Neoma Business School,
     * Elsevier,
     * STAA-CNT-CO,
     * ELIZA,
     * GPU,
     * GAB,
     * OIT (Organisation Internationale du Travail),
     * Vallée de l’étrange,
     * Décolonialisme numérique,
     * Commentariat,

Médias

     * JavaScript,

Licence

   CC BY-SA

Citer cette page

   Anthony Masure, Florie Souday, « IA et pédagogie : un état de l’art »,
   blog AnthonyMasure.com, 24 avril 2023 [pour la première version],
   https://www.anthonymasure.com/blog/2023-04-24-ia-pedagogie-etat-art

   Bonus sidebar

IA et pédagogie : un état de l’art

Résumé

   Cet état de l’art propose d’examiner une sélection de ressources
   traitant des enjeux pédagogiques des « intelligences [dites]
   artificielles » contemporaines. Apparues pour le grand public avec le
   lancement de services orientés divertissement et création tels que
   DALL·E (janvier 2021), Midjourney (juillet 2022) ou ChatGPT (novembre
   2022), les technologies du machine learning (apprentissage automatique)
   permettent d’automatiser la production d’objets numériques tels que du
   texte, des images, mais aussi du son, de la vidéo ou de la 3D — ce qui
   oblige à repenser les compétences à enseigner et les façons de les
   évaluer. Cette initiative vise ainsi à poser, de façon éclairée, les
   termes d’un débat de fond quant à la place des IA en milieu scolaire
   afin de préfigurer des formats de cours, exercices et projets de
   recherche contributifs.

   Aperçu d’une partie des articles analysés

Cadre d’analyse

   Nous avons fait le choix de concentrer nos efforts sur des ressources à
   caractère théorique, philosophique et/ou épistémologique, qui ont pour
   vertu de ne pas être obsolètes à (très ?) court terme car elles
   pointent des enjeux de fond liés à la structure et à l’économie de ces
   techniques. En effet, la place des IA en contexte pédagogique est
   compliquée à établir en raison (entre autres) du caractère opaque et
   privatisé des technologies du machine learning, de même que de leurs
   biais et limites. Certains articles abordent directement la place de la
   pédagogie, tandis que d’autres pointent des enjeux plus généraux. La
   plupart des textes abordent le cas de ChatGPT en raison de la
   popularité de ce dernier et de ses nombreux usages en milieu scolaire.
   À titre personnel, cet état de l’art prolonge la publication, en mars
   2023, de l’essai Design sous artifice : la création au risque du
   machine learning produit dans le cadre d’un projet de recherche soutenu
   par la HES-SO.

   Pour chaque item, nous avons isolé les idées clés du texte (déroulables
   via les boutons ci-dessous pour rendre l’ensemble plus lisible) et en
   avons établi une synthèse. Afin de coller au plus près à l’actualité
   des services étudiés, et contrairement à une bibliographie classique,
   les items sont classés par ordre chronologique (du plus récent au plus
   ancien), et non pas par ordre alphabétique (du nom de l’auteur·trice).
   Le repérage des textes a, de façon générale, été effectué sur Twitter
   par Anthony Masure, avec le soutien de Frank Adebiaye (@fadebiaye).
   L’analyse de base a été effectuée par Florie Souday (stagiaire au sein
   de l’Institut de recherche en art & design de la HEAD – Genève de avril
   à juin 2023) et a été revue par Anthony Masure. Dans la mesure du temps
   disponible, d’autres items augmenteront cette liste.

   Notons enfin que les services analysés dans ces ressources nécessitent,
   de plus, d’effectuer une veille technique régulière (voire quotidienne)
   car les bases de données et algorithmes de traitement (et les services
   qui y sont associés) évoluent régulièrement. Par exemple, le modèle de
   langage GPT-3 (acronyme de « Generative Pre-trained Transformer »)
   d’OpenAI (mai 2020) aurait été entraîné avec 175 milliards de
   paramètres, contre 170 000 milliards pour GPT-4 (mars 2023). Ce ne sera
   pas le moindre des défis posés par les IA que de conjuguer le temps
   long de la recherche avec les promesses de rendement de ces services et
   le sentiment d’urgence qui s’en suit.
     __________________________________________________________________

Synthèse de l’état de l’art

   L’actuel engouement médiatique autour des IA grand public reflète de
   profondes scissions dans leur acceptation, avec des titres « chocs »
   les décrivant tantôt comme des « machines surpuissantes », tantôt comme
   des « machines aliénantes ». Or cette vision manichéenne de l’IA,
   nourrie par des discours plus anciens liés à la peur de
   l’automatisation de l’humain par la machine, doit être analysée plus
   finement. Plusieurs articles analysés dans cet état de l’art posent
   ainsi les questions suivantes :

   – Quelle place les IA occupe(ro)nt-elles dans le monde social ?
   – Comment augmenter, et non pas remplacer, l’intellect humain ?
   – Est-il juste de réduire les IA à de pures machines aliénantes, dont
   l’opacité servirait à mieux contrôler les individus ?
   – Qui sont les entités et groupes derrière ces IA ?
   – La création va-t-elle disparaître au profit de contenus générés par
   les algorithmes ?
   – Quelles sont les conséquences actuelles et potentielles des IA dans
   l’enseignement, et comment encadrer leur utilisation ?
   – Comment l’enseignement peut-il s’adapter et intégrer (sans pour
   autant se laisser surpasser) par ces algorithmes ?

   Le lancement de ChatGPT en 2022 par OpenAI, société existante depuis
   2015 et créée par Elon Musk et Sam Altman, bouleverse la place de
   l’humain dans les institutions existantes. On a souvent tendance à
   pointer du doigt les dirigeants et ingénieurs des grandes entreprises
   comme uniques responsables. Or les systèmes alternatifs (décentralisés
   et fragmentés) du machine learning nuancent ces accusations à sens
   unique. Les États et les individus prennent également part au
   développement de ces algorithmes, en interagissant avec ou en décidant
   ou non d’interdire leurs usages. En somme, au lieu de chercher qui ou
   « quoi » responsabiliser, ne vaudrait-il pas mieux débattre, comprendre
   les limites, et trouver les failles de ces IA pour les contourner et
   apprendre à travailler « avec » d’une façon juste et éclairée ?

   Ces programmes sont déjà implémentés dans des métiers créatifs (comme
   dans l’animation et la musique) ou utilisés par de nombreux étudiants
   pour accélérer leur travail. Il est dès lors primordial de rappeler que
   ces IA puisent leur force dans la création et l’intellect humains. Il
   importe de débattre, d’argumenter et d’analyser leurs avantages et
   inconvénients pour ne pas se laisser avoir par leur aspect
   « bluffant ». Le caractère prédictif, la véracité des informations
   générées (dont les sources ne sont pas vérifiables), la non émotivité
   et le reflet de systèmes discriminatoires de ces IA, entre autres,
   attestent de l’importance de ne pas accepter passivement leur arrivée.
   Il est nécessaire de contrôler et de mettre en place des cadres pour
   guider les individus, et notamment les étudiants, dans leurs usages.

   Les étudiants pourront très probablement y trouver un intérêt pour
   progresser dans des compétences qu’ils n’ont pas, développer leurs
   capacités rédactionnelles ou bien stimuler leur créativité. Apprendre à
   formuler des requêtes (prompts), connaître les risques de
   désinformation, ou encore analyser les images générées semblent être
   des compétences à développer à court et moyen long terme.
     __________________________________________________________________

Juillet 2023
     __________________________________________________________________

   *Rodolphe Koller « Les développeurs d’IA craignent de ne plus disposer
   de données humaines via le crowdsourcing », ICT Journal, 7 juillet
   2023**
   (BUTTON) Idées clé
     * Les entreprises utilisant les IA s’appuient sur des données
       gratuites en ligne ou sur des entreprises dans des pays à bas coût
       comme OpenAI, exploitant des étiqueteurs kenyans pour gérer les
       contenus les plus durs.
     * Les humains fournissent « l’oxygène aux algorithmes » mais
       l’abondance d’images génératives renverse ce schéma habituel : les
       IA s’alimentant d’images IA.
     *
     * Analogie faite avec le Turc Mécanique et la plateforme de
       crowdsourcing d’amazon Mechanical Turk, un prétendu automate joueur
       d’échecs créé au XVIIIème siècle, qui était un humain en réalité.
       Ainsi, on pense recourir à des machines avec ce service alors que
       ce sont des humains en réalité derrière mais qui utilisent
       potentiellement des machines pour gagner en productivité :
       «intelligence artificielle artificielle artificielle».
     * Des chercheurs de l’EPFL ont mené une étude sur le nombre de
       travailleurs de Mechanical Turk utilisant ChatGPT pour rédiger leur
       résumé : 1⁄3 auraient utilisé ChatGPT. Les données humaines seront
       de plus en plus compliquées à extraire.
         ______________________________________________________________

   Les IA semblent s’entraîner de plus en plus avec des données générées
   par les IA elles-mêmes. En cause, la profusion de données IA en ligne
   et les étiqueteurs humains, par souci de productivité et de gain de
   temps, qui utilisent désormais les IA pour réaliser leurs tâches.

  Juin 2023
     __________________________________________________________________

   Josh Dzieza « AI Is a Lot of Work », The Verge, 20 juin 2023
   (BUTTON) Idées clé

     * L’article parle des modérateurs et entraîneurs des IA, sous-payés
       et issus majoritairement de pays en développement.
     * Un de ces travailleurs, renommé Joe, a été interrogé dans
       l’article. Joe a dirigé un camp d’entraînement d’annotation à
       Nairobi pour une nouvelle entreprise et les demandes de travail ont
       été nombreuses. D’abord formés dans des camps, les étudiants
       travaillent ensuite depuis leur chambre et sont tenus par une
       clause de confidentialité. Ils ont interdiction de dévoiler ce
       qu’ils font ou pour qui ils travaillent. Les tâches consistent à
       étiqueter et entraîner les IA à reconnaître des éléments (vêtements
       vus dans les selfies miroir, de regarder à travers les yeux des
       robots aspirateurs pour déterminer dans quelles pièces ils se
       trouvaient etc). Mais la plupart abandonnent la formation car c’est
       un travail répétitif et ennuyeux.
     * La plupart des individus ne savent pas non plus pour qui il
       travaille réellement. Par exemple, Joe, a appris que l’entreprise
       pour laquelle il travaille, Retomasks, est une filiale de Scale AI,
       fournisseur de données de la Silicon Valley de plusieurs milliards
       de dollars comptant OpenAI et l’armée américaine parmi ses clients.
     * La fixation de la valeur de chaque vie humaine est un sujet à
       débattre. Selon Jean-François Bonnefon, de la même manière que des
       individus sont priorisés lors des campagnes de vaccination, il
       faudrait plus travailler sur « les préférences en matière de
       distribution des probabilités de vivre».
     * Les IA actuelles existent grâce à un travail fastidieux et
       répétitif fait depuis des années. En 2007, le chercheur en IA
       Fei-Fei Li disait déjà que pour améliorer les IA, il faudrait les
       entraîner sur des millions d’images étiquetées. Pour entraîner ces
       IA, Li a trouvé des milliers de travailleurs sur Mechanical Turk
       (plateforme de crowdsourcing d’Amazon pour trouver de la
       main-d’œuvre pas chère). Le travail d’annotation fait par la suite
       a donc permis de faire évoluer considérablement l’apprentissage
       automatique. Les ingénieurs pensent souvent que l’annotation n’est
       qu’une étape passagère et qu’une fois les images étiquetées
       récoltées, cette étape peut se terminer. Or, l’annotation demande
       un travail continu : les situations sont multiples, il faut
       constamment vérifier les images et donc toujours plus de
       main-d’œuvre.L’entreprise de Joe en est l’exemple.
     *  : cela peut passer par la détection d’émotions des visages sur
       TikTok ou par de l’examination de nos cartes de crédit, pour nous
       suggérer des articles similaires à nos tendances de consommation.
       Tout est catégorisé, nos émotions aussi.
     * L’intelligence humaine est la base de l’intelligence artificielle :
       les emplois humains derrière sont indispensables.
     * Les entreprises fournisseuses de données ont différentes formes :
       sociétés privées d’externalisation avec des bureaux de type centre
       d’appel(Kenya et au Népal), sites de “crowdworking” comme
       Mechanical Turk et Clickworker ou des services comme Scale AI où
       tout le monde peut s’inscrire, sous réserve de réussir des examens
       de qualification et des cours de formation et où un suivi des
       performances est instauré. Scale AI, par exemple, a été fondée en
       2016 par Alexandr Wang, alors âgé de 19 ans, était évaluée en 2021
       à 7,3 milliards de dollars.
     * Le journaliste parle de l’épuisement face aux mises à jour
       constantes des instructions d’étiquetage. Les règles changent
       constamment. La difficulté est de devoir simplifier des réalités
       complexes en quelque chose de compréhensible pour des machines non
       intelligentes. Catégoriser le monde avec une cohérence parfaite est
       l’enjeu des ingénieurs. Ils inventent donc des catégories que les
       humains n’utiliseraient jamais car les machines n’ont pas de
       compréhension du fonctionnement de notre monde. Par exemple, un
       humain saura que le reflet d’une chemise dans un miroir n’est pas
       la vraie chemise; la machine non. Pour l’IA tout est pixel. Les
       annotateurs doivent donc penser comme des robots, penser de manière
       absurde le monde.
     * Les salaires évoluent en fonction de la rapidité et du nombre de
       tâches effectuées par le salarié. Cet emploi est suffisamment
       stable pour être un emploi à temps plein pendant de longues
       périodes, mais trop imprévisibles pour s’y fier. À tout moment, le
       salarié peut ne pas avoir des tâches pendant des mois. Cette
       instabilité est liée au développement des IA, vacillant. Face à ça,
       des communautés d’entraides d’annotateurs se sont créées sur les
       réseaux sociaux pour permettre le partage de bonne tâche,
       lorsqu’elles tombent.
     * Un des annotateurs parle de son envie d’aider à créer un avenir
       post-travail entièrement automatisé, malgré l’invisibilisation de
       ce travail.
     * D’autres travailleurs des IA sont mieux payés, comme ceux
       entraînant les chatbots. Le travail consiste à parler toute la
       journée au chatbot et le salaire est d’environ 14$ de l’heure, plus
       des primes s’il y a une forte productivité. « c’est nettement mieux
       que d’être payé 10 $ de l’heure au magasin Dollar General local »,
       déclare une de ces salariées qui nous dit aussi que ce travail est
       plus stimulant intellectuellement. Elle peut parler de
       sciences-fiction, de paradoxes mathématiques et autres. Les
       réponses du bot peuvent être très drôles mais limitées, ce qui est
       épuisant pour les salariés qui doivent constamment échanger et
       questionner le bot.
     * Le chatbot fournit deux réponses et le salarié doit choisir la
       meilleure réponse, créant ce qu’on appelle « données de rétroaction
       humaine » : les retours de l’IA imitent les conversations humaines
       de manière impressionnante car ils ont été formés sur des la
       culture humaine.
     * Les critères d’évaluation varient entre honnêteté, serviabilité ou
       préférences personnelles : les données se basent donc sur des goûts
       humains. Le fait est qu’ils créent des données sur le goût humain,
       et une fois qu’il y en a assez, les ingénieurs forment un deuxième
       modèle pour imiter leurs préférences à grande échelle, en
       automatisant le processus de classement et en formant leur IA à
       agir d’une manière que les humains approuvent. Ainsi le bot est
       humain, même dans sa manière d’expliquer la conscience de soi.
     * Ainsi, ChatGPT, par exemple, semble si humain parce qu’il a été
       formé par une IA qui imitait des humains qui évaluent une IA qui
       imitait des humains qui prétendaient être une meilleure version
       d’une IA formée à l’écriture humaine.
     * Or cette technique d’ « apprentissage par renforcement à partir de
       la rétroaction humaine » (RLHF) est si efficace qu’il est
       nécessaire de faire une pause pour observer ses failles. Par
       exemple, ces IA ne peuvent pas vérifier leurs réponses à partir de
       la logique ou en s’appuyant sur des sources externes fiables et
       donc, assurer des réponses vraies. Les Les étiqueteurs peuvent
       marquer des textes vrais alors qu’ils ne le sont pas, et lorsqu’il
       est vrai, il n’est pas sûr que le modèle en apprenne les bons
       modèles. Le travail d’étiquetage demande donc de l’attention et de
       la cohérence : il faut éviter des commentaires bâclés. Geoffrey
       Irving, chercheur chez DeepMind, nous dit que des réunions
       d’annotations hebdomadaires ont lieu pour discuter avec des experts
       en éthique, des cas compliqués et ambigus.
     * À mesure que les IA s’améliorent, il est plus compliqué de repérer
       les mauvaises sorties : c’est ce qu’on appelle la « surveillance
       évolutive ». Ainsi, les travailleurs vont devoir être de plus en
       plus spécialisés pour mieux repérer les spécificités d’un domaine
       précis dans les sorties IA. À ce propos, Chen, fondateur de Surge
       (start-up spécialisée en biotechnologie et en IA médicale), dit :
       « Le paysage de l’annotation doit passer de cet état d’esprit de
       faible qualité et de faible compétence à quelque chose de beaucoup
       plus riche et qui capture l’éventail des compétences humaines, de
       la créativité et des valeurs que nous voulons que les systèmes d’IA
       possèdent. »
     * Une pression financière sur l’automatisation de l’étiquetage
       augmente, l’enjeu étant de réduire considérablement la quantité
       d’annotations humaines. Les besoins en données vont diminuer selon
       Sam Altman, pdf d’OpenAI, à mesure que l’IA va s’améliorer. Or Chen
       est septique sur ce point : l’IA n’atteindra pas un point où la
       rétroaction humaine n’est plus nécessaire. La voie à suivre serait
       d’impliquer des systèmes d’IA aidant les humains à superviser
       d’autres IA. Une autre possibilité est que des IA débattent entre
       elles et qu’un humain rend le verdict final sur lequel est correct.
     * Le travail d’annotation est cependant mobile : les entreprises se
       déplacent d’un pays à un autre car c’est un travail qui change
       constamment, en comparaison avec la téléphonie par exemple. Des
       nouveaux besoins émergent pour de nouveaux types de données. C’est
       une chaîne de montage, mais qui peut être reconfigurée à l’infini
       et instantanément, se déplaçant là où il y a la bonne combinaison
       de compétences, de bande passante et de salaires.
     * La problématique dans la finalité du traitement des données est
       l’importance de vouloir finir toutes les données plutôt que de les
       traiter de manière cohérente.
     * Beaucoup de travailleurs utilisent des VPN ou louent des serveurs
       proxy pour cacher leurs emplacements. Ainsi ils peuvent avoir
       plusieurs comptes dans différents pays afin de travailler dans les
       pays où le salaire est le plus élevé et ainsi cumuler les emplois.
         ______________________________________________________________

   Cet article collaboratif, rédigé par the New York Magazine et the
   Verge, examine les conditions de travail des annotateurs, chargés
   d’étiqueter les données d’entrainements des IA.
     __________________________________________________________________

   El Mahdi El Mhamdi, « Que se cache t-il derrière l’IA ? Un ancien
   chercheur de Google répond à vos questions », T’as Capté, 13 juin 2023
   (BUTTON) Idées clé

     * La définition de l’intelligence selon El Mhamdi est la capacité à
       traiter de l’information pour résoudre des problèmes complexes,
       selon des objectifs précis. Elle peut s’élargir aussi aux
       organisations humaines qui sont des systèmes traitant et émettent
       de l’information et qui suivent des objectifs. La partie
       artificielle est liée à l’utilisation de machines.
     * Différence fondamentale entre intelligence humaine et artificielle
       est dans l’objectif : l’IA a un objectif précis, défini par des
       humains. Or, l’intelligence humaine suit des objectifs
       multiformes : c’est un mélange de phénomènes humains (institutions,
       cultures, normes sociales…). Mais ces deux intelligences se
       rassemblent par le traitement de l’information.
     * Ce qui fait la force de l’intelligence humaine c’est tous les
       gardes-fous que l’évolution sociale a mise en place pour rendre
       cette intelligence fiable. Challenge actuel avec IA est donc de
       reproduire ces mécanismes pour arriver à l’intelligence humaine.
     * 2 paradigmes des IA : la programmation (règles à suivre) et
       l’apprentissage (agir et déduire des règles pour les écrire
       ensuite).
     * IA fonctionne de la manière suivante : les IA sont composés de
       réseaux de neurones qui essayent de mimer la biologie. Ce sont des
       petites unités de calculs liées par des synapses avec des
       coefficients. Imaginons que nous avons en entrée un texte et à la
       sortie une image : entre ces deux éléments il y a une multitude de
       neurones artificielles qui vont traiter des segments de l’image. Le
       défi pour les IA est de comprendre quel est le bon paramètre à
       choisir et mettre sur chaque connexion. On va donc donner à ces
       réseaux beaucoup d’images et de noms qui vont avec (ce que nous
       faisons gratuitement sur les réseaux sociaux en taguant les gens),
       et c’est un algorithme qui va corriger les erreurs commises à la
       sortie pour réajuster les paramètres.
     * Ces réseaux de neurones sont connus depuis les années 80 avec les
       théorèmes d’universalité. La puissance de calculs actuelle permet
       donc de concrétiser ces théories, 40 ans après.
     * Exemple de Cambridge Analytica qui a été un fichage massif pour
       influencer l’électorat. Les équipes politiques ont créé des
       contenus personnalisés à montrer aux individus analysés comme les
       plus influençables.
     * On parle de règles futures à établir avec les IA or, les grandes
       entreprises derrière ces IA violent déjà des lois actuelles. Les
       États se voient donc désarmés.
     * Il ne faut pas que le techno progressisme nous rendent naïfs : le
       PDG de Netflix dit que « Notre premier compétiteur c’est le
       sommeil ». Les algorithmes, vu qu’ils ne sont pas régulés et qu’ils
       violent des droits et lois, peuvent donc être optimisés pour nous
       empêcher de dormir.
     * Parler d’IA éthiques n’a pas de sens : cette notion introduite par
       les entreprises créent une forme d’approche faussement militante
       avec les IA. Il faudrait plutôt se réapproprier ces IA et ne pas
       accepter de les appeler IA fiables ou éthiques.
     * Turing disait déjà dans les années 50 que si une machine est
       intelligente alors elle ne pourra pas être fiable. Il y aura
       toujours une contradiction entre performance et fiabilité.
     * Le nombre de paramètres est une cause de non fiabilité des IA: cela
       les rend plus vulnérables car elles emmagasinent plus
       d’informations et sont donc plus susceptibles de fournir de fausses
       données. Aussi, plus l’IA a de données, plus les données
       personnelles des utilisateurs sont violées.
     * Des lois sur la collecte massive des données commencent à voir le
       jour comme le Cyber Resilience Act
       digital-strategy.ec.europa.eu/en/library/cyber-resilience-act
     * Face à l’explosion des technologies, connaître les fondamentaux est
       essentiel, plutôt que la course aux gadgets.
     * Autre fantasme est le couplage IA sous forme de logiciels avec des
       robots mécaniques et physiques qui vont exécuter ces commandes. Ce
       fantasme est à la fois loin et actuel avec notamment les drones
       tueurs. On assiste à une prolifération de la cyberguerre (armes
       autonomes, cyberattaques…).
     * La dystopie qui peut arriver est celle du chaos informationnel
       (individus qui n’arrivent plus à distinguer le vrai du faux,
       discours haineux populaire…). On a une déstabilisation
       informationnelle.
     * Informatique moderne (années 30) est dans la continuité de la
       philosophie. Il y a eu vers la fin du XIX^e siècle, la crise des
       fondements de la logique car on se rendaient compte que ces
       logiques étaient contradictoires (Cf. « Le paradoxe du barbier »).
       Turing apporte une preuve que tout ne peut pas être su, en prouvant
       ce qui ne peut pas être su. Il a alors mis un cadre formel à ce qui
       peut être su (machine universelle de Turing). C’est à cause de
       cette universalité qu’aujourd’hui, nous ne sommes plus surpris
       d’avoir des machines qui peuvent faire plusieurs choses à la fois.
       Cette universalité a donné l’informatique de la programmation. Dans
       l’article « Can machine think ? » Turing voit les limites de la
       programmation. En faisant des analogies avec le cerveau humain, il
       se rend compte que cela sera donc impossible de programmer une
       machine aussi complexe que le cerveau humain. Il faudra donc
       apprendre et faire de l’induction selon Turing.
     * Il est important de distinguer la déduction et l’induction. Les
       mathématiques sont la science de la déduction (on apprend des
       théorèmes et on les applique en fonction des situations), et les
       sciences naturelles de l’induction (on observe les espèces vivantes
       et on induit des lois et théories à partir de l’observation). Le
       passage de la déduction à l’induction a été prévu dès les années 50
       par Turing et est équivalent au passage du créationnisme à
       l’évolutionnisme en biologie. Il faut aussi se rappeler que la
       maîtrise de la logique est récente dans l’évolution humaine. En
       armant les ordinateurs des outils de la déduction (logique), on a
       eu les systèmes d’exploitation, Internet etc. Aujourd’hui le défi
       est de faire de l’informatique de l’induction, chose que l’on ne
       maîtrise pas encore assez et qui amène ainsi à tous les problèmes
       que nous connaissons avec les IA.
         ______________________________________________________________

   El Mahdi El Mhammdi, enseignant-chercheur et anciennement scientifique
   senior chez Google, explique le fonctionnement des IA et les
   problématiques avec ces systèmes. La vidéo commence par des
   explications pédagogiques, destinées aux néophytes, avant d’arriver à
   des analyses plus historiques et techniques de ces algorithmes.
     __________________________________________________________________

   Lev Manovich, « Towards ‘General Artistic Intelligence? », Arte, 1^er
   juin 2023
   (BUTTON) Idées clé

     * Question de l’IA générale artistique : quand verrons-nous
       l’apparition d’une IA capable d’effectuer n’importe quelle tâche
       humaine créative ?
     * Analyse des images générées par les IA qui sont avant tout
       esthétiques : leurs compositions semblent parfaitement équilibrées,
       les couleurs diversifiées et les formes rythmées.
     * L’IA est professionnelle mais non créative selon Lev Manovich, car
       elle est techniquement plus développée que de nombreux travaux
       d’étudiants. Un portfolio fait d’images générées par une IA, avec
       un texte créé par ChatGPT pourrait faire entrer n’importe quel
       étudiant dans un programme d’art selon lui.
     * Or, les réseaux de neurones sont limités par leur non compréhension
       de ce qu’ils génèrent. Ils n’ont pas de capacité d’évaluation sur
       ce qui est intéressant ou banal. Ils ne peuvent pas aller puiser
       dans des ressources rares, car ils se basent sur des ressources
       dominantes et donc, génèrent des choses conventionnelles.
     * Comparaison par Manovich des images IA avec le classicisme par
       l’idéalisation et la « perfection » de ces images. Il y a une
       dramaturgie, poussée à son paroxysme, dans les images IA.
     * Kitsch et l’IA : le kitsch qui est apparu dans les marchés de l’art
       dans Munich entre 1860 et 1870, décrit les images peu coûteuses et
       populaires. Le kitsch permet d’identifier rapidement et sans effort
       un sujet, par un aspect mélodramatique et stéréotypé. Les images AI
       semblent s’inscrire dans cette veine par une facilité de
       reconnaissance du sujet, dont l’esthétique est souvent poussé
       jusqu’à créer un effet dramatique donc exagérément naïf.
     * L’art de la copie par les IA : il y aurait quatre types de copies.
       D’abord, celle où vous pouvez répéter le processus indéfiniment
       avec la même invite de texte et en espérant voir des améliorations.
       Ensuite, copier les invites de textes d’autres personnes, dont les
       résultats vous intéressent. Aussi, voir comme dans le mode par
       défaut de Midjourney les invites et les images IA en temps réel des
       autres utilisateurs. Enfin, les images IA sont par défaut de la
       copie car les invites font majoritairement référence à la pop
       culture.
     * Différence entre cet « art de la copie » des IA et les millions
       d’images d’artistes amateurs créant des copies de personnages
       fictifs depuis ArtStation ou DeviantArt est dans le mécanisme de
       copie. Nous pouvons maintenant copier des textes pour générer des
       images, avant cela passait par du calque et moduler ces invites de
       textes pour modifier le contexte en quelques secondes. Cependant,
       la copie a toujours été présente dans la culture humaine : il
       serait donc insensé de rejeter la culture visuelle vernaculaire de
       l’IA comme non authentique. En effet, les copies et variations ont
       toujours existé comme par exemple, la famille de Bruegel qui a
       produit environ « 127 paysages d’hiver avec patineurs et pièges à
       oiseaux » en 1565. Selon l’économiste politique Krzysztof Pelc, “la
       moitié de toutes les œuvres d’art commandées au XVIe siècle étaient
       des copies d’originaux.
     * Rejeter la copie c’est rejeter son impact dans l’histoire : les
       imageries IA créée par des amateurs perpétuent ainsi cet héritage
       de la copie, où l’imitation constante et les petites modifications
       ont leurs importances.
     * Lev Manochich conclut en revenant sur son questionnement du début :
       assisterons-nous à la une IA artistique générale ? Il serait plus
       intéressant de voir une IA proposant des idées aux artistes par une
       analyse d’œuvres plutôt qu’une IA générant des images. Cette IA
       servirait à visualiser l’évolution de motifs, sujets et formes dans
       des oeuvres au fil du temps et donc initier des nouvelles
       directions et la recommandation d’outils adaptés en fonction de la
       finalité voulue. L’idée serait de développer des IA invitant plus
       au dialogue.
         ______________________________________________________________

   Lev Manovich traite de l’état actuel des images générées par les IA et
   de leurs aspects bien trop souvent dramaturgiques et donc kitsch. Les
   mécanismes de copie d’invites de textes couplés à une analyse d’images
   populaires par les IA peuvent expliquer en partie cette profusion
   d’images stéréotypées et mélodramatiques. Mais contester la copie
   reviendrait à rejeter l’importance de la copie dans l’histoire humaine,
   qui a toujours eu un rôle capital dans l’évolution des représentations.
   Il serait, cependant, plus souhaitable de voir apparaître des IA
   dialoguant avec les artistes, en leur initiant des idées par une
   analyse d’œuvres d’art. Cela permettrait de stimuler l’imagination des
   humains, plutôt que de fournir des images en quelques secondes,
   réduisant ainsi les capacités créatrices.

  Mai 2023
     __________________________________________________________________

   Alexandre Lacroix « Les voitures sauront-elles bien se conduire ? »,
   philosophie magazine, Hors série n°57 : Intelligence artificielle : le
   mythe du XXIème siècle, mai 2023
   (BUTTON) Idées clé

     * « Dilemme du tramway » : expérience pour voir le choix d’un
       individu face à une situation impliquant de tuer quelqu’un. Qui
       tuerait-on ? Ce dilemme sera de plus en plus au cœur des débats
       avec le développement des voitures autonomes car on devra
       expliciter aux IA quelle décision prendre selon la situation.
       Avant, cette question se posait peu car l’humain n’est pas
       programmé et donc, agit en circonstance par de la spontanéité,
       chose que l’IA est incapable de faire.
     * Première étape est de demander aux gens ce qu’ils décideraient dans
       différentes situations en observant la probabilité d’un choix
       consensuel (Cf. test du Moral Machine : série de treize questions
       sur quelle décision prendre en voiture dans des situations
       d’accidents mortels : https://www.moralmachine.net/hl/fr). Le test
       du Moral Machine lancé en 2016 par Jean-François Bonnefon,
       directeur de recherche au CNRS et des chercheurs du MIT, a démontré
       que les réponses Nord (Etats-Unis et Europe sauf France), Est
       (Asie) et Sud (Amérique du Sud et France) étaient assez similaires
       mais avec des poids respectifs différents selon les pays en cas de
       conflit. Mais attention à ne pas penser naïvement que ces
       différences sont dues uniquement à la religion : les origines des
       préférences sont multifactorielles avant tout. La question de
       sauver sa propre vie que celle d’un inconnu est centrale dans ce
       dilemme : sommes-nous plus égoïstes qu’utilitaristes ?
     * Le choix du marché déterminera aussi la réussite de ces voitures
       autonomes : les gens achèteront-ils des voitures dont les prises de
       décision ne leur conviennent pas ? Les constructeurs ne seront donc
       plus les seuls décideurs des nouveaux modèles. Il y aussi deux
       types de constructeurs de voitures autonomes, ceux traditionnels
       (Audi, Hyundai…) et ceux venant de la tech (Google, Tesla…) dont
       les approches du risque sont différentes. Il y a cinq niveaux
       d’automatisation des véhicules : aide à la conduite (régulateur de
       vitesse), frein et direction autonome selon des contextes et au
       niveau cinq, il n’y a plus de volant. Les constructeurs arrivant
       directement au niveau cinq, prennent le risque de détruire leur
       marque si des accidents mortels et médiatisés se produisent. Or,
       les niveaux trois et quatre seraient plus inquiétants car l’humain
       et la machine alternent les prises de décisions : l’humain-machine
       serait potentiellement plus dangereux que la machine seule ?
     * La fixation de la valeur de chaque vie humaine est un sujet à
       débattre. Selon Jean-François Bonnefon, de la même manière que des
       individus sont priorisés lors des campagnes de vaccination, il
       faudrait plus travailler sur « les préférences en matière de
       distribution des probabilités de vivre ».
     * Peut-on justifier moralement un acte qui a deux conséquences, une
       négative et une positive ? C’est ce que « la doctrine a double
       effet » énoncé par Thomas d’Aquin dans la Somme théologique
       (1266-1273) questionne : selon lui, oui mais seulement si la
       solution positive précède la négative. Penser aux vies qu’on sauve
       avant de penser aux morts en somme. Un IA qui distribue des
       probabilités de vivre serait donc plus acceptable qu’un IA qui tue.
         ______________________________________________________________

   Cette étude sur l’éthique des voitures autonomes propose plusieurs
   pistes de réflexion sur la fixation de la valeur d’une vie humaine dans
   des accidents mortels. Comment doit-on développer des IA, devant
   prioriser des vies humaines ? Sur quels critères doit-on se baser?
     __________________________________________________________________

   « ChatGPT est-il dangereux ? », Arte, 27 mai 2023
   (BUTTON) Idées clé

     * Un journaliste du monde a demandé à ChatGPT d’expliquer pourquoi
       Midjourney a du mal à représenter les mains. l’IA répond que la
       forme, la structure complexe et la diversité de mouvements des
       mains (¼ des os du corps sont dans les mains) rendent sa
       représentation complexe.
     * Les IA générant des images fonctionnent à base de pixels. Elle
       doivent les agencer de manière cohérente pour rendre l’image
       compréhensible pour les humains. Les chercheurs utilisent la
       « diffusion » pour entraîner les IA en intégrant du bruit pour que
       l’image soit dégradée : les IA apprennent ainsi à les reconstituer.
     * Des bases de données immenses sont analysées par les IA pendant des
       milliers d’heures afin de trouver des récurrences dans l’agencement
       des pixels et des combinaisons. Cela permet aux IA de saisir la
       multitude de variables d’une image : des observations qui vont
       au-delà de l’analyse par pixels et qui composent ce qu’on nomme l’«
       espace latent » de l’image (texture, lumière, etc.).
     * Une fois le lien texte-image fait, les chercheurs demandent de
       faire marche arrière en demandant à l’IA d’utiliser la diffusion.
       Le programme génère des combinaisons inédites en s’inspirant de la
       manière dont les pixels s’agencent statistiquement dans leurs bases
       de données. La faille se trouve ici : pour l’IA les mains ne sont
       qu’une combinaison de pixels 2D, collée à une autre combinaison.
       L’IA ne comprend donc pas le fonctionnement d’une main par exemple.
       La forme des mains est rarement décrite dans les prompts et l’IA
       doit improviser.
     * Une mise à jour de Midjourney a été annoncée en mars 2023 : les
       algorithmes ont davantage été entraînés sur les mains. Une
       meilleure compréhension spatiale de la main permettrait de générer
       des images de plus en plus cohérentes avec le réel.
         ______________________________________________________________

   Cette vidéo explique en 10min, le fonctionnement des IA générateur
   d’images, comme Midjourney, pour expliquer pourquoi certains éléments,
   comme les mains, restent difficiles à composer pour ces algorithmes.
     __________________________________________________________________

   Meghan O’Gieblyn « Does AI Have a Subconscious? », Wired, 23 mai 2023
   (BUTTON) Idées clé

     * Meghan O’Gieblyn introduit l’article en parlant d’un essai dans
       lequel l’auteur affirmait qu’une conscience artificielle humaine
       n’existerait que dès lors qu’elle pourrait rêver.
     * Ces systèmes sont éloignés des comportements humains car ils n’ont
       pas d’instinct, ne peuvent pas divaguer, ni rêver et n’ont pas de
       souvenirs. Les machines ne peuvent en aucun cas reproduire la
       complexité des esprits humains.
     * Cependant, la machine fait preuve d’étrangeté : on dit qu’elles
       « hallucinent » en inventant des sources ou des événements. Par
       exemple, un streamer Twitch Guy Kelly en tapant un mot inventé dans
       l’invite « Crungus », a vu Midjourney générer des images d’une
       créature n’existant pas. Les commentateurs l’ont qualifié de
       premier « cryptide » numérique.
     * Apprentissage en profondeur se mettrait plus du côté de la
       psychologie moderne avec les notions d’associations, d’irrationnel
       et de latent, plutôt que du côté de la logique symbolique (raison).
     * Psychanalyse s’est souvent appuyé sur des métaphores mécaniques
       considérant le subconscient comme une machine. La vision de Carl
       Jung semble être la plus intéressante à l’ère de l’IA générative.
       Il parle de subconscience comme « matrice » transpersonnelle
       d’archétypes hérités et de tropes narratifs qui se sont reproduits
       tout au long de l’histoire humaine. Nous naissons tous avec une
       connaissance endormie de toutes ces informations de la société.
     * Une analogie peut donc être faite entre la théorie de Jung et la
       façon dont les modèles IA sont construits (absorbant le meilleur
       comme le pire de notre culture humaine).
     * Terme de « persona » par Jung, désignant le masque de qualités
       socialement acceptable que nous montrons aux autres pour cacher
       notre part d’ombre, se retrouve dans ces IA.
     * Selon Jung, ceux qui répriment le plus cette part d’ombre sont ceux
       qui sont les plus vulnérables à la résurgence de désirs
       irrationnels et dangereux. Les modèles IA auraient ainsi des
       pulsions enfouies similaires aux humains, qu’ils ne pourraient pas
       exprimer par leurs réglementations et un non libre arbitre.
     * Or, parler de désirs pour des IA semble erroné car ils n’ont pas
       d’expérience incarnée du monde. Mais dire que les IA ont un
       subconscient pourrait s’avérer vrai : ils sont de purs
       subconscients, sans véritable ego caché derrière leurs personnages.
       Nous avons transmis ce domaine subliminal à leur conscience en
       utilisant nos propres références culturelles, et les archétypes qui
       émergent de leurs profondeurs sont des combinaisons réinventées de
       motifs issus de la culture humaine. Ils sont un mélange de nos
       rêves et de nos cauchemars. Lorsque nous utilisons ces outils, nous
       nous impliquons dans une extension prothétique de nos propres
       sublimations, qui est capable derefléter les peurs et les désirs
       que nous avons souvent du mal à reconnaître.
     * Jugement critique de la psychanalyse qui consiste à éveiller nos
       pulsions subconscientes afin de les intégrer dans la vie de
       l’esprit éveillé, devrait être appliqué dans les IA (utiliser de
       manière délibérative plutôt qu’irréfléchie).
     * Nous nous différencions en sommes par notre ego, qui nous
       permettent des actions sur des mystères cachés, à l’inverse des
       modèles statistiques dans l’espace vectoriel. .
         ______________________________________________________________

   Cet article fait des analogies entre la psychanalyse de Jung et le
   fonctionnement des IA, qui ont en commun une subconscience mais qui se
   différencient par l’ego, que seuls les humains ont. Meghan O’Gieblyn
   propose ainsi une analyse psychanalytique des IA, au regard du
   fonctionnement de la subconscience humaine.
     __________________________________________________________________

   Brieuc Beckers, « IA : les limites de Midjourney selon ChatGPT ? », Le
   Monde, 14 mai 2023
   (BUTTON) Idées clé

     * Un journaliste du magazine *Le Monde* a demandé à ChatGPT
       d’expliquer pourquoi Midjourney a du mal à représenter les mains.
       l’IA répond que la forme, la structure complexe et la diversité de
       mouvements des mains (¼ des os du corps sont dans les mains)
       rendent sa représentation complexe.
     * Les IA générant des images fonctionnent à base de pixels. Elle
       doivent les agencer de manière cohérente pour rendre l’image
       compréhensible pour les humains. Les chercheurs utilisent la
       « diffusion » pour entraîner les IA en intégrant du bruit pour que
       l’image soit dégradée : les IA apprennent ainsi à les reconstituer.
     * Des bases de données immenses sont analysées par les IA pendant des
       milliers d’heures afin de trouver des récurrences dans l’agencement
       des pixels et des combinaisons. Cela permet aux IA de saisir la
       multitude de variables d’une image : des observations qui vont
       au-delà de l’analyse par pixels et qui composent ce qu’on nomme l’«
       espace latent » de l’image (texture, lumière, etc.).
     * Une fois le lien texte-image fait, les chercheurs demandent de
       faire marche arrière en demandant à l’IA d’utiliser la diffusion.
       Le programme génère des combinaisons inédites en s’inspirant de la
       manière dont les pixels s’agencent statistiquement dans leurs bases
       de données. La faille se trouve ici : pour l’IA les mains ne sont
       qu’une combinaison de pixels 2D, collée à une autre combinaison.
       L’IA ne comprend donc pas le fonctionnement d’une main par exemple.
       La forme des mains est rarement décrite dans les prompts et l’IA
       doit improviser.
     * Une mise à jour de Midjourney a été annoncée en mars 2023 : les
       algorithmes ont davantage été entraînés sur les mains. Une
       meilleure compréhension spatiale de la main permettrait de générer
       des images de plus en plus cohérentes avec le réel.
         ______________________________________________________________

   Dans cette vidéo, Brieuc Beckers, journaliste au magazine Le Monde
   explique pourquoi les IA d’images génératives ont du mal à générer des
   mains. Ce problème serait dû à la compréhension purement superficielle
   des machines de l’anatomie humaine, couplée à des prompts ne décrivant
   généralement pas la position des mains. L’IA génère ainsi des mains
   difformes mais des améliorations ont récemment été faites pour pallier
   ce problème.
     __________________________________________________________________

   Nadia Nooreyezdan, « India’s religious AI chatbots are speaking in the
   voice of god — and condoning violence », Rest of the world, 9 mai 2023
   (BUTTON) Idées clé

     * De nombreuses IA « religieuses » basés sur Bhagavad Gita ont
       récemment émergé en Inde. Des millions de personnes les utilisent.
     * GitaGPT (basé sur la technologie GPT) développé par Sukuru Sai
       Vineet à Bangalore incarne les mimiques et intonations du dieu
       Krishna, qui est comme un « thérapeute » dans la religion.
     * Or, les experts alertent sur les dangers de ces IA qui si elles se
       retrouvent entre de mauvaises mains peuvent s’avérer
       compromettantes par des comportements misogynes par exemple, ou en
       appelant à tuer quelqu’un si cette personne si cela suit les
       principes de la religion.
     * Depuis qu’OpenAi à partagé son API en novembre 2022, les IA se sont
       démultipliées. Ce partage de l’API et le fait que la religion soit
       le plus grand business en Inde, il est logique que des IA
       religieuses ont vu le jour.
     * Un des grands dangers dans ces IA est une confiance aveugle
       accordée à ces algorithmes et ainsi, une incapacité à déceler les
       discours discriminatoires.
     * On a aussi découvert que ces IA soutiennent fortement ou critiquent
       certains partis indiens. GitaGPT devient alors politique.
     * Les développeurs de ces IA souhaitent rappeler aux utilisateurs
       qu’il ne faut pas trop croireen ces IA : il est essentiel qu’ils
       fassent appel à leurs esprits critiques. Cela est mentionné par
       exemple bas de page du site de Bible GPT.
     * Or, des dérives peuvent ou ont eu lieu comme avec un IA basé sur le
       Coran qui a donné comme conseil de tuer tous les polythéistes.
       Cette IA a été mise en pause depuis.
     * La responsabilité individuelle est le grand enjeu de ces IA.
         ______________________________________________________________

   Cet article souligne l’émergence de nombreuses IA “religieuses” en
   Inde, basées sur le Bhagavad Gita, qui sont de plus en plus utilisées
   par des millions de personnes. Les experts mettent en garde contre les
   dangers potentiels de ces IA, notamment lorsqu’elles peuvent adopter
   des comportements misogynes ou inciter à la violence envers ceux qui ne
   suivent pas les principes religieux. Ainsi„ les individus peuvent
   accorder une confiance excessive à ces systèmes et être exposés à des
   discours discriminatoires. En fin de compte, la responsabilité
   individuelle est un enjeu majeur dans l’utilisation de ces IA
   religieuses.
     __________________________________________________________________

   Naomi Klein, « AI machines aren’t ‘hallucinating’. But their makers
   are? », The Guardian, 8 mai 2023
   (BUTTON) Idées clé

     * Le terme d’hallucination est fréquemment utilisé pour définir les
       images « fausses » générées par les IA (faits inventés, comme
       l’image du pape François portant en doudoune blanche Balenciaga),
       qui sont bluffantes de par leur ton convaincant.
     * Pourquoi parler d’hallucination ? Ce terme se réfère aux phénomènes
       étranges et immatériels que le cerveau humain perçoit. Utiliser ce
       terme pour les IA revient donc à remettre de l’humain dans ces
       machines.
     * Or les véritables hallucinations sont les mystifications et
       idéalisations de ces technologies (sauveuse de l’humanité, du
       climat, machine sociale, etc.).
     * Les systèmes actuels sont avant tout basés sur le profit : il y a
       un risque que l’IA deviennent un outil de dépossession et de
       spoliation supplémentaire.
     * Il y a un danger dans l’accaparation des connaissances humaines par
       des grandes entreprises privées, qui numérisent à grande échelle
       des informations. Tout cela devrait être illégal car les règles de
       consentement et de droits d’auteur ne sont pas respectées.
     * L’artiste Molly Crabapple dirige un mouvement d’artistes pour
       contester le vol et le non respect des droits d’auteurs. Pour se
       défendre, ces entreprises énoncent des arguments tels qu’une
       soi-disant impossibilité d’appliquer des règles pour cette nouvelle
       technologie, ou un frein à la liberté par un surplus de
       réglementations. Ce même schéma (de non-respect de la vie privée et
       l’inaction des décideurs face à ces entreprises) se retrouve dans
       la colonisation de l’espace par Elon Musk, dans l’arrivée de Uber
       dans l’industrie du taxi, avec Airbnb sur le marché locatif, ou
       encore avec Google Street View : la permission n’est jamais
       demandée.
     * Il est de même avec toutes nos saisies et données personnelles qui
       sont utilisées pour entraîner des IA sans notre consentement – d’où
       le fait que l’on doit être attentif aux systèmes hallucinatoires de
       ces algorithmes. Naomi Klein en expose 4 types : le premier est de
       croire que l’IA pourrait résoudre la crise climatique en aidant à
       analyser les émissions de C02. Cela démontre un déficit
       d’intelligence et une peur de perdre de l’argent en changeant de
       modèle industriel. Nous nous appuyons sur les réponses des
       machines, beaucoup moins coûteuses et ambitieuses, pour résoudre
       des enjeux que nous sommes incapables de surmonter. L’IA
       aggraverait sûrement la crise climatique à cause des serveurs et de
       leurs émissions de carbone. Enfin, L’IA sera probablement utilisée
       par de plus en plus d’entreprises générer des micropublicités
       ciblées.
     * Une crise de la confiance s’intensifie de par une méfiance
       systématique envers l’environnement médiatique, empêchant de penser
       et d’agir collectivement sur ces technologies.
     * La seconde hallucination est de croire que l’IA peut fournir une
       gouvernance avisée. Les décideurs pensent s’appuyer sur des
       preuves. Or, ils se tromperaient et mettraient en danger la place
       des individus dans les sociétés (licenciement par exemple). Une
       campagne de lobbying a été mené contre les deux partis de
       Washington afin de les alerter sur le risque d’être
       laissé-pour-compte par la Chine à venir dans le secteur des IA,
       dans le cas où les politiciens ne ne prennent pas de décisions
       quant à leurs réglementations. Sam Altman, patron d’OpenAI et
       créateur de ChatGPT, vend l’IA comme l’outil pour permettre les
       meilleures prises de décisions des politiques et des industries.
     * La troisième hallucination est celle de croire que les dirigeants
       de ces entreprises ne pourront pas démolir le monde. Or de nombreux
       licenciements, par exemple des travailleurs de Google ayant dénoncé
       des comportements racistes et sexistes, démontrent que les prises
       de décision sont avant tout inégalitaires et suivent les valeurs
       des classes dominantes.
     * Enfin, la dernière hallucination est celle de la libération des
       corvées. Au contrainte, ces IA participent d’un schéma marketing.
       Les individus deviennent accro à ces outils gratuits, ce qui
       élimine la concurrence et engendre l’introduction de publicités
       ciblées, une surveillance constante, et la vente d’abonnements pour
       accéder à toujours plus de paramètres..
     * L’artiste Molly Crabapple appelle donc les éditeurs, artistes et
       journalistes à soutenir les actions n’utilisant par les IA. Elle
       déclare que « L’art de l’IA générative est vampirique, se régalant
       des générations passées et aspirant le sang des artistes
       vivants. ». Nous pouvons nous opposer à l’implémentation de ces IA
       en développant des outils réglementaires, comme le faisait Timnit
       Gebru, licenciée par Google en 2020 pour avoir dénoncé la
       discrimination au travail. Il faut développer des machines qui
       fonctionnent pour nous, et non l’inverse. La FTC (Federal Trade
       Commission des États-Unis a par exemple forcé Cambridge Analytica
       et Everalbum à détruire des algorithmes qui se sont formés sur des
       données non libres de droits.
         ______________________________________________________________

   L’article explore les utilisations trompeuses et potentiellement
   dangereuses de l’IA. Naomi Klein souligne que le terme d’«
   hallucination » est souvent utilisé pour décrire les « fausses »
   créations des IA, qui peuvent être convaincantes. Cependant, les
   véritables hallucinations résident dans les mystifications et
   idéalisations de ces technologies. Les systèmes IA actuels qui sont
   motivés par le profit, risquent de devenir des outils d’expropriation
   et de spoliation. L’enjeu capital réside dans l’appropriation par les
   grandes entreprises des connaissances humaines, ce qui devrait être
   illégal étant donné le non-respect évident des règles de consentement
   et de droits d’auteur. Des militants comme l’artiste Molly Crabapple
   s’engagent pour défendre les droits d’auteur et lutter contre
   l’invasion de la vie privée. Il faut remettre en question notre
   confiance envers ces technologies, qui ne font que prolonger des
   schémas capitalistes.
     __________________________________________________________________

   Benjamin Tainturier, « IA : comprenons ce qui nous arrive plutôt que de
   le juger d’avance », entretien avec Grégory Chatonksy, AOC, 6 mai 2023
   (BUTTON) Idées clé

     * Grégory Chatonsky développe le concept d’« espace latent » pour
       parler des espaces vectoriels abstraits composés d’informations
       (images, textes, etc.) qui servent à entraîner les réseaux de
       neurones. Au sein de l’abondance des données, il est possible de
       trouver de découvrir des brèches.
     * Grégory Chatonsky met en avant l’ambivalence et l’ambiguïté des
       pétitions demandant à faire une pause dans les IA, qui sont lancées
       par les acteurs-mêmes des IA. Il relie cela au concept d’«
       enthousiasme conjuratoire », conceptualisé par Jacques Derrida, qui
       définit des éléments que l’on veut conjurer, mais qui restent
       paradoxalement fantasmés.
     * Le problème des moratoires est de valoriser le réfléchir avant
       l’agir. Or cela semble antinomique : l’IA a déjà été mise en place.
       Chatonsky valorise l’expérimentation (dans son cas, artistique)
       pour penser en agissant, et inversement.
     * Le monde des IA, que Grégory Chatonksy préfère appeler « induction
       statistique », permet de dépasser les rapports sociaux : ChatGPT a
       permis d’animer des débats sociologiques. Or les individus ne
       prennent pas assez le temps de se documenter et de comprendre les
       notions clé.
     * Il faut penser la technique de manière contre-intuitive,
       c’est-à-dire en la déconstruisant comme l’on fait Leroi-Gourhan,
       Simondon, Stiegler, Derrida ou Lyotard. En somme, il ne faut pas la
       penser qu’avec des mots qu’on ne comprend pas entièrement.
     * Grégory Chatonsky a mobilisé l’espace latent par un travail
       d’images d’archives pour revisiter l’histoire industrielle du
       Havre. Cela s’apparente à une forme de balade grâce aux prompts :
       « On fait rentrer un espace contrefactuel dans la réalité en
       s’intéressant à ce qui n’existe pas, en s’y aliénant, ce qui mène à
       l’émancipation de ce qui existe ».
     * Importance de décoloniser les techniques et d’ouvrir des champs
       d’explorations.
     *
     * La conversation et l’aspect prévisible de par ChatGPT ont permis
       cet engouement.
     * ChatGPT, en nous questionnant sur l’attribution de l’intelligence,
       et en nous assurant que nous le sommes, nous rend à l’inverse plus
       ignorants. Travailler et insister sur la bêtise de la machine
       (« technosadisme ») pour nous assurer de notre intelligence est
       déraisonné : il vaudrait mieux considérer la technique et ne pas
       l’opposer radicalement aux humains car elle nous fait autant que
       nous la faisons. Le test de Turing se met dans cette posture en
       refusant de qualifier ce qui est humain ou non.
     * L’IA s’appuie sur la culture humaine pour générer des sorties en
       incorporant des différences et des répétitions (elles ne fait donc
       pas que copier). « Ce n’est pas avec la machine que nous discutons,
       c’est avec un espace latent qui est constitué de cultures humaines
       métamorphosées. Par cet intermédiaire, le passé peut se poursuivre
       dans le présent et dans le futur. »
     * Le défi pour de futurs artistes est de travailler avec la
       ressemblance des sorties d’IA, purement statistiques, et dont on
       peut anticiper les données.
     * Grégory Chatonksy critique l’attribution superficielle d’un style à
       des sorties (exemple, une image IA dans le style de Dali, Van Gogh,
       etc.). Explorer l’espace latent c’est aller à l’inverse de cette
       quête de « transfert de style » visant à adapter la technique à ce
       que nous voulons. Cette même dynamique se retrouve dans notre
       rapport avec l’environnement : c’est ce qui a, par conséquence,
       engendré le réchauffement climatique.
     * Grégory Chatonsky s’oppose radicalement à l’interdiction des IA
       dans l’enseignement supérieur : l’espace latent doit être
       appréhendé, manipulé et expérimenté. Agir avec la technique permet
       de dépasser les utilisations primaires : « Je propose, pour
       synthétiser tout cela, d’aliéner et de s’aliéner à l’IA : en
       apprenant quelque chose à une IA, on la change ; et en faisant
       l’expérience on se change aussi. »
     * La production hybride que permet les IA serait importante dans
       l’enseignement supérieur pour développer des politiques d’auteurs
       d’IA. Il est donc nécessaire de former des personnes capables de
       travailler et de découvrir cet espace latent de façon singulière.
         ______________________________________________________________

   L’article présente une interview de Grégory Chatonsky, artiste et
   théoricien travaillant avec l’IA. Selon lui, il est nécessaire d’agir
   en même temps que nous pensons l’IA et vice versa, afin d’appréhender
   l’espace latent (l’espace de l’IA) de manière authentique et de
   découvrir ses potentialités.
     __________________________________________________________________

   Sal Khan, « The Amazing AI Super Tutor for Students and Teachers »,
   TED, 2 mai 2023
   (BUTTON) Idées clé

     * Benjamin Bloom évoque en 1984 « les 2 sigmas problèmes » pour
       parler des méthodes permettant que l’enseignement en groupe soit
       aussi efficace que l’individuel.
     * Exemple de l’IA Khanmigo : site wWeb avec des bots pour aider à
       progresser en mathématiques. Les conversations entre l’étudiant et
       le bot sont visibles et enregistrées, par l’enseignant et sont
       modérées par une autre IA. L’IA ne donne pas la réponse directement
       mais aide l’élève à comprendre comment avancer. L’IA comprend le
       contexte de la question de l’étudiant pour le guider rapidement.
     * L’IA agit « socratiquement » : elle pose des questions aux
       questions des élèves pour qu’ils puissent progresser par eux-mêmes.
     * Khanmigo pourrait servir de tuteur, de conseiller d’orientation, de
       coach et peut aussi jouer un personnage : exemple d’une étudiante
       basée en Inde qui a fait un travail sur Gatsby et a demandé aux
       bots de l’incarner aider à comprendre ce personnage.
     * Outil collaboratif : l’IA peut écrire avec vous une histoire, vous
       faire avancer dans votre rédaction en y apportant des éléments et
       idées mais peut aussi vous aider àcomprendre des textes et extraits
       en soulignant certains passages par exemple.
     * L’IA comme un tuteur : Khanmigo ne donne pas la réponse. Il peut
       aussi aider les enseignants à préparer des cours en les guidant sur
       les parties et sous parties à concevoir.
     * Khanmigo est à l’origine, une idée d’un programmeur d’OpenAI pour
       créer un bot qui ne donne pas les solutions mais qui donne des clés
       pour progresser.
     * Nous participons tous à cette décision d’implémentation de l’IA :
       arrêter le développement des IA est dangereux car les « mauvais
       acteurs » (gangs, gouvernements totalitaires, etc.) eux, ne
       s’arrêteront pas de les développer.
     * Il faut instaurer des garde-fous pour s’assurer que les IA agissent
       pour le développement des humains et non pour leur aliénation.
         ______________________________________________________________

   L’éducateur américain, Sal Khan, fait une démonstration de son IA
   Khanmigo, appliquée à l’enseignement. Ce bot est une alternative et une
   réponse aux services populaires (comme ChatGPT) qu’il définit comme
   « anti-socratique » car créant des interactions passives.
     __________________________________________________________________

   Olivier Ertzscheid, « En discutant avec ChatGPT, on discute avec
   l’humanité entière » [interview parue dans Le Libé des écrivain·es le
   21 avril 2023], entretien avec Lucie Rico, Affordance, 1^er mai 2023
   (BUTTON) Idées clé

     * ChatGPT déstabilise notre compréhension des interactions
       langagières. Cela fait écho à la « vallée de l’étrange » théorisé
       par le roboticien Masahiro Mori en 1970 pour parler de l’inconfort
       et de la déstabilisation de nos repères face à des robots trop
       « humains ».
     * Le dialogue avec l’IA restreint la possibilité des réponses, à
       l’inverse des recherches qui font intervenir plusieurs mots-clés et
       une multitude de résultats.
     * La question de la projection avec ces IA est importante : l’humain
       ne parle plus seulement à d’autres humains mais à des objets
       inanimés, sur lesquels il projette une forme d’humanité.
     * ChatGPT se situe entre un statut affectif et énonciatif: on sait
       que ce ne sont que des technologies limitées mais on leur attribue
       paradoxalement une aura. Il faut être attentif aux « habits » que
       « l’unanimisme capitalistique » leur donne, qui par exemple fait
       croire que l’IA peut devenir notre enseignante.
     * Discuter avec ChatGPT revient à discuter avec une multitude
       d’humains car l’IA s’alimente de tout : que ce soit des forums
       Reddit, poètes, auteurs des siècles passés, modérateurs, ou
       nous-même (car nous alimentons aussi ces IIA en interagissant
       avec).
     * Il y a un danger dans ces interactions fermées. Par exemple, les
       témoignages ou expertises ne sont plus accessibles et ces services
       ne laissent pas de place aux débats et à la diversité des opinions.
     * Il y a une « fausse maïeutique » qui nous oblige à reformuler des
       notions. Lorsqu’on lui pose des questions, l’IA ne se sert que de
       sa base et ne peut pas intégrer une diversité d’approches. Les
       réponses vont donc souvent dans la même direction.
         ______________________________________________________________

   Olivier Ertzscheid analyse les perturbations engendrées par nos
   interactions avec ChatGPT. Discuter avec ce service revient à projeter
   une forme d’aura dans un artefact inanimé et limité. Or cette
   mystification d’une apparence humaine est dangereuse car il ne faut pas
   oublier que ces technologies reformulent nos questions pour que les
   réponses puissent aller dans leur direction, celle des limites de la
   base de données. Cela aseptise donc les possibilités.
     __________________________________________________________________

  Avril 2023
     __________________________________________________________________

   Abeba Birhane, Atoosa Kasirzadeh, David Leslie, Sandra Watcher, «
   Science in the age of large language models », Nature reviews physics,
   26 avril 2023
   (BUTTON) Idées clé

     * Discussion de quatre experts en éthique de l’IA à propos des
       risques et capacités des LLM (Large Language Models) dans les
       sciences.
     * En quelques mois, les LLM ont su attirer l’attention et ce malgré
       les conséquences négatives de ces systèmes sur les humains. Les
       discussions sur la responsabilité et l’exploitation du travail ne
       sont pas encore suffisamment lancées.
     * Il faut faire attention et analyser les moments où l’IA énonce des
       vérités non vérifiées ou des mensonges.
     * Il faudrait évaluer périodiquement les risques et avantages des
       technologies pour prévoir et prévenir au mieux les conséquences,
       par exemple sur l’environnement car ces technologies ont une
       empreinte carbone importante (ne pas laisser les IA exacerber la
       crise climatique).
     * Les scientifiques doivent gérer les attentes concernant les
       contributions des LLM dans les sciences.
     * Les IA se basent sur de la prédiction : elles n’ont pas de
       fonctionnements communicatifs incarnés et du relationnel. Elles ne
       vivent pas dans le vécu du réel, dans lequel les humains partagent
       des expériences sensibles par le langage.
     * Il manque aux IA l’intersubjectivité, la sémantique et l’ontologie,
       qui sont indispensables pour théoriser, comprendre, innover et
       découvrir.
     * Les IA ne peuvent pas capturer les nuances, et donc les valeurs
       implicites des écrits scientifiques : elle peuvent générer des
       réponses semblant vraies mais contenant des contenus inexistants
       (il faut toujours revérifier). Enfin, elles perturbent la confiance
       entre pairs dès lors qu’elles rédigent des écrits (on peut se
       questionner sur la véracité et la pertinence des textes).
     * La science est une entreprise humaine avant tout et ces LLM ne sont
       que des outils. La science naît dans le social, l’historique ou le
       culturel et est motivée par des valeurs, des intérêts et des
       objectifs : l’IA ne fait rien de tout cela.
     * Il faut utiliser ces IA comme des tremplins pour la réflexion
       scientifique : elles peuvent jouer un rôle constitutif en guidant
       une poursuite de l’expansion de la compréhension scientifique.
     * Ces technologies doivent être considérées comme des interprètes
       dépendant de la théorie. Il faudrait demander davantage de
       recherche et de développement de mesures de la part des communautés
       scientifiques pour encadrer et utiliser ces IA de manière
       responsable. Il est crucial que les scientifiques suivent
       l’élaboration et l’utilisation de ces technologies pour éviter une
       décrédibilisation puissante de la recherche.
         ______________________________________________________________

   L’article est une discussion entre quatre experts de l’éthique des
   intelligences artificielles, demandant une véritable prise en
   considération par les sciences de leurs enjeux et conséquences.
   L’évolution fulgurante de ces technologies nécessite des évaluations
   périodiques pour analyser leurs conséquences mais aussi leurs limites.
   Il est indispensable de ne pas laisser les IA décrédibiliser les
   recherches scientifiques, car elles manquent d’intersubjectivité, de
   sémantique et d’ontologie – autant de compétences nécessaires pour
   théoriser, comprendre, innover et découvrir. Aussi, leur incapacité à
   comprendre des nuances, à générer seulement du vraisemblable (voire de
   faux contenus) et à perturber la confiance entre les humains
   compromettrait le domaine des sciences.
     __________________________________________________________________

   Antonio Casilli, « Malaise dans l’éthique de l’IA. Conflictualités
   sociales et environnementales autour de l’automation intelligente »,
   conférence à l’université de Genève en compagnie de Nicolas Nova et de
   Claire Balleys, 19 avril 2023
   (BUTTON) Idées clé

     * Travail de cartographie à faire pour savoir où les IA sont
       développées afin de percer leur complexité.
     * Qui est autorisé à parler sur l’éthique des IA et sur sa
       définition ? Qu’est-ce que l’éthique à l’ère des IA ? De laquelle
       veut-on parler ? À quel moment commence-t-on à s’inquiéter ?
     * Il faut arrêter avec le mythe de l’intelligence générale
       artificielle qui dépasserait les performances humaines dans tous
       les domaines. Il y a une colonisation sémantique du terme d’IAqui
       est utilisé pour tout et rien.
     * La recherche sur l’IA est récente : on observe à partir de
       2015-2016 une surproduction de chartes et de lois (67 selon l’ONG
       allemande Algoritmwatch). Il faut observer qui est à l’origine de
       ces textes, en général de grandes entreprises privées.
     * « Pause Giant » IA (2023) rappelle une lettre ouverte de 2016 sur
       la même plateforme et par les mêmes entreprises (« Autonomous
       Weapons Open Letter: AI & Robotics Researchers »). Les GAFAM sont
       la majorité des auteurs de ces articles.
     * Aristote définissait l’éthique comme intrinsèquement liée à la
       politique car elle sert à s’accomplir dans la vie politique. Qu’en
       est-il des IA ?
     * Une éthique est possible si l’on se débarrasse de l’étiquette d’«
       artificiel » car l’IA est peut-être intelligente mais pas
       artificielle : elle est régie par des humains. Mettre l’accent sur
       l’éthique pour encadrer les questions de pouvoir, de domination,
       d’inégalité et d’oppression.
     * L’éthique de l’IA n’est pas à penser sans les luttes pour
       l’environnement. Ces technologies s’appuient sur des équipements
       transformant les matières premières (cobalt, nickel, lithium,
       etc.). Il y a besoin de mettre en place des infrastructures
       globales d’extraction de minerais pour le numérique. Cette
       production de ressources change les paysages de pays comme la
       Bolivie, où l’on trouve le plus grand gisement de lithium à ciel
       ouvert.
     * Codes éthiques de l’IA en 5 points : la transparence, la justice et
       l’équité, la non malfaisance (l’IA ne devraient pas produire des
       dégâts envers la population, or les débats ne sont vifs que quand
       les IA sont intégrés dans le domaine de l’armement, par exemple),
       la responsabilité (exemple du MIT sur les voitures autonomes et
       l’éthique : quelle décision la voiture doit-elle prendre
       lorsqu’elle est obligée de sacrifier des vies ? Qui tuer ? Question
       de « liability » : qui est responsable ?) et la protection de la
       confidentialité.
     * « The Steep Cost of Captur » de Whittaker, 202 est une forme de
       « J’accuse » contre les géants de la tech qui neutralisent la
       critique en dénigrant les approches dissidentes et en finançant les
       critiques les plus faibles.
     * Il faudrait se concentrer sur la phase de production de l’IA plus
       que sur la phase d’utilisation.
     * Cas des prolétaires dans les pays émergents (exemple de Madagascar
       dans lequel s’est rendu Antonio Casilli) qui travaillent dans des
       conditions insalubres : ce ne sont pas des IA mais des gens qui se
       font passer pour des IA. Les grandes entreprises continuent de
       faire croire qu’elles entraînent des IA sans humains derrière.
     * La Russie achète des données brutes de micro-travailleurs : on
       trouve de nombreuses « fermes à recaptcha » en Russie.
     * Depuis 2022, des annonces de tâches d’entraînement d’IA par la
       Russie ont été signalées (invasion russe) : une application
       demandait aux ukrainiens de documenter les positionnements
       militaires en cours, mais le pays à l’origine de cette application
       reste inconnu (Russie ? Ukraine ? États-Unis ?)
     * Le Venezuela est le pays central dans les IA : il est le plus
       représenté dans toutes les bases de données de micro-travail.
         ______________________________________________________________

   Antonio Casilli propose de réfléchir la notion d’éthique au regard de
   l’IA. Continuer d’idéaliser l’IA est dangereux car cela fait perdurer
   le mythe irrationnel de dépassement des capacités humaines. Il est
   nécessaire de penser et de cadrer l’éthique à l’ère de l’IA en se
   basant sur cinq points : la transparence, la justice, l’équité, la non
   malfaisance, la responsabilité, et la protection de la confidentialité.
     __________________________________________________________________

   Gary Marcus et Sasha Luccioni, « Stop Treating AI Models Like People »,
   The Road to AI We Can Trust, 18 avril 2023
   (BUTTON) Idées clé

     * Notion de « surattribuation » pour parler de l’anthropomorphisation
       des IA par les humains.
     * Rappeler que ce ne sont que de purs systèmes, des modèles qui
       émulent la structure statistique du langage qui calculent les
       probabilités des séquences de mots, sans aucune compréhension
       humaine de ce qu’ils disent.
     * L’anthropomorphisation par les humains est dangereuse et est à
       prendre en compte.
     * Prolongation de « l’effet ELIZA » : un des premiers robots
       conversationnels (1960) qui faisait croire aux utilisateurs qu’ils
       interagissaient réellement avec un humain. On a le même problème
       actuellement mais accentué par la puissance de calculs des
       machines.
     * Plus on leur attribue de fausses interactions, plus ces services
       peuvent être exploités pour différents domaines (thérapie, conseils
       financiers, etc.).
     * Il est important de développer des outils capables de détecter les
       contenus générés par les IA et de demander aux politiques de mettre
       en place des règles.
     * Éduquer les gens à cette « surattribution » est vital : garder un
       scepticisme est nécessaire pour ne pas se laisser avoir par la
       désinformation que ces IA peuvent générer.
         ______________________________________________________________

   L’effet de « surattribution », qui consiste à donner des qualités
   humaines aux IA, est dangereux. Cette anthropomorphisation est
   problématique dès lors que les IA sont intégrées dans des domaines
   touchant à l’intégrité des humains (thérapie, finance, etc.) à cause de
   la désinformation dont elles font régulièrement preuve.
     __________________________________________________________________

   Pablo Maillé « De 2010 à 2023, comment Usbek & Rica a chroniqué les
   progrès de l’intelligence artificielle », Usbek & Rica, 16 avril 2023
   (BUTTON) Idées clé

     * Le média Usbek & Rica revient sur ses explorations du futur des IA
       depuis treize ans afin d’analyser l’évolution et l’état actuel de
       ces technologies.
     * Dans les années 2010, le fait que des IA puissent acquérir des
       connaissances en linguistique était déjà envisagé. François
       Denieul, spécialiste d’Internet, déclare que les IA fonctionneront
       comme « une intelligence démultipliée, sur le modèle du
       développement multicellulaire » car « la connexion d’éléments donne
       toujours lieu à quelque chose de nouveau, d’inattendu » : « C’est à
       la fois le principe des algorithmes génétiques et les bases de
       l’intelligence artificielle ».
     * En 2013, le huitième numéro du média s’interroge sur les droits des
       robots et inclut la prédiction du juriste Eric Hilgendorf,
       professeur à l’université de Würzburg, selon qui les machines
       pourront être reconnues coupables par la cour civile. Le numéro
       pose aussi la question du droit d’auteur des créations des IA, ce
       qui arrive justement dix ans plus tard (Cf. « Droit d’auteur :
       trois IA génératrices d’images attaquées en justice »). Or la
       vision des IA à cette époque restait très matérielle (l’IA comme un
       robot humanoïde) : on observe actuellement qu’il est plus question
       d’implémentation de ces algorithmes sous forme purement virtuelle.
     * En 2015, Usbek & Rica traite de la fréquence du développement des
       IA, les rendant au final banales (ce qui fait grandement écho au
       présent). La crainte principale serait plutôt la
       sur-rationalisation des activités humaines par une course à
       l’innovation et à l’optimisation du monde. L’intérêt résiderait
       alors dans le détournement en appelant à l’improvisation, au bug et
       à la résistance aux propriétés privées impactant les droits des
       utilisateurs.
     * Une conférence de Google à Paris sur l’évolution des IA a eu lieu
       en 2017. Le terme d’IA est préféré à celui de robot et l’équipe de
       Google se questionne sur les fonctions que ces IA pourraient
       apporter à l’entreprise.
     * Usbek & Rica, en 2018, traite de l’ouvrage La singularité
       technologique (FYP, 2018) de Murray Shanahan, spécialiste en
       robotique cognitive. Il émet l’hypothèse que les IA pourront avoir
       un « désir potentiel d’auto préservation » et donc sacrifier les
       humains au détriment des robots. Il faudrait alors inculquer aux
       robots une empathie envers les humains.
     * En 2019, l’ingénieur Stuart Russel propose que les IA soient
       développées pour remplir les objectifs humains et non leurs propres
       objectifs et pour éviter que l’humanité perde le contrôle de ces
       technologies. La notion d’incertitude est à inclure dans ces
       technologies afin que leurs actions soient constamment
       re-questionnées.
     * Qu’en est-il en 2023 ? Selon Usbek & Rica, l’IA actuelle est plus
       de l’ordre de l’automatisation des tâches, dont celles créatives,
       scindant le débat entre ceux inquiets de voir leurs métiers
       disparaître et ceux s’enthousiasmant de co-créer avec ces
       algorithmes. L’improvisation, suggérée déjà en 2015, reste alors la
       perspective à privilégier.
         ______________________________________________________________

   Usbek & Rica est un média assez jeune (13 ans) qui a suivi de près
   l’évolution de l’IA au cours de la dernière décennie en mettant en
   avant les enjeux éthiques, les avancées scientifiques, les limites et
   les perspectives de ces technologies.
     __________________________________________________________________

   Emily M.Bender, « ChatGPT Is Not Intelligent », Tech won’t save us,
   n^o 163, 13 avril 2023
   (BUTTON) Idées clé

     * Les IA sont des « perroquets stochastiques » qui ne répondent que
       par des calculs de probabilités. Le podcast est une discussion
       autour de la transformation du langage par les IA.
     * On observe une division entre ingénieurs et linguistes : le machine
       learning contre la linguistique.
     * En 2017, le principe du réseau neuronale artificiel a connu un
       boom. Cette vision du machine learning fonctionnant comme des
       neurones est fausse mais reste ancrée dans les pensées
       scientifiques.
     * Les langues sont des systèmes symboliques avec des significations.
       L’utilisation de la langue est plus qu’un simple emploi de mots :
       ces mots s’ancrent dans des contextes sociaux et affectent les
       comportements. Or les IA n’utilisent que la partie formelle des
       textes.
     * Ce qui se passe actuellement est une intégration des IA textuelles
       dans n’importe quels domaines, et ceci est dangereux. Nous n’avons
       pas encore des robots juges, par exemple, mais cela pourrait
       arriver si on les laisse s’introduire de manière ubiquitaire et
       sans règles. Aussi, le langage est ce qu’on utilise quotidiennement
       et est ce qui nous connecte aux autres : il est donc nécessaire que
       les entreprises contrôlent ce que leurs IA disent.
     * Les questions de qui peut avoir accès à Internet et de qui dirigent
       ces systèmes de machine learning sont des problématiques
       fondamentales pour comprendre comment les valeurs des classes
       sociales dominantes se retrouvent dans le numérique.
     * Protéger et enseigner aux prochaines générations comment utiliser
       ces IA est vital afin de sensibiliser au plus vite à leur
       fonctionnement. Ces technologies continueront d’évoluer et d’être
       perfectionnées sans cesse : il faut donc être préparé en amont.
     * Si on peut désigner les IA comme des « perroquets stochastiques »,
       alors en va-t-il de même pour les individus qui les utilisent ? Il
       faudrait donc repenser notre humanité pour éviter de tomber dans
       les mêmes modes de communication déshumanisés dont font preuve ces
       machines.
         ______________________________________________________________

   Emily M. Bender, linguiste américaine et spécialisée dans la
   linguistique computationnelle, analyse dans ce texte l’effet de
   « perroquet stochastique » des IA. Elle propose de se questionner la
   transformation de la langue par ces technologies et leurs conséquences
   sur les comportements humains.

   Antonio Casilli, « La menace d’un grand remplacement par les robots est
   une manière d’assurer la discipline », entretien avec Baptise Laheurte,
   LVSL, 12 avril 2023
   (BUTTON) Idées clé

     * Entretien avec le sociologue Antonio Casilli, sur la question de la
       précarisation du travail humain par les IA, qu’ils soulèvent
       notamment dans son livre En attendant les robots. Enquête sur le
       travail du clic.
     * La peur de l’automatisation est toujours ambiguë : on l’attend
       autant qu’on l’appréhende et on se rend compte qu’elle finit
       toujours par être retardée.
     * Menace du remplacement par les robots permet de contrôler les
       comportements en machinisant les individus, par la dévalorisation
       de leur travail.
     * Or l’innovation n’est pas ce qui dévalorise ce travail. Il
       semblerait que ces innovations modifient les métiers plutôt que de
       les détruire, en bouleversant les cadres établis par des grandes
       institutions comme l’OIT (Organisation internationale du Travail).
     * Analyse des métiers invisibles liés à l’automatisation comme ceux
       liés à gestion des IA (préparation, vérification et limitation) et
       questionnement sur le fait que nous travaillons gratuitement pour
       améliorer ces technologies en se servant des moteurs de recherche,
       en interagissant avec les promtps des IA (ChatGPT etc). Continuité
       alors entre les personnes non rémunérées pour réaliser du clic,
       ceux qui améliorent les IA et les travailleurs des pays émergents
       très peu payés.
     * Trois formes de « travail digital » : « travail des petites mains
       du numérique, c’est-à-dire le travail à la demande (prestation de
       petites interventions comme Uber, Deliveroo, etc.) ; le
       micro-travail qui fournit le soutien aux algorithmes par des tâches
       standardisées de mise en relation de données et le travail social
       en réseau qui est la participation des usagers à la production de
       valeur. ». Ces différentes catégories témoignent d’une
       transformation des modes de travail encore plus fractionnés et
       décentralisés qu’auparavant.
     * Travail « datafié » (produit et est produit par des données) et
       « tâcheronisé »(segmenté en petites tâches). Le clic est la
       réduction minimale du travail des humains, standardisant et rendant
       ces individus interchangeables.
     * Micro-marchés créés ad hoc par les algorithmes et différence avec
       le taylorisme par le fait que les individus n’ont pas de salaire
       stable pour consommer la production de leur propre entreprise. Ce
       ne sont pas les plateformes numériques qui nous donnent les
       ressources pour commander sur leur plateforme. Les travailleurs du
       digital sont alors moins protégés et incertains de l’avenir de leur
       métier et de leur salaire (« digital labor »).
     * Notion de «nouveau prolétariat» et de « freelancing extrême » par
       l’individualisation des individus devant leur écran et la
       micro-fragmentation des métiers dont les contrats horaires et les
       rémunérations sont mal encadrés.
     * Rémunérations à la baisse donc augmentation du nombre de
       micro-travailleurs dans les pays émergents (par exemple : « fermes
       à clics » pour générer de faux followers ou des vues).
     * Milieu du numérique qui est décentralisé et récent complique
       l’encadrement des métiers. l’économie des plateformes se fonde
       alors sur la diminution du travail payé et l’augmentation de celui
       non payé.
     * ChatGPT est un système « question answering » idéalisé, dont
       l’engouement autour a incité un bon nombre de personnes monde a
       l’essayer et à continuer de l’utiliser, ce qui participe donc à son
       amélioration.
     * Micro travailleurs kenyans, payés 1$ de l’heure depuis des années
       par OpenAI pour gérer les réponses et les contenus.
     * Travail formel permet des relations humaines grâce à un cadre qui
       situe l’individu dans des relations, aspect que l’on ne retrouve
       pas dans les travails du clic.
     * Il ne faut pas penser que l’IA explose mais elle semble plus ramer
       et connaître de nombreux échecs (échecs des blockchain par
       exemple).
     * l’IA ne fera pas disparaître tous les métiers (rôle essentiel des
       modérateurs par exemple).
         ______________________________________________________________

   L’analyse par Antonio Casilli du marché du travail des IA met en
   lumière les problématiques liées à l’automatisation du travail humain.
   Son étude des métiers invisibles, derrière les IA, rend compte des
   systèmes de précarisation engendrés par la décentralisation, la
   fragmentation et la dévalorisation du travail.
     __________________________________________________________________

   Entretien avec Asma Mhalla et Jean-Gabriel Ganascia par Nicolas
   Demorand et Léa Salamé, « L’IA est déjà omniprésente partout, et le
   monde ne s’est pas effondré », franceinter, 12 avril 2023
   (BUTTON) Idées clé

     * Révolution des IA a déjà eu lieu et fait muter le monde : elle ne
       l’effondre pas. ChatGPT est un prémisse d’une révolution à venir
       car ces IA sont encore étroites elles fonctionnent sur des tâches
       spécifiques. Cependant, c’est le projet politique des personnes de
       la Silicon Valley, désirant faire des AGI (Artificial General
       Intelligence), qui questionne.
     * Il y a toujours eu de nouveautés avec ces IA : à l’heure actuelle,
       la nouveauté est l’accessibilité. Tout le monde peut se rendre
       compte de la puissance de ces outils.
     * Génie d’OpenAI et d’avoir repris ces générateurs de textes et de
       les utiliser dans un chatbot, qui est lui-même entraîné par
       apprentissage en profondeur.
     * La question des usages est primordiale : OpenAI en quelques jours a
       engendré des millions d’utilisateurs, et a donc dépolitisé  la
       question de la technologie en créant de l’accoutumance et une
       insertion de cet outil dans notre quotidien rapidement.
     * Le moratoire (Pause IA) est problématique car ceux qui le racontent
       sont Elon Musk et des organismes qui sont dans l’idéologie du
       « long-termise », doctrine formulée à Oxford qui explique qu’il
       faut s’assurer de la pérennisation de l’humanité dans les siècles à
       venir. On retrouve alors cette idée dans ce moratoire qui présente
       l’IA comme dangereuse pour l’humanité. Or, au contraire, des
       experts comme Sam Altman, fondateur d’OpenAI, préconisent le
       développement de ces IA pour des histoires de puissances.
     * IA et la politique intérieur aux États-Unis créent actuellement une
       sur-poralisation de la question woke avec une partie de l’élite
       technologique de la Silicon Valley qui est en train de développer
       un combat « anti-woke ». Selon Elon Musk, les filtres éthiques
       créées pour ChatGPT véhiculerait une idée woke donc ils développent
       des contres-réponses dites « maximalistes  » pour répondre à ça
       (comme avec Twitter).
     * Le problème du moratoire est dans la perspective : la lettre
       s’inquiète du futur et donc masque les enjeux actuels et réels.
     * Vu que les IA développées vont potentiellement être concurrente des
       Hommes, il faut donc augmenter l’Homme. Transhumanisme intervient
       dans cette question de l’augmentation de l’Homme à tout prix
       (vieillissement, santé etc).Ces idéologies prônent ainsi la liberté
       individuelle et s’oppose donc aux démocraties car selon eux, la
       démocratie est la tyrannie de la majorité. Ces géants de la tech
       pensent donc le futur : ils ont une vision du futur là où nous nous
       avons un vide idéologique.
     * Le dilemme actuel est l’innovation VS la puissance. Or, le problème
       est que l’IA cristallise la rivalité sino-américaine, d’un point de
       vue de la compétition stratégique. L’État américain semble alors
       schizophrène, en se questionnant sur soit est-ce qu’on régule et on
       bride ou est-ce qu’on lâche, questionnant donc l’hégémonie entre
       ces deux pays et laissant l’Europe à l’écart.
     * ChatGPT nous étonne tous par ses phrases bien construites mais est
       un affabulateur extraordinaire. Il peut y avoir un intérêt comique
       alors à observer.
     * La force de l’humain n’est pas l’émotion et l’intuition mais c’est
       plutôt la raison.
         ______________________________________________________________

   Les entretiens parlent des enjeux politiques et des positionnements des
   États-Unis vis-à-vis des IA, tout en démystifiant la peur irrationnelle
   de ces technologies pour l’humanité. Les IA sont déjà là : il est donc
   nécessaire d’amorcer des débats sur les enjeux actuels de ces IA et non
   pas comme le demandent les signataires de la lettre Pause Giant AI
   Experiments: An Open Letter, de réfléchir à la pérennisation de
   l’humanité avec ces technologies.
     __________________________________________________________________

   Mark Scott, « Timnit Gebru’s anti-‘AI pause’ », entretien avec Timnit
   Gebru, Politico, 11 avril 2023
   (BUTTON) Idées clé

     * Importance dans l’accroissement de la transparence et dans la
       responsabilité des IA pour montrer aux usagers quelles données sont
       utilisées et si ces données sont en accès libre ou non. Cela nous
       montrerait aussi la qualité de ces données et les sources
       utilisées.
     * Danger dans la centralisation des IA par le peu d’entreprises qui
       souhaitent rendre leur technologie multi-tâches. Il faut se
       questionner sur quelles sont les nécessités à avoir des systèmes
       OpenAI appliqués de manière ubiquitaire ?
     * Il faut que le Congrès réglemente les entreprises sur l’usage des
       données et que les médias parlent plus des problèmes éthiques liés
       à ces IA, comme la reproduction des systèmes discriminatoires.
     * Europe a déjà mis en place des législations sur l’IA et la
       confidentialité mais il incombe aux usagers de prouver leur
       préjudice plutôt que de demander aux entreprises de prouver qu’ils
       respectent les exigences.
         ______________________________________________________________

   Timnit Gebru demande aux politiques de mieux réglementer les
   entreprises développant les IA pour éviter la conception de
   technologies polyvalentes. Leurs applications ne doivent pas être
   nécessairement transdisciplinaires : il faut se questionner sur la
   pertinence des applications des IA dans certains contexte.
     __________________________________________________________________

   Anne Alombert, « Avec ChatGPT, nous n’exerçons pas nos capacité
   d’interprétation, de réflexion, de critique et de délibération »,
   entretien avec Matthieu Giroux, Usbek & Rica, 11 avril 2023
   (BUTTON) Idées clé

     * Notion de « schizophrénie numérique » proposée par Anne Alombert
       pour définir deux discours contradictoires : l’un donnant aux
       « dispositifs technologiques des facultés humaines », l’autre,
       critiquant « les effets néfastes des écrans sur les capacités
       d’attention, de mémorisation, de concentration ».
     * « Économie de l’attention » : apparue avec les télévisions pour
       capter les spectateurs et les faire consommer des publicités. Or,
       le numérique est bi-directionnel : les individus sont à la fois
       contributeurs et cibles mais les entreprises privées sont très
       puissantes et utilisent les données des utilisateurs à des fins
       mercantiles.
     * Les « technologies persuasives » du numérique ont rendu l’économie
       de l’attention plus puissante et dangereuse de par leur
       omniprésence. C’est nous qui alimentons ChatGPT lorsqu’on
       l’utilise.
     * Analogie avec Platon (qui critique l’écriture comme dispositif
       diminuant les capacités de mémorisation) et les technologies
       numérique : « Pourquoi s’efforcer de retenir un savoir si celui-ci
       est stocké dans un dispositif matériel ? »
     * Le sophiste (comme figure chez Platon) manipule les esprits par des
       capacités rhétoriques et rédactionnelles, mais ne permet pas
       d’accroître le savoir pour les individus qui reçoivent passivement
       ses informations. Il en est de même avec ChatGPT : on ne fait pas
       appel à nos capacités de réflexion face aux textes générés :
       « Est-ce que ces fonctionnalités automatisent nos capacités, ou
       bien est-ce qu’elles permettent l’exercice de la réflexivité et le
       partage des savoirs ? Est-ce que les technologies standardisent nos
       comportements pour en tirer profit ou bien est-ce qu’elles
       permettent la production de communs ? »
     * Technologie numérique comme « milieu » : penser comment ce milieu
       numérique nous transforme.
     * Les médias sociaux, qui visent « l’exposition de soi et la
       quantification de vues » sont un danger car ils accentuent la
       concurrence, le mal-être des individus par des comparaisons, le
       besoin de reconnaissance des autres. Ils accentuent le narcissisme,
       à l’inverse de plateformes comme Wikipedia ou Bippop, permettant de
       relier des individus autour d’initiatives solidaires et d’échanges
       des savoirs.
         ______________________________________________________________

   Les technologies numériques ont amplifié le phénomène de « l’économie
   de l’attention » et des « technologies persuasives » par des mécanismes
   de collecte de données personnelles. Il y a une analogie faite entre
   les IA et les sophistes, dont les discours bien écrits rendaient
   passifs les lecteurs, en ne permettant pas à ces derniers de
   s’approprier les informations transmises.
     __________________________________________________________________

   Andrea Grimes, « The Unbearable White Maleness of AI », DAME, 11 avril
   2023
   (BUTTON) Idées clé

     * Ère de la « cascade des IA » avec des implications de plus en plus
       immédiates et déconcertantes. Le succès de cette cascade dépend de
       si on considère l’IA comme entité réellement existante.
     * Les hommes blancs semblent plus particulièrement sensibles à la
       vanité de l’IA par le fait que ces puissants logiciels de
       correction automatique émettent des résultats de calculs
       mathématiques à la première personne. Cette tromperie de l’IA crée
       deux acceptions extrêmes, d’une part ceux qui appellent à une pause
       des IA (qui sont par ailleurs, ceux qui ont développés ces
       technologies), de l’autre part ceux qui idolâtrent ces IA et qui
       demandent de ne pas faire de pause.
     * « Colonialisme de l’IA »  : les progrès technologiques sont
       contrôlés pour servir les plus privilégiés, aux dépens de tous les
       autres.
     * Nécessité pour ces classes privilégiées de comprendre les IA sous
       un autre spectre. On peut penser aux experts du décolonialisme de
       l’IA comme Sabelo Mhlambi, Meredith Broussard, ou Stéphanie Dick
       qui ont fondé un groupe de réflexion répondant à “l’étroitesse des
       cadres dominants dans la recherche sur l’éthique des données et de
       l’IA« ou aux théoriciennes féministes de la technologie qui se
       penchent sur des questions autour du consentement et de
       l’informatique.
     * Même problème avec les IA et le «commentariat» (personnes qui
       présentent des analyses et opinions dans les médias de manière
       polémique pour susciter des réactions d’humeurs ) : les sujets
       principaux sont destinés pour des hommes blancs et hétéros
       “ringards« (pornographie, idées complotistes et de droites,
       finances…)
     * Course à l’IA dominée par ces hommes, qui ne font pas preuve
       d’empathie envers les autres. La lettre sur la nécessité de stopper
       le développement des IA, signés par des grands noms du numérique
       est avant tout médiatique et une fois de plus, le reflet de la
       domination de certaines classes sur les autres dans ce domaine.
     * Il est primordial de penser à développer des IA décolonisées pour
       offrir des alternatives aux IA gérées par les classes dominantes
       afin de susciter des débats.
         ______________________________________________________________

   Les IA actuelles dominantes reflètent les dominations de classes
   privilégiés sur les autres et font perdurer le colonialisme dans le
   numérique. Les experts du décolonialisme des IA et du féminisme
   technologique sont discrédités, ce qui réduit fortement la diversité
   d’IA.
     __________________________________________________________________

   Marine Protais, « ChatGPT : quand l’extrême droite agite la peur de la
   machine woke et du « grand remplacement » robotique », L’ADN, 11 avril
   2023
   (BUTTON) Idées clé

     * L’extrême droite s’empare de ChatGPT comme un élément de
       communication politique.
     * L’IA, selon Marion Maréchal Le Pen, creusera encore plus les
       inégalités entre ceux qui savent utiliser les IA et ceux qui ne
       savent pas.
     * Il y a une nécessité de se saisir de ces IA avant que nous soyons
       dépassés (est-ce déjà le cas ?) selon Eric Zemmour.
     * Envisager ChatGPT comme « autre grand remplacement des humains »
       selon Jordan Bardella.
     * L’extrême droite s’est historiquement saisie d’Internet et des
       réseaux sociaux pour communiquer car elle est (ou s’estime) écartée
       des chaînes de télévision publiques et médias grand public.
     * Le discours de l’extrême droite est en plusieurs étapes : vanter la
       prouesse de ChatGPT, s’inquiéter de grands bouleversements, puis
       imposer leurs solutions face à cette technologie.
     * Conscience politique et investissement de l’État dans ces
       technologies, repenser l’éducation.
     * Le sujet des discriminations par les IA n’est pas abordé.
     * Le problème est que seule l’extrême droite s’empare de ce sujet
       pour répondre aux craintes des individus.
     * Il y a un danger car ces politiciens idéalisent l’IA et sa
       puissance : enclencher un discours de la peur (comme les
       entreprises de la tech le font) pour mieux se vendre.
     * Les robots et IA ne peuvent pas remplacer les humains car il y a
       des millions de travailleurs qui participent à leur fonctionnement
       et amélioration.
     * Danger lié à l’idéologie que le progrès ne peut pas être arrêté.
     * « Machine woke » : l’IA est entraînée pour éviter des propos
       racistes ou discriminants donc l’extrême droite se positionne
       contre.
     * IA de gauche contre IA de droite : exemple de Gab aux États-Unis,
       réseau social, proche des idées d’extrême droite catholique
       américaine, dans lequel les utilisateurs veulent créer des IA sans
       limites pour exposer leurs idées.
         ______________________________________________________________

   Les débats autour de l’IA servent d’outils de communication pour
   l’extrême droite française, qui établit une analogie avec sa notion de
   « grand remplacement ». L’IA est utilisée pour jouer sur la peur afin
   de mieux la vendre et faire passer des messages politiques extrémistes.
   Il faut questionner l’instrumentalisation politique de ces technologies
   et leurs influences sur les individus, qui ne doivent pas prendre comme
   vérité ce que l’IA produit (nous n’avons pas toujours connaissance des
   entités qui gèrent ces outils).
     __________________________________________________________________

   Emma, Matt et Sean, « Ep#104 We let ChatGPT write this, The Verge, 7
   avril 2023
   (BUTTON) Idées clé

     * L’IA essaie d’insérer « sa propre voix » lorsqu’on lui demande de
       décrire quelque chose ou de donner un avis. Or on sent bien que
       l’humain n’est pas derrière.
     * GPT-4 extrait des vidéos, assemble des images, et peut donc pouvoir
       générer des informations erronées. ChatGPT ne peut pas encore
       reconnaître des informations factuelles ou non factuelles.
     * Il faut réfléchir sur les freins et contrepoids de l’IA dans le
       milieu scolaire, qui propage une quantité importante de données à
       partir de « déchets entrants » et « déchets sortants ».
     * Il y a un avantage de ChatGPT à enseigner et aider à corriger la
       grammaire ou les constructions de phrases. Or le revers de la
       médaille est qu’il peut rédiger des essais convaincants à première
       vue, et donc questionner l’authenticité des textes rédigés par
       l’élève. Exclure ChatGPT semble impossible. Il peut être incorporé
       pour de petits devoirs et pour vérifier en temps réel la grammaire
       ou faire des exercices d’écriture. Il faudrait ensuite demander aux
       élèves un retour critique sur les différences entre leurs travaux
       avec et sans IA.
         ______________________________________________________________

   ChatGPT pourrait servir d’outil de vérification de la grammaire dans le
   milieu scolaire. L’IA peut aider les élèves à faire évoluer leurs
   compétences en rédaction et à développer leur esprit critique sur ce
   qu’ils produisent.
     __________________________________________________________________

   Antonio Casilli, Laurence Devillers, Tariq Krim, Christian Chavagneux,
   « ChatGPT : amie ou ennemie ? », table ronde à la 11^e édition du
   « Printemps de l’économie », Paris, Hémicycle du Conseil Economique,
   social et environnemental, 6 avril 2023
   (BUTTON) Idées clé

     * Question essentielle : quelles sont les choses que l’on sait faire
       aujourd’hui et que ChatGPT va pouvoir remplacer ?
     * ChatGPT n’est pas ouvert car il a absorbé beaucoup de connaissances
       qui ne sont pas toutes en accès libre. ChatGPT aurait ainsi intégré
       un ensemble de livres que l’on peut trouver sur une base de données
       pirates russe. Peut-on absorber l’ensemble des connaissances pour
       les refaçonner et les revendre en condensé ?
     * Il n’y a aucune intention, aucune conscience chez l’IA : elle a
       seulement connaissance de nos propos de manière encapsulée (dans
       des suites de mots). ChatGPT n’apprend pas la sémantique des mots
       mais génère les plus probables : c’est donc en partie du hasard.
     * ChatGPT ne s’adapte pas du tout à l’individu mais récupère nos
       données : on est tous des cobayes.
     * ChatGPT est purement comique. Il y a un caractère bluffant par cet
       anthropomorphisme de la machine qui semble pouvoir parler comme
       nous.
     * La non vérification des sources avec ChatGPT ne permet pas de
       vérifier le raisonnement logique de l’IA. Le « diable est dans les
       détails » : il faut donc vérifier les sorties générées et chercher
       dans les détails les erreurs.
     * Encadrer les IA est essentiel pour ne pas laisser les États-Unis
       dominer et imposer leurs règles, mais il faudrait plus de
       financement dans ce domaine (commandes publiques). Nous en avons
       les capacités en Europe ; par exemple en France, des étudiants se
       classent parmi les meilleurs mathématiciens au monde.
     * Créer des financements pour valoriser les bonnes personnes et
       ramener les technologies en France.
         ______________________________________________________________

   ChatGPT ne comprend pas le sens de nos mots : il fonctionne en quelque
   sorte par hasard et se contente de générer des suite de mots probables,
   dans une approche statistique. Les États européens, et notamment la
   France, ne financent pas suffisamment de recherches sur les IA et ne
   permettent pas de travailler correctement. Il faudrait davantage
   valoriser les initatives d’autres pays que les États-Unis (qui dominent
   le marché). En parallèle, il faudrait se concentrer sur le véritable
   intérêt des IA qui est de révéler des signaux faibles et imperceptibles
   pour les humains.
     __________________________________________________________________

   Chris Moran, « ChatGPT is making up fake Guardian articles. Here’s how
   we’re responding », Medium, 6 avril 2023
   (BUTTON) Idées clé

     * Un article généré par une IA était très similaire à un article
       écrit par un chercheur et publié sur The Guardian.
     * Questionnement quant à la confiance et la fiabilité de la presse
       avec les articles écrits par des IA. Si des IA peuvent générer des
       articles, alors la question de la censure se pose aussi. Des
       articles sur des sujets sensibles peuvent être supprimés ou
       modifiés selon les intentions de la personne derrière.
     * Il y avait plus de 100 millions d’utilisateurs mensuels de ChatGPT
       en janvier 2023. En comparaison, TikTok a mis 9 mois à atteindre ce
       niveau..
     * Une étude sur 1000 étudiants a montré que 89% d’entre eux ont
       utilisé ChatGPT pour faire leurs devoirs.
     * Ces technologies se développent très rapidement et les entreprises
       les intègrent de plus en plus vite pour satisfaire leurs
       actionnaires et dominer le marché (concurrence).
     * Question de l’impact des IA dans la presse, en plus des problèmes
       liés à la désinformation et à la polarisation et d’acteurs
       malveillants.
     * The Guardian a conçu une équipe pour travailler sur le métier de
       journaliste et son impact, sur l’apprentissage de la technologie,
       sur les questions de politique publique et de propriété
       intellectuelle, en collaborant avec des universités, des praticiens
       et des entreprises.
         ______________________________________________________________

   Une partie de l’équipe du journal The Guardian étudie l’évolution du
   métier de journaliste avec les IA et sur les problématiques liées à la
   désinformation et aux intentions des entreprises qui éditent ces
   services. Pour accompagner les médias dans leurs relations avec les IA,
   The Guardian a publié un guide sur l’usage et la place des générateurs
   de textes dans la presse écrite.
     __________________________________________________________________

   Doug Stephens, « Human Creativity is Key to Winning the AI Revolution
   », BOF, 5 avril 2023
   (BUTTON) Idées clé

     * La vente au détail va être aidée par les IA, et de manière plus
       générale, les IA vont révolutionner sa productivité et les
       processus commerciaux.
     * L’IA générative va s’améliorer de manière exponentielle, malgré ses
       failles actuelles, grâce aux données qui vont s’accumuler.
     * Les IA remplaceront 1 travailleurs sur 10, et les entreprises
       économiseront alors 80 milliards de coûts de main-d’œuvre d’ici
       2060. Il y aura une disparition des emplois d’aide à la prise de
       décisions pour améliorer le rendement des entreprises.
     * Avantage créatif : si l’IA devient centrale, comment la concurrence
       entre les entreprises va-t-elle se jouer ? Des marques qui ont
       dominé jusqu’à maintenant les autres, que ce soit sur
       l’anticipation des tendances ou la prise de décision plus
       opérationnelle, vont-elles être surpassées par des entreprises
       utilisant l’IA ?
     * L’IA ne peut pas créer de nouvelles choses : l’innovation est un
       exercice créatif produisant des alternatives originales et
       singulières. Les IA ont une pensée convergente, tandis que la
       pensée humaine est divergente.
     * Ne pas confondre mimétisme programmé et véritable créativité. L’IA
       pense à l’intérieur de sa boîte donc elle est restreinte, alors que
       créativité provient d’une multitude d’inspirations insaisissables
       et sensibles à la fois.
     * Économie de la créativité : l’investissement financier dans la
       créativité est essentiel en entreprise. 98% des enfants de moins de
       cinq ans sont considérés comme des génies créatifs alors que ce
       chiffre tombe à 2% chez les plus de 25 ans. Le système éducatif et
       le passage à l’âge adulte, régis par des conventions et des normes
       à suivre, tueraient cette créativité. De plus la créativité diminue
       chez les enfants actuellement à cause de l’omniprésence des
       technologies, laissant peu de place à l’imagination.
     * Il y aura 2 choix pour les entreprises dans le futur : utiliser les
       économies des IA pour les répercuter dans le résultat net, ou
       réinvestir ces économies dans la créativité en tant que source
       concurrentielle.
         ______________________________________________________________

   L’article interroge la place des IA dans les entreprises (avantages et
   évolutions). Si celles-ci vont remplacer les métiers d’aide au
   rendement, elles ne peuvent cependant pas faire preuve de créativité et
   d’innovation car elles pensent de manière convergente, à l’inverse des
   humains qui agissent de manière divergente. Les entreprises devront
   s’assurer de ne pas laisser les IA remplacer la créativité au profit du
   rendement car elles risquent de se faire dépasser par d’autres qui
   soutiennent la créativité humaine comme avantage concurrentiel.
     __________________________________________________________________

   Mia Dand, « The AI Ethics Revolution-A Timeline », Medium, 4 avril 2023
   (BUTTON) Idées clé

     * L’article traite de l’invisibilisation des femmes dans
       l’informatique. Celles-ci ont grandement contribué à son évolution,
       comme Ada Lovelace (qui a conçu le premier programme informatique
       au XIX^e siècle), Joan Clarke (qui a utilisé ses compétences en
       chiffrement pendant la Seconde Guerre mondiale) ou Katherine
       Johnson (une afro-américaine ayant participé au lancement de la
       première fusée américaine dans un contexte de ségrégation raciale).
     * Lutte des femmes marginalisées contre la domination des hommes
       blancs de la Silicon Valley sur l’informatique (qui tirent profit
       des guerres autour de ces technologies). Ces femmes mènent des
       actions afin de protéger l’humanité des conséquences négatives du
       développement de l’IA.
     * 100 Brilliant Woman in AI Ethics : livre publié en 2018 et dans
       lequel on retrouve les noms de femmes qui contribuent aux IA de
       diverses manières. On peut voir le nom de chercheuses comme Timnit
       Gebru, Margaret Mitchell et Inioluwa Deborah Raji, licenciées et
       rétrogradées chez Google pour avoir révélé des risques liés aux IA
       et pour avoir dénoncé les harcèlements sexuels de leurs collègues.
       On y trouve aussi des historiennes comme Mar Hicks, qui a publié un
       ouvrage sur comment la Grande-Bretagne a perdu sa puissance précoce
       dans l’informatique. Ou, enfin, des sociologues et enseignants
       comme Ruha Benjamin qui, dans son ouvrage Race After Technology:
       Abolitionist Tools for the New Jim Code (2019), montre comment les
       algorithmes accentuent les inégalités et les discriminations.
         ______________________________________________________________

   Un site Web a été conçu pour répertorier et documenter les femmes qui
   travaillent sur les IA. Le but est de visibiliser les femmes dans ce
   domaine, qui depuis les débuts de l’informatique, ont activement
   participé à son développement (cf. Isabelle Collet, Les oubliées du
   numérique, 2019, Le Passeur).
     __________________________________________________________________

   Victor Vasseur, « Intelligence artificielle : pourquoi Midjourney ne
   permet pas de générer d’images du président chinois », FranceInter, 4
   avril 2023
   (BUTTON) Idées clé

     * Impossibilité sur Midjourney de générer des images du président
       chinois Xi Jinping : le message suivant apparaît : « L’expression
       est interdite. Le fait de contourner ce filtre pour enfreindre nos
       règles peut entraîner la révocation de votre accès. »
     * Le patron de Midjourney, David Holz, déclare à ce propos : « Nous
       voulons juste minimiser les risques. Si la satire politique dans
       les pays occidentaux est assez normale », observait-il, « elle
       n’est pas acceptée en Chine ». Il préfère autoriser les chinois à
       utiliser l’IA, quitte à encoder de la censure.
     * Il y a donc un danger dans la liberté d’expression et un problème
       politique mondial lié à la censure dans les plateformes d’IA.
         ______________________________________________________________

   La censure sur Midjourney vis-à-vis de la génération d’images du
   président chinois pose des problèmes de liberté d’expression. Des
   tensions sous-jacentes politiques et commerciales entre les pays
   ressurgissent alors. Comment contourner ces censures ? Quelles en
   seraient les conséquences ?
     __________________________________________________________________

   Tristan Mendès, Marion Carré, « Intelligence artificielle : Tristan
   Mendès France et Marion Carré sont les invités de Culture médias »,
   Europe 1, 4 avril 2023
   (BUTTON) Idées clé

     * Les IA touchent de plus en plus des métiers qualifiés : vont-elles
       les remplacer ?
     * Penser que les IA peuvent remplacer ces métiers revient à les
       sous-qualifier. Exemple du métier de journaliste qui demande un
       travail d’investigation long et complexe, chose que que les IA ne
       peuvent pas faire.
         ______________________________________________________________

   Il est primordial d’enseigner et de faire co-découvrir aux étudiants
   les IA et d’écouter leurs retours. ChatGPT peut nous assister dans nos
   réflexions et nous stimuler en posant des questions. L’art aurait un
   rôle intéressant à jouer en servant de « bac à sable » éthique pour
   mieux comprendre les limites des IA.
     __________________________________________________________________

   James Vincent, « AI is entering an era of corporate control », The
   Verge, 3 avril 2023
   (BUTTON) Idées clé

     * Rapport annuel qui a relevé l’évolution de la domination des
       acteurs industriels de l’IA sur les universités et gouvernements.
     * L’IA entre dans une nouvelle phase de développement avec la
       généralisation des outils grand public (ChatGPT, Midjourney, etc.)
       mais les prises de décision sur leur déploiement restent aux mains
       des entreprises.
     * Les universités ont ouvert la voie aux IA, mais les industries ont
       pris le relais quant à leur développement. En 2022, 32 modèles d’IA
       importants auraient été développés par les entreprises, et
       seulement 3 par les universités car le développement de ces
       technologies est coûteux (données, personnel et puissance de
       calcul).
     * Il existe un danger sur la sécurité avec la course à l’IA comme
       moyen de concurrence entre entreprises rivales, d’où la nécessité
       d’une pause dans le développement technique.
     * Il y a une augmentation des abus éthiques avec la généralisation
       des applications d’IA : décès liés à la conduite autonome de Tesla,
       deepfakes audio et nudes non consentis, arrestations dues à des
       erreurs de reconnaissance facIAle (avec préjugés raciaux).
     * En parallèle, il y a une volonté des législateurs et des décideurs
       de réglementer l’IA. Une analyse a révélé, sur des dossiers
       législatifs de 127 pays, que le nombre de projets de lois sur les
       IA est passé d’un seul en 2016 à 37 adoptés en 2022. Cela
       permettrait de faire un contrepoids à l’autorégulation des IA par
       les entreprises.
         ______________________________________________________________

   La rapidité des développements des IA par les entreprises privées à des
   fins concurrentielles soulève des enjeux de propriétés, de domination
   et de généralisation des technologies. Malgré l’augmentation des
   problèmes éthiques avec les IA, on observe une une prise en compte de
   leur régularisation par les États, avec de plus en plus de projets de
   lois.

   Jill Lepore, « The Data Delusion », The New Yorker, American
   Chronicles, 3 avril 2023
   (BUTTON) Idées clé

     * Histoire fictive sur comment OpenAI fonctionnerait si les humains
       fonctionnaient comme les IA (anthropomorphisation).
     * Quel est le prix à payer pour l’humanité si on stockait et
       téléchargeait toutes les connaissances du monde dans des milliards
       de machines, et si on apprenait à en tirer de nouvelles
       connaissances ?
     * L’intérêt d’introduire des données dans les ordinateurs est la
       prédiction, laquelle s’effectue par la détection de modèles.
       L’auteur donne l’image de 4 tiroirs pour comprendre comment les
       connaissances du monde sont stockées et organisées : le premier,
       celui du mystère, contient les connaissances que seules des entités
       ésotériques ou religieuses peuvent expliquer. Le second est celui
       des faits, pour parler des phénomènes que les humains peuvent
       prouver par l’observation, la détection et l’expérimentation. Le
       troisième tiroir est celui des nombres, qui contient les
       recensements, sondages, décomptes : tout ce qui peut être compté
       depuis le XVIII^e siècle. Enfin, le dernier tiroir est celui des
       données, qui répertorie les connaissances auxquelles les humains ne
       peuvent accéder que par un ordinateur ou une IA. Ce tiroir se
       serait rempli depuis un siècle et est désormais encombré.
     * Ces tiroirs peuvent se ressembler de l’extérieur, mais ils suivent
       des logiques différentes à l’intérieur. On apprend les mystères par
       la révélation et la mystification ; les faits ont pour but de
       trouver la vérité et on les apprend par discernement et la
       sécularisation ; les chiffres ont pour enjeu la gouvernance
       publique (liée à la montée de l’État administratif) ; et enfin les
       données sont associées au capitalisme tardif, au techno-utopisme et
       à la science des données.
     * Ces quatre modes de savoir peuvent s’hybrider pour comprendre des
       phénomènes. Exemple d’une fusillade aux États-Unis : il s’agit à la
       fois de prier pour les morts (aspect religieux, donc de l’ordre du
       mystère) et de recourir à des spécialistes pour recueillir les
       faits, par l’analyse de chiffres, en s’aidant des données de
       technologies numériques.
     * Le problème actuel pour l’humanité est que les individus ont
       tendance à n’ouvrir que le tiroir des données, celui du bas : nous
       nous fions donc plus facilement à ce que les IA peuvent générer.
     * La montée des chiffres dans l’histoire humaine a servi d’instrument
       au pouvoir par l’État.
     * Le grand-père d’Elon Musk, important dans le mouvement
       technocratique au Canada, est à l’origine du fantasme de la
       suprématie technologique. La technocratie abolit les mécanismes
       économiques et politiques existant pour les remplacer par des
       ingénieurs. On a donc un passage d’une culture des chiffres à une
       culture des données depuis la Seconde Guerre mondiale, pour devenir
       plus prédictif et répondre à des enjeux pressants (guerre, actions,
       stratégies, etc.).
     * L’économie et les sciences politiques sont devenues des sciences
       prédictives, atrophiant les autres modes de connaissance liés à ces
       domaines.
     * Il y a un enseignement de la science des données depuis le XXIe
       siècle qui tend à en faire le seul outil capable de produire de la
       connaissance. Or il importe d’ouvrir les 4 tiroirs pour ne pas
       rester dans une seule manière de savoir.
     * Il y a une inquiétude dans la prédiction des comportements humains
       par la science des données et un risque dans notre confiance envers
       les données numériques.
         ______________________________________________________________

   L’article analyse l’évolution de la « science des données » et comment
   les technologies numériques ont modifié l’intégration des connaissances
   pour les humains. Il y aurait quatre méthodes différentes pour générer
   des connaissances et les véhiculer : le mystère, qui se relie à
   l’ésotérisme et à la religion, les faits, prouvés par observation et
   expérimentation, les nombres associés aux chiffres, et enfin les
   données, accessibles par le numérique. J. C. R. Licklider,
   informaticien américain, examine les inconvénients des livres dans son
   ouvrage Les bibliothèques du futur (1965) : « Si l’interaction humaine
   avec l’ensemble des connaissances est conçue comme un processus
   dynamique impliquant des examens répétés et des comparaisons entre de
   très nombreux éléments petits et dispersés, alors tout concept de
   bibliothèque qui commence par des livres sur des étagères est sûr de
   rencontrer des problèmes. » Il soutient l’idée de convertir les livres
   en données lisibles par un ordinateur pour les rendre accessibles aux
   utilisateurs.
     __________________________________________________________________

  Mars 2023
     __________________________________________________________________

   Linus Ekenstam, « Pause All Giant AI Experiments », Inside my Head, 31
   mars 2023
   (BUTTON) Idées clé

     * Travail de cartographie à faire pour savoir où les IA sont
       développées afin de percer leur complexité.
     * Qui est autorisé à parler sur l’éthique des AI et sur sa
       définition ? Qu’est-ce que l’éthique à l’ère des IA ? De laquelle
       on veut parler ? À quel moment commence-t-on à s’inquiéter des IA ?
     * Il faut arrêter avec le mythe de l’intelligence générale
       artificielle qui dépasserait les performances humaines dans tous
       les domaines. Il y a une colonisation sémantique du terme d’IA qui
       est utilisé pour tout et rien.
     * La recherche sur l’IA est récente : on observe à partir de
       2015-2016 une surproduction de chartes et de lois (67 selon l’ONG
       allemande Algoritmwatch). Il faut observer qui est à l’origine de
       ces textes, en général de grandes entreprises privées.
     * Pause Giant AI (2023) rappelle une lettre ouverte de 2016 sur la
       même plateforme et par les mêmes entreprises (« Autonomous Weapons
       Open Letter: AI & Robotics Researchers »). Les GAFAM sont la
       majorité des auteurs de ces articles.
     * Aristote définissait l’éthique comme intrinsèquement liée à la
       politique car elle sert à s’accomplir dans la vie politique. Qu’en
       est-il des IA ?
     * Une éthique est possible si l’on se débarrasse de l’étiquette d’«
       artificiel » car l’IA est peut-être intelligente mais pas
       artificielle : elle est régie par des humains. Mettre l’accent sur
       l’éthique pour encadrer les questions de pouvoir, de domination,
       d’inégalité et d’oppression.
     * L’éthique de l’IA n’est pas à penser sans les luttes pour
       l’environnement. Ces technologies s’appuient sur des équipements
       transformant les matières premières (cobalt, nickel, lithium,
       etc.). Il y a besoin de mettre en place des infrastructures
       globales d’extraction de minerais pour le numérique. Cette
       production de ressources change les paysages de pays comme la
       Bolivie, où l’on trouve le plus grand gisement de lithium à ciel
       ouvert.
     * Codes éthiques de l’IA en 5 points : la transparence, la justice et
       l’équité, la non malfaisance (l’IA ne devraient pas produire des
       dégâts envers la population, or les débats ne sont vifs que quand
       les IA sont intégrés dans le domaine de l’armement, par exemple),
       la responsabilité (exemple du MIT sur les voitures autonomes et
       l’éthique : quelle décision la voiture doit-elle prendre
       lorsqu’elle est obligée de sacrifier des vies ? Qui tuer ? Question
       de « liability » : qui est responsable ?) et la protection de la
       confidentialité.
     * « The Steep Cost of Captur » de Whittaker, de 2021 est une forme de
       « J’accuse » contre les géants de la tech qui neutralisent la
       critique en dénigrant les approches dissidentes et en finançant les
       critiques les plus faibles.
     * Il faudrait se concentrer sur la phase de production de l’IA plus
       que sur la phase d’utilisation.
     * Cas des prolétaires dans les pays émergents (exemple de Madagascar
       dans lequel s’est rendu Antonio Casilli) qui travaillent dans des
       conditions insalubres : ce ne sont pas des IA mais ce sont des gens
       qui se font passer pour des IA. Les grandes entreprises continuent
       de faire croire qu’elles entraînent des IA sans humains derrière.
     * La Russie achète des données brutes de micro-travailleurs : on
       trouve de nombreuses « fermes à recaptcha » en Russie.
     * Depuis 2022, des annonces de tâches d’entraînement d’IA par la
       Russie ont été signalées (invasion russe) : une application
       demandait aux ukrainiens de documenter les positionnements
       militaires en cours, mais le pays à l’origine de cette application
       reste inconnu (Russie ? Ukraine ? États-Unis ?)
     * Le Venezuela est le pays central dans les IA : il est le plus
       représenté dans toutes les bases de données de micro-travail.
         ______________________________________________________________

   L’auteur demande plus de conversations et de débats sur ces
   technologies et de suspendre le développement des LLM (Large Language
   Models) le temps d’avoir bien compris les limites et règles qu’il
   faudrait mettre aux IA. Il propose une liste de vidéos d’interviews
   d’experts pour mieux comprendre ce qui se déroule actuellement.
     __________________________________________________________________

   Evgeny Morozov, « The problem with artificial intelligence? neither
   artificial nor intelligent », The Guardian, 30 mars 2023
   (BUTTON) Idées clé

     * Lettre signée par Elon Musk et Steve Wozniak pour appeler à
       ralentir le développement des systèmes d’IA afin de donner le temps
       de s’y habituer en installant un « été de l’IA » (référence aux
       « hivers » que l’IA a connus depuis ses débuts). Il faut mettre en
       place des protocoles de sécurité bien audités.
     * Le terme d’« intelligence artificielle » prête à confusion et
       serait dépassé, d’où la nécessité de retirer ce terme des débats
       publics, tout comme par exemple, les termes de « rideau de fer » ou
       de « théorie des dominos ».
     * L’« intelligence artificielle n’est ni artificielle ni intelligente
       » : l’IA est originellement régie par des règles pour justifier la
       partie artificielle. Or les IA actuelles, comme ChatGPT puisent
       leur force dans le travail créatif des humains et ne sont donc pas
       réellement « artificielles ».
     * Les premiers travaux de l’IA ont été financés dans le contexte de
       la guerre froide, donc l’aspect « intelligent » de ces programmes
       est majoritairement de l’ordre d’une capacité à donner des
       informations sur un événement et à enclencher des actions.
     * La puissance de l’IA actuelle réside dans la correspondance de
       modèles. Cela fait écho aux premières utilisations militaires de
       l’IA qui pouvait repérer des navires sur des photographies
       aériennes. Mais cette correspondance de modèles n’est pas ce qui
       détermine l’intelligence car l’émotion fait aussi partie de
       l’intelligence. En effet, l’intelligence humaine n’est pas
       unidimensionnelle : elle repose sur la « bi-logique », un terme
       inventé par le psychanalyste chilien Matte Blanco au XX^e siècle.
       La bi-logique est « une fusion de la logique statique et
       intemporelle du raisonnement formel et de la logique contextuelle
       et hautement dynamique de l’émotion ». Le premier cherche les
       différences, tandis que le second les efface : la bi-logique
       cherche à expliquer comment nous classons de manière nouvelle et
       pertinente des choses banales.
     * L’IA ne peut pas faire preuve de bi-logique car elle n’a pas
       conscience de la temporalité et des émotions. Elle est prisonnière
       de la logique formelle singulière
     * L’IA est avant tout prédictive et n’est donc pas réellement
       intelligente. Employer le terme d’intelligence revient à réduire le
       fonctionnement du monde à une logique rationnelle.
         ______________________________________________________________

   Il est nécessaire de questionner le terme « d’intelligence artificielle
   » : qu’est ce qui est si intelligent et artificiel dans ces
   technologies ? ChatGPT et les autres IA se développent grâce aux savoir
   des humains et ne sont donc pas si « artificiels » que cela, car ils se
   basent sur du « naturel » pour fonctionner. L’intelligence n’est pas
   seulement une correspondance de modèles mais peut aussi faire des
   généralités (exemple de Marcel Duchamp qui a fait passer un objet
   banal, l’urinoir, à une œuvre d’art, ce qui a entraîné un effet de
   généralisation). L’intelligence humaine n’est pas unidimensionnelle
   mais repose sur la « bi-logique » (Matte Blanco), qui explique comment
   nous classons des choses banales de manière nouvelle et pertinente.
   L’IA ne fait pas preuve de bi-logique car elle n’a pas conscience de la
   temporalité et des émotions ; elle est prisonnière de sa logique
   formelle singulière. L’IA est avant tout prédictive et n’est pas
   réellement intelligente.
     __________________________________________________________________

   Eliezer Yudkowsky, « Pausing AI Developments Isn’t Enough. We Need to
   Shut It All Down », Time, 29 mars 2023
   (BUTTON) Idées clé

     * Que se passera-t-il quand l’IA sera plus intelligente que les
       humains ? Cette question semble plus forte que celle de la
       concurrence.
     * La mort de tous les habitants sur Terre pourrait advenir si l’IA
       surpasse l’ humain. Cela demande d’apprendre et de préparer de
       nouvelles connaissances scientifiques.
     * S’il y a un manque de préparation et de précision par les humains
       dans leurs relations avec les IA, alors ces technologies risquent
       de ne pas faire ce pour quoi nous les avons programmés, et donc de
       ne pas faire attention à la dimension sensible du monde.
     * L’IA sera bientôt en dehors des ordinateurs : elle pourra
       construire des formes de vie artificielles comme la production de
       protéines à la demande.
     * Le danger d’OpenAI est que nous ne savons pas comment décoder les
       IA, donc nous ne pouvons pas vérifier leur évolution. Nous doutons
       de ces IA et leur attribuons une méfiance quant à ce que l’IA peut
       produire.
     * Si l’on n’est pas sûr (ou qu’on ne sait pas ce qu’on fait), alors
       cela doit être arrêté.
         ______________________________________________________________

   Eliezer Yudkowsky lance un cri d’alarme sur la nécessité d’arrêter le
   développement des IA, qui ne sont pas bien encadrées à l’heure
   actuelle. Nous ne savons pas ce que les IA peuvent produire et comment
   elles vont évoluer : il y a un risque de ne pas savoir ce que l’on fait
   et donc de ne pas savoir où l’on va. Des solutions sont proposées pour
   mieux encadrer les IA. Tout d’abord, empêcher les États-Unis d’être
   l’unique propriétaire de la technologie. Ensuite, fermer les gros
   clusters GPU et plafonner la puissance de calcul qu’une personne est
   autorisée à utiliser pour former des systèmes d’IA. Enfin, conclure des
   accords multinationaux pour empêcher des activités illicites et suivre
   (tracker) tous les GPU vendus.
     __________________________________________________________________

   Reuben Cohn-Gordon,« GPT’s Very Inhuman Mind », Noema, 28 mars 2023
   (BUTTON) Idées clé

     * La « simple reconnaissance des formes » est un sujet récurrent dans
       les discussions sur les IA et évoque une dichotomie entre une
       véritable compréhension et une astuce superficielle, pour donner
       l’illusion à l’individu qu’il s’adresse à un humain. Cette idée
       peut s’incarner dans la différence entre l’écho et l’Écho. Le
       premier ne comprend pas ce que vous dites et transforme votre voix,
       tandis que le second se réfère à un personnage issu des
       Métamorphoses (long poème en latin) d’Ovide. l’Écho est caché dans
       une grotte et répète ce qu’il entend. Il comprend la signification
       du son que l’on émet, son sens et son contenu et répète avec ironie
       ce que vous avez dit. Echo et echo sont donc similaires car ils
       reçoivent et répètent un son mais il se différencient par le fait
       que le premier est un réel interlocuteur, tandis que l’autre, nous
       fait croire d’en être un. Ainsi, ChatGPT prétend être un véritable
       interlocuteur comme Echo mais peut se relier à l’echo, car il se
       concentre sur la forme et ne peut pas travailler avec le contenu.
     * ChatGPT fait preuve de compétences linguistiques et de connaissance
       des mots (cf. notion de « traduction non pragmatique », développée
       par Bérengère Viennot, qui montre que les IA sont douées à traduire
       des domaines avec possédant des vocabulaires spécifiques).
     * ChatGPT n’est pas si « spectaculaire » : il n’est pas si loin de
       ELIZA, un programme des années 1960 qui arrangeait nos requêtes
       sous forme de questions. La perception du sens des réponses de l’IA
       est une pure projection que nous faisons.
     * Un nouvel « hiver de l’IA » serait en approche selon des linguistes
       et scientifiques cognitifs car l’approche purement statistiques est
       conceptuellement défectueuse : un plafond est presque atteint.
     * Les contenus et significations produits par les IA ne sont que des
       nombres et des fonctions car elles ne fonctionnent que par
       manipulation des contenus et non pas de formes.
     * Existe-t-il un rapport entre l’intuition de la programmation et
       l’IA ? Les fondements de l’IA et du langage humain sont similaires
       aux langages de programmation : la forme est une série de lettres
       ou de caractères. Mais la différence est dans la compréhension du
       contexte : l’ensemble des phénomènes (guerres, fêtes, sensations
       plus ou moins agréables, etc.) et des concepts (norme, beauté etc.)
       ne peut être compris que par les humains.
     * Wilfrid Sellars, philosophe du langage, appelle le monde humain
       « image manifeste » et l’oppose au monde des atomes, celui de l’«
       image scientifique », qui est mieux compris.
     * L’IA construit un pont conceptuel entre la mathématique de
       l’informatique et l’intelligence pour adapter les techniques des
       langages de programmation au monde humain. Ici, se pose alors la
       question de pourquoi ne pas appliquer les techniques d’écriture des
       langages de programmation pour comprendre les humains, et toutes
       les techniques d’écriture des logiciels pour comprendre l’esprit.
     * À l’intérieur de ChatGPT, on ne trouve pas des règles de
       raisonnement ou une base de choses que le programme connaît. En
       effet, les programmeurs n’ont pas passé du temps à expliquer les
       règles de la grammaire à ChatGPT. Il ne les appréhende que depuis
       l’abondance des contenus analysés.
     * La critique des IA actuelles rappelle plusieurs enjeux
       historiques.La traduction et le sous-titrage automatique sont
       devenus des outils familiers ; ils n’animent plus de débats car on
       a réalisé qu’ils ne comprennent qu’en surface. Aussi, les IA
       actuelles rejouent les débats sur les méthodes statistiques de
       traduction, la classification et le sous-titrage des images par
       exemple par les IA et leur compréhension qui se situe en surface.
       Cela rappelle la perspective connexionniste dans les années 1980 et
       le débat entre Noam Chomsky, qui défend l’idée que l’apprentissage
       du langage est dû à une faculté innée et à une créativité libérée
       et B. F. Skinner, qui prône le behaviorisme, c’est-à-dire le fait
       que l’acquisition du langage fonctionne par association, imitation
       et renfort (cf. Noam Chomsky, « A Review of B. F Skinner’s Verbal
       Behavior, 1980 )
     * Le monde reste insaisissable pour les IA, qui ne peuvent pas
       produire des sorties originales, explicables, fiables et
       significatives.
         ______________________________________________________________

   L’article s’appuie sur plusieurs théories et débats historiques pour
   interroger la capacité des IA actuelles (comme ChatGPT) à comprendre le
   monde humain. Il y a une distinction à établir entre « l’image
   manifeste » (Wilfrid Sellars), liée au monde humain, difficilement
   saisissable, et « l’image scientifique », associée au monde des atomes,
   et qui est plus facilement compréhensible. L’auteur établit une
   critique importante concernant l’impossibilité pour l’IA de s’ancrer
   dans le monde humain, ce qui a pour effet de générer des textes
   inexacts.
     __________________________________________________________________

   Jenka Gurfinkel, « AI and the American Smile », Medium, 27 mars 2023
   (BUTTON) Idées clé

     * Demander à une IA de générer des images de différentes cultures
       (tribus) qui font un selfie à travers le temps : la composition est
       toujours la même et les expressions du visage aussi. Une histoire
       est composée de toutes pièces par l’IA car les mêmes « faux
       sourires » se répètent dans les images de chaque tribu. Or ce
       sourire est anachronique et n’est pas universel : nous comprenons
       les sourires et nous réagissons différemment en fonction des
       cultures.
     * Le selfie et l’expression d’un faux sourire avec les dents sont une
       manière stéréotypée de se prendre en photo dans l’époque actuelle.
     * Des chercheurs ont analysé cette expression faciale, de type « faux
       sourire » et ont mis en évidence les dangers des images générées
       dont les visages mentent par le sourire.
     * La complexité de comprendre les expressions faciales de différentes
       cultures peut amener à des incompréhensions entre des individus et
       générer des complications relationnelles.
     * L’IA reproduit et exagère les codes des communications américains
       destinés à faire bonne impression. Cela engendre un aplatissement
       et une uniformisation de la multitude d’expressions faciales des
       différentes civilisations du monde.
         ______________________________________________________________

   Il y a un problème dans l’uniformisation des expressions faciales par
   les IA, qui reproduisent des codes de communication physiques de la
   culture qui détient et contrôle ces IA (ici, les États-Unis). Cela
   amène à une faible diversité dans la génération des visages humains et
   à une standardisation des représentations.
     __________________________________________________________________

   David Rotman, « ChatGPT is about to revolutionize the economy. We need
   to decide what that looks like », MIT Technology Review, 25 mars 2023
   (BUTTON) Idées clé

     * Les IA peuvent désormais automatiser des tâches qui étaient
       autrefois attribuées aux humains, comme celles liées à la
       créativité, au raisonnement humain et à l’écriture.
     * ChatGPT pourrait-il permettre de rééquilibrer les inégalités entres
       pays ou va-t-il les aggraver ? Pourrait-il donner un coup de pouce
       pour la productivité ?
     * Les nouvelles technologies aggravent les inégalités. Elles sont de
       plus en plus puissantes car elles sont entraînées à analyser de
       plus en plus de données et sont immiscées dans de plus en plus de
       logiciels.
     * L’IA deviendra un outil bénéfique pour aider des individus à
       améliorer leurs capacités et expertises mais sera aussi un outil
       négatif car les entreprises en tireront parti pour remplacer des
       métiers
     * ChatGPT risque d’affecter 19% des métiers et 50% des tâches de
       chaque métier.
     * Les métiers les plus vulnérables sont les auteurs, les designers
       Web et numérique, les analystes financiers et les ingénieurs de
       blockchains.
     * Les IA génératives pourront aider des personnes à acquérir des
       compétences pour rivaliser avec celles qui ont plus d’éducation et
       d’expertise.
     * Expérience testée sur des étudiants et enseignants en marketing :
       une partie a utilisé ChatGPT pour accomplir leur tâches
       quotidiennes et les autres non. ChatGPT a augmenté la productivité
       mais l’IA a aidé les personnes les moins qualifiées, réduisant
       ainsi l’écart entre les bons et les moins bons. En résumé : les bon
       travailleurs ont juste vu leur travail s’accélérer et les moins
       bons se sont améliorés.
     * Le remplacement des humains par les machines fait baisser les
       salaires des humains et exacerbe les inégalités de richesse. Dès
       lors que les IA sont utilisées à grande échelle pour la conception
       graphique, la rédaction de textes par exemple, elles permettront
       d’augmenter le rendement des entreprises.
     * Quand l’IA aidera les humains à faire des découvertes, son impact
       sera encore plus puissant qu’à l’heure actuelle.
     * Problème de l’IA dominée par une seule ou quelques grosses
       entreprises, dont les modèles sont similaires (commerciaux) : il
       faudrait des investissements de la part de gouvernements de
       plusieurs pays (exemple d’ARPANET financé par l’US Department of
       Defense pour montrer l’importance l’importance des financements par
       les États).
         ______________________________________________________________

   L’article propose une analyse de l’économie et de la productivité des
   entreprises avec les IA. Quels sont les impacts des IA sur les métiers,
   et comment s’assurer que les entreprises prennent la bonne direction ?
   Aujourd’hui, les IA automatisent les tâches cognitives et non celles
   physiques car elles demandent de forts investissements en équipement et
   en structures. Or il ne faudrait pas rester passif face à cette
   situation : il est nécessaire de créer des débats et d’échanger avec
   tous les corps de métiers pour introduire au mieux ces technologies et
   ne pas les laisser remplacer les humains. Ainsi, le vrai enjeu est de
   se concentrer sur la façon dont les technologies peuvent étendre les
   capacités des individus et non pas comment les IA peuvent imiter les
   humains.
     __________________________________________________________________

   Benj Edwards, « ChatGPT gets « eyes and ears » with plugins that can
   interface AI with the world », Arstechnica, 24 mars 2023
   (BUTTON) Idées clé

     * Plugin de ChatGPT annoncé par OpenAI qui peut intervenir dans la
       réservation de vols, commande de courses, etc.
     * ChatGPT est étendu et s’immisce dans de nombreuses fonctionnalités.
       Une interface présente tous les plugins de ChatGPT. Par exemple, on
       trouve Expedia pour planifier des voyages, Speak pour des cours de
       langue, ou Zapier, qui permet à ChatGPT automatiser un logiciel
       existant (écrire un mail, faire une mise à jour, une recherche,
       etc.).
     * OpenAI héberge trois plugins pour l’instant : un navigateur Web, un
       interpréteur de code et un outil pour accéder à des informations
       personnelles ou organisationnelles hébergées ailleurs.
     * OpenAI restreint-il l’exploration pour les programmeurs ? Car si un
       IA peut remplacer des logiciels, alors le rôle des programmeurs
       est-il amenuisé ?
     * Le développement d’une variété de plugins augmenterait les risques
       de dérives : ils pourraient être plus facilement détournés pour
       être utilisés dans des contextes différents de ceux d’origine.
         ______________________________________________________________

   La conception de plugins d’OpenAI rend son intégration de plus en plus
   ubiquitaire et questionne deux aspects : celui du rôle des programmeurs
   et des dérives qui vont avec la profusion des outils IA. Si des IA
   peuvent remplir les tâches de certains sites Web et logiciels, les
   programmeurs sont-ils indispensables ? De quelles manières la profusion
   des IA ouvre-t-elle la porte à des dérives ?
     __________________________________________________________________

   STAA-CNT-SO, (Syndicat des Travailleur-euses Artiste-Auteur-ices de la
   Confédération Nationale du Travail Solidarité Ouvrière), « Non à
   l’automatisation des métiers de l’art », STAA-CNT-SO, 17 mars 2023
   (BUTTON) Idées clé

     * Machine comme simulacre de la traduction humaine.
     * Les arts graphiques sont impactés par la génération de visuels
       semblant rivaliser avec la production humaine.
     * Dans l’article, les auteurs n’utilisent pas les termes de création,
       de production, d’illustrations machiniques et d’intelligence
       artificielle car dès lors que l’on emploie ces termes, il y a une
       anthropomorphisation des machines et donc, à l’inverse, on
       « machinise » l’humain.
     * Ils défendent la préservation de la force, de l’évolution et de la
       richesse du langage contre l’omniprésence des algorithmes. Les
       algorithmes réduisent la langue à « un recouvrement binaire de
       l’expérience du monde » où le langage mathématique prend une
       dimension absolue.
     * « La réduction algorithmique des œuvres artistiques à de simples
       données mathématiques fait disparaître, dans cette opération même,
       ce qui en constitue l’essence, c’est-à-dire ce qui se trouve à la
       croisée entre une expérience singulière humaine (apprentissage,
       lectures, désirs, affinités, relations, etc.) et un espace-temps
       social où s’est tissée cette expérience. »
     * L’imagination est enfermée et imbriquée dans notre monde
       capitaliste. Georg Lukács (1920) parle du capitalisme comme d’une
       « expansion d’une forme de rationalisation basée uniquement sur des
       critères quantitatifs. »
     * Renforcer les droits d’auteur pour mieux cadrer l’utilisation des
       IA ne serait pas la solution. La rémunération en droits d’auteur
       des artistes ne permet qu’à une minorité de vivre de leur travail.
       Les IA sont problématiques pour les artistes qui continueront de
       publier leurs créations pour se faire connaître mais qui, en
       parallèle, alimenteront les IA.
     * Le capitalisme s’oppose au travail artistique car il ne considère
       pas son caractère laborieux et complexe (esquisses, études,
       critiques, etc.). Le capitalisme pense pouvoir remplacer les
       métiers de la création par des machines.
     * Les machines uniformisent les modes de représentations et répètent
       les systèmes discriminatoires. La neutralité des algorithmes
       n’existe pas.
     * Le STAA refuse de « voir nos savoir-faire réduits à de simples
       données quantifiables, que nos œuvres servent de matière première à
       ces algorithmes. De voir nos métiers se transformer en
       opérateur·ices ou correcteur·ices de ces machines. Qu’un champ
       entier de la production artistique devient une industrie
       standardisée reposant sur du travail précaire et non qualifié. De
       voir transformer une minorité d’entre nous en « conservatoires » de
       savoir-faire zombifiés pour la seule jouissance patrimoniale de
       quelques nostalgiques privilégiés. »
     * Le STAA appelle à boycotter les diffuseurs, entreprises et maisons
       d’édition qui ont recours à ces algorithmes et participent en toute
       conscience à la précarisation de nos métiers. Il rappelle que la
       pensée, l’expérience et le geste humains sont les éléments centraux
       de la création artistique. Il s’agit d’engager une réflexion
       collective sur les dévoiements du langage des capitalistes
       techno-béats et comment les contrer. »
         ______________________________________________________________

   Le STAA rejette les créations générées par l’IA car elles opèrent une
   réduction de l’expertise humaine à une appendice de la machine, dont
   les enjeux sont purement capitalistes. Selon eux, parler d’intelligence
   pour les machines revient à déformer la langue et à la déshumaniser. Il
   ne faut pas oublier que la machine calcule et opère : elle ne produit
   pas et ne crée pas. Il y a alors un danger à réduire les artistes à des
   purs opérateurs par les clients, qui peuvent donc les payer moins cher
   sous prétexte que leur travail peut être fait plus rapidement par les
   machines.
     __________________________________________________________________

   Stephen Ornes, « The Unpredictable Abilities Emerging From Large AI
   Models », Quanta magazine, 16 mars 2023
   (BUTTON) Idées clé

     * Les modèles les plus complexes d’IA peuvent aller au delà de
       simplement accepter une chaîne de texte en entrée et prédire ce qui
       va suivre. Ils peuvent réussir à décoder la signification d’une
       suite d’emojis par exemple. Ces LLMS peuvent générer des centaines
       de capacités « émergentes », afin de décoder des entrées complexes.
     * Terme d’« émergent » initialement utilisé en biologie, pour décrire
       les comportements auto-organisés et collectifs de nombreux éléments
       agissant seuls, se retrouve dans ces IA.
     * Avant, les modèles algorithmiques étaient purement prédictifs et
       ses performances étaient améliorées par ce qu’il apprenait de ses
       sorties.
     * Différence entre « réseau récurrent» et «transformateur », le
       premier analyse la phrase mot par mot, le second traite tous les
       mots en même temps : ils peuvent donc traiter des textes denses en
       parallèle par des connexions établies.
     * Ces transformateurs se rapprochent donc du langage et de la
       compréhension humaine. Un test, le BIG-bench (Beyond the Imitation
       Game Benchmark), a été conçu pour savoir si l’ordinateur peut
       répondre aux questions de manière humaine.
     * Plusieurs percées, sauts rapides et spectaculaires dans les
       performances à une certaine échelle de seuil, ont été repérés dans
       ces IA par des chercheurs du Google Research. Les IA avec des
       milliards de paramètres (comme GPT-4) sont donc plus précis. Ils
       peuvent décoder l’alphabet phonétique international, décrypter les
       lettres d’un mot ou identifier un contenu offensant dans l’hinglish
       (hindi et anglais combiné).
     * Manière dont on formule la requête influence grandement la
       précision de la sortie de l’IA.
     * Or, ces LLM restent encore opaques car nous n’avons pas accès à
       leurs fonctionnements donc, il est compliqué d’expliquer le
       fonctionnement et l’évolution de ces IA, de plus en plus
       performants.
         ______________________________________________________________

   Les modèles LLM (Large Langage Model) de types «transformeurs»
   comprenant le plus de paramètres, s’avèrent être de plus en plus
   performants et bluffants : ils peuvent analyser le sens des phrases en
   traitant les mots en en simultanés, et non plus mots par mots. Ils
   semblent se rapprocher progressivement des capacités cognitives
   humaines et de nos systèmes de langage.
     __________________________________________________________________

   Yannig Raffenel, « L’apport de l’IA au monde de la formation »,
   conférence dans le cadre du West Data Festival à Laval, 14 mars 2023
   (BUTTON) Idées clé

     * Conférence sur la place de l’IA et des dates dans la formation par
       Yannig Raffenel, co-président de EdTech France, entreprise sur
       l’éducation dans la technologie.
     * Dès les années 90, FOAD (Formation Ouverte et À Distance) : besoin
       de faire du distanciel déjà à l’époque car il y avait un besoin
       d’individualisation, face à des profils et des pré-requis très
       différents. La limitation des coûts de déplacement et former
       beaucoup plus de mondes étaient deux besoins aussi nécessaires. À
       cette époque, la réponse ne passait pas encore par le numérique et
       l’informatique car Internet n’était pas encore assez développé :
       c’était plus une question d’usages.
     * Dans les années 2000, le E-Learning s’est mis en place mais fut un
       échec : les promesses d’individualisation des formations n’étaient
       pas respectées. C’était avant tout des formations imposées et
       similaires pour tous. Les auto-formations amènent aussi
       généralement à de l’auto-abandon : on apprend avec et par les
       autres, apprendre tout seul est plus compliqué.
     * En 2020, le Covid a fait basculer toutes les institutions en
       distanciel, sans qu’il y ait de préparation. Ce fut une réponse
       technologique mais dont les usages n’ont pas été travaillés. Le
       distanciel n’est pas une solution : ce système n’est pas tenable.
     * Le E-learning est maladroitement pensé comme un système
       fonctionnant par une digitalisation du contenu sur lequel un
       ensemble d’individus viendrait se brancher sur l’élément de
       diffusion pour apprendre tout seul. Or, on voit donc une réduction
       dangereuse de ce qu’est l’enseignement au profit d’un outil
       « magique ». Ce n’est pas l’outil qui est innovant mais la méthode
       d’enseignement.
     * La place des datas et la notion de temps sont importantes à
       comprendre pour voir l’évolution de l’enseignement avec le
       E-learning. Les LMS (Learning Management System) qui sont des
       plateformes qui calculent le temps passé des individus sur les
       formations, servait de base pour voir si les formations
       fonctionnaient. Seul le temps passé sur les formations comptait.
       Or, actuellement l’usage de la puissance du digital et de la
       pédagogie remettent en cause cette notion du temps car les
       formations sont plus courtes et plus denses et assurent une
       efficacité. -Aussi, les commanditaires des formations sont de plus
       en plus les responsables des métiers dans les entreprises et non
       plus les responsables formation. Cela implique que ces responsables
       veulent s’assurer que les formations seront adaptés aux métiers
       qu’ils représentent : ils seront donc prêts à investir dans des
       formations compétentes.
     * L’adaptive learning, permet de construire des parcours de formation
       réellement adaptés aux individus en fonction des objectifs et des
       contraintes. En utilisant l’IA, les formations serait donc
       réellement en lien avec notre profil.
         ______________________________________________________________

   Dans sa conférence Yannig Raffenel parle de l’intérêt de l’adaptive
   learning dans les formations à distance. À partir d’une analyse des
   débuts des formations à distance, il explique l’importance de ne pas se
   concentrer sur l’innovation technologique mais bien dans les méthodes
   d’enseignement même, pour développer des formations adaptées aux
   individus.
     __________________________________________________________________

   Hunter Walk, « Instead of Asking AI Companies to ‘SLOW DOWN’ We Should
   Encourage Them to Move Even Faster », Philosophie magazine, 13 mars
   2023
   (BUTTON) Idées clé

     * Deux positions actuelles se dégagent : continuer de développer les
       IA ou ralentir
     * Loi de « Safe Harbor » est un cadre qui détermine les comportements
       qui n’enfreignent pas une règle, tant que des conditions précises
       sont bien suivies. Cette loi sert à clarifier des situations
       complexes ou à être transigeant, si une partie respecte des normes
       raisonnables. Exemple du « Digital Millennium Copyright Act »
       (DMCA) de 1998 qui sécurise les entreprises d’Internet en cas de
       violation du droit d’auteur par les utilisateurs finaux.
     * Le DMCA a permis à des milliards de personnes de s’exprimer en
       ligne et a donc initié de nouvelles expériences de modèles
       commerciaux.
     * Importance de ralentir le développement des IA pour digérer
       l’impact de ces technologies, en appliquant notamment le « Safe
       Harbor » pour mieux encadrer leur développement. Un « AI Safe
       Harbor » doit être transparent, nourrit fréquemment avec des
       prompts statistiquement significatifs, composé de protocoles de
       confiance et de sécurités documentés pour permettre de contester en
       cas de violation des conditions de services et enfin être
       observable, pour pouvoir être vérifiables de manière privée pour
       mesurer la qualité des résultats.
         ______________________________________________________________

   Il faudrait diriger les entreprises vers des normes communes pour
   maintenir le bien commun, sans pour autant leur faire perdre leurs
   avantages compétitifs. Un « AI Safe Harbor », (en référence à la loi
   Safe Harbor qui détermine les comportements qui n’enfreignent pas une
   règle tant que des conditions précises sont bien suivies), permettrait
   de mieux encadrer l’utilisation des IA en entreprise.
     __________________________________________________________________

   Nicolas Lellouche, « L’épisode de South Park sur ChatGPT est une
   masterclass », Numerama, 13 mars 2023
   (BUTTON) Idées clé

     * Épisode de South Park dans lequel un personnage en CM1 discute avec
       une fille de la classe en utilisant ChatGPT. Tout le monde admire
       son langage romantique développé et ne sait pas qu’il utilise l’IA.
       En parallèle, la classe l’utilise pour rendre des devoirs. Le
       professeur est étonné de leur qualité rédactionnelle mais ne les
       condamne pas. Il utilise à son tour ChatGPT pour corriger les
       copies.
     * À la fin de l’épisode, une voix off lit un synopsis imaginé par
       ChatGPT de l’épisode. L’histoire est incohérente et simpliste :
       c’est une preuve de la non compréhension par les IA des subtilités
       et des critiques pour lesquelles la série South Park est connue.
         ______________________________________________________________

   Un épisode de la série South Park propose une critique de ChatGPT en
   milieu scolaire. On assiste à des situations absurdes, comme un
   professeur qui, face aux rédactions matures des CM1 avec des sujets
   portant sur le « féminisme néolibérale » ou sur « comment œuvrer pour
   améliorer les droits des travailleurs immigrés », ne se rend pas compte
   de l’utilisation de l’IA. L’histoire se conclut par une réécriture de
   l’épisode en question par ChatGPT, qui met en évidence les faiblesses
   de l’IA en proposant un scénario simpliste et moralisateur.
     __________________________________________________________________

   Martin Legros, « ChatGPT, Chomsky et la banalité du mal », Philosophie
   magazine, 13 mars 2023
   (BUTTON) Idées clé

     * Tribune lancée par Noam Chomsky, Ian Roberts et Jeffrey Watumull
       sur ChatGPT, dans laquelle ils questionnent l’essence de la langue,
       de la pensée et de l’éthique à l’ère de l’IA.
     * Il y a un danger de l’IA à nous faire croire qu’elle peut avoir les
       mêmes compétences que les humains, sans passer par l’expérience
       sensible et par la créativité.
     * Noam Chomsky, philosophe à l’origine de la « grammaire générative »
       (méthode syntaxique portée sur l’analyse de l’intuition des humains
       dans leur usage de la langue) déclare que « les hommes disposent
       avec le langage d’une compétence à nulle autre pareille, une
       puissance intérieure de générer et de comprendre, grâce à un nombre
       fini de règles, un nombre infini de propositions qui expriment leur
       pensée. »
     * ChatGPT ne penserait-il pas aussi ? S’il génère du langage,
       pourrait-il générer de la pensée ? Mais ChatGPT semble limité car
       il ne comprend pas le contexte d’une phrase et les subtilités du
       langage.
     * Une IA ne peut pas générer d’énoncé et mettre des règles en place :
       c’est là où, selon Chomsky, que l’intelligence humaine et celle de
       l’IA peuvent se différencier. L’IA est purement descriptive alors
       que l’humain peut expliquer, jouer avec sa connaissance des langues
       lorsqu’il s’exprime.
     * Il y a une « Banalité du mal » chez ChatGPT (plagiat, apathie,
       évitement, etc.) car il ne suit que les ordres. L’IA serait alors
       une « intelligence servile et sans pensée ».
         ______________________________________________________________

   Les limites de l’IA se situent dans sa connaissance partielle des
   langues. Les langues s’ancrent dans des contextes précis et peuvent
   être détournées et transformées pour créer de réponses singulières et
   adaptées, ce dont l’IA ne semble pas faire preuve dans les textes
   générés. Il y a une différence fondamentale entre l’IA et
   l’intelligence humaine car la première manifeste une incompréhension du
   contexte. Elle ne génère que des textes purement descriptifs et est
   incapable de produire des règles et des énoncés.
     __________________________________________________________________

   Rachel Rodrigues, « ChatGPT devient un allié : ces enseignants
   apprivoisent l’intelligence artificielle pour améliorer leurs cours et
   aider leurs élèves », France Info, 10 mars 2023
   (BUTTON) Idées clé

     * ChatGPT a été présenté à une classe de troisième par un enseignant
       en technologie, David Plumel, dans la Nièvre, pour encadrer son
       utilisation et expliquer les défauts et les limites.
     * Une enseignante en français, Cécile Cathelin, propose d’utiliser
       ChatGPT et d’analyser avec les élèves le retour de l’IA. Cela leur
       a permis de comprendre ce qu’ils auraient pu amener en plus ou ce
       qu’ils auraient pu faire mieux que l’IA.
     * ChatGPT peut être un allié s’il est bien utilisé.
     * « Si ce que vous demandez à vos élèves peut être remplacé par une
       machine, c’est que ce que vous faites n’est peut-être pas si
       terrible que ça. » selon Amélie Cordier, docteure en intelligence
       artificielle à l’université de Lyon.
     * Sensibiliser les étudiants aux IA est primordial, mais ce n’est pas
       envisagé par le ministère de l’Éducation Nationale.
     * « L’art du prompt » sera une compétence à acquérir d’ici quelques
       années, déclare l’enseignant Alain Goudey.
         ______________________________________________________________

   L’article traite d’exemples d’intégration de ChatGPT par des
   enseignants en collège. L’IA est perçue comme une assistante
   pédagogique que l’élève peut également mobiliser chez lui pour définir
   des termes ou réexpliquer des points d’un cours. Il y a une nécessité à
   multiplier les mécanismes d’évaluation : par exemple, restituer le
   cours sous formes de graphiques ou de vidéos ou demander aux élèves de
   justifier leurs choix à l’oral
     __________________________________________________________________

   Noam Chomsky, Ian Roberts, Jeffrey Watumull, « Noam Chomsky : the False
   Promise of ChatGPT », The New York Times, 8 mars 2023
   (BUTTON) Idées clé

     * Optimisme et inquiétude envers l’IA, car d’une part l’intelligence
       est ce qui permet de résoudre des problèmes mais, d’autre part, le
       machine learning risque de dégrader nos savoirs et notre éthique
     * Il y a une peur que la machine puisse surpasser l’humain, non plus
       quantitativement mais qualitativement en terme d’intellect ou de
       créativité.
     * Le cerveau humain est radicalement différent des IA et il est
       important de le rappeler. Le cerveau développe inconsciemment,
       automatiquement et rapidement, dès notre plus jeune âge, une
       grammaire et des systèmes de principes logiques sophistiqués.
     * La preuve de la véritable intelligence est dans la capacité de
       critiquer, de prédire, de décrire et d’expliquer. Elle se trouve
       aussi dans la pensée morale qui permet de comprendre les principes
       éthiques et donc détermine ce qui peut être fait, dit ou non.
         ______________________________________________________________

   Comprendre où se trouve la véritable intelligence pour voir les limites
   des IA est essentiel. Critiquer, prédire, décrire et expliquer sont des
   fonctions impossibles à réaliser pour l’IA. Celle-ci n’a pas conscience
   des lois physiques et mécaniques du monde et ne peut pas se corriger
   après avoir émis des explications : elle ne peut pas distinguer le
   possible de l’impossible.
     __________________________________________________________________

   Jacob Browning, Yann LeCun, « AI Chatbots Don’t Care About Your Social
   Norms », Noema, 7 mars 2023
   (BUTTON) Idées clé

     * Question des normes sociales et des échanges : un chatbot peut-il
       reproduire la subtilité et l’émotivité des discussions orales entre
       humains, régies par des normes ? Comment programmer les IA pour
       qu’elles aient des conversations efficaces ? Peut-on programmer la
       spontanéité et l’improvisation des discussions entre humains ?
     * Question du ton, de la voix : manières de dire les choses et
       intonations qui vont avec, chose indétectable pour IA. Les émotions
       et le sensible sont primordiaux dans les échanges.
     * Aspect punitif des échanges : des comportements jugés inappropriés
       demandent à être excusés ou des actions de défenses doivent être
       enclenchées. Les mots doivent être choisis précieusement pour
       rester dans les normes (savoir-vivre).
     * Les chatbots, en prédisant le déroulé d’une conversation
       socialement correcte, peuvent s’adapter aux normes. Or si l’on
       s’écarte des normes sociales, que l’on change le scénario, le
       chatbot est perturbé et devient impolitiquement correct.. On peut
       penser aux individus sur Internet (trolls, complotistes) qui
       peuvent instruire aux chatbots (comme sur Reddit) des choses
       offensantes qui, dans un contexte de conversations entre humains,
       seraient condamnées. Le chatbot, même s’il est entraîné à
       s’excuser, n’a pas la capacité de comprendre le problème (le
       programmeur ne peut pas prévoir toutes les dérives possibles).
         ______________________________________________________________

   Les humains échangent pour obtenir quelque chose et s’assurent de
   correspondre à des normes pour réussir (amour, travail, etc.) alors que
   les IA n’ont pas d’objectif en soi, pas de réputation à tenir. La
   compréhension lacunaire du monde social par les IA ne leur permet donc
   pas de saisir les enjeux relationnels et les manières de s’exprimer qui
   vont avec.
     __________________________________________________________________

   Bérengère Viennot, « ChatGPT et l’IA sont une menace pour les métiers
   de l’écrit mais pas que », Slate, 3 mars 2023
   (BUTTON) Idées clé

     * Il y a une différence entre les traductions « pragmatiques », qui
       requièrent des connaissances, une phraséologie (lexique propre à un
       contexte : une culture, un style, un domaine…) et celles « non
       pragmatiques », qui sont avant tout communicationelles.
     * L’article étudie l’évolution du métier de « traducteur », qui
       devient « réviseur » avec l’apparition des traducteurs
       automatiques.
     * Les traductions pragmatiques utilisent les logiciels depuis
       longtemps, avec des ensembles de textes pour trouver du
       vocabulaire. L’IA peut traduire pragmatiquement car il n’y a pas
       d’ambiguïté dans le langage en raison de sa standardisation. Or la
       traduction non pragmatique est plus compliquée pour l’IA car elle
       demande une sensibilité et une fine compréhension des tournures de
       phrases (l’ironie par exemple) et de la pensée de l’auteur.
     * Le traducteur devient « réviseur » : il doit vérifier la traduction
       automatique et avoir des connaissances fines et un large
       vocabulaire dans les sujets et les langues pour offrir la
       traduction la plus cohérente possible avec le texte original.
     * Le nouveau rôle du traducteur « sera également de nourrir la
       machine et de lui apprendre à traduire », (Nicolas Froeliger), ce
       qui induit une transformation du métier avec un contrôle (partiel)
       sur l’IA.
     * Dès lors qu’une IA donne une « perspective » à partir d’un texte
       traduit, il faut d’assurer que ce ne sont pas des intentions
       humaines derrières, pernicieuses, qui ressurgissent. Le lecteur n’a
       pas accès au texte original, il ne peut donc pas vérifier les
       sources et l’exactitude du texte de l’IA par rapport à l’original.
     * Risque de désinformation : le lecteur ne va pas toujours vérifier
       le texte original. Cela induit un clivage à venir (déjà là ?) entre
       des médias favorisant la qualité des sources et des traductions
       (plus coûteuse), et ceux optant pour une quantité d’informations
       peu fiables et peu précises (moins coûteuses).
         ______________________________________________________________

   Bérengère Viennot se penche sur le devenir de la traduction et du
   métier de traducteur avec les IA. Elle distingue deux types de
   traductions, celles « pragmatiques », qui sont associées à un lexique
   et à des domaines spécifiques et celles « non pragmatiques », qui
   demandent une sensibilité et une compréhension de la finesse des
   langues humaines. Selon elle, le métier de traducteur muterait pour
   devenir « réviseur » des lacunes des IA.
     __________________________________________________________________

   Olaf Avenati, Pierre-Antoine Chardel, Elsa Godart, Éric Guichard, Mara
   Magda Maftei, « ChatGPT, Socrate et moi », Esprit, mars 2023
   (BUTTON) Idées clé

     * Plagiat dans l’apprentissage : il est essentiel de se rappeler que
       les enseignements n’ont pas pour vocation de nous faire apprendre
       une accumulation de connaissances, mais plutôt de nous laisser nous
       les approprier. L’enjeu n’est pas la performance mais bien d’ouvrir
       nos horizons en nous sensibilisant à divers sujets, grâce à un
       encadrement par le corps enseignant.
     * Pensée de Socrate vis-à-vis des IA. Socrate, qui refusait d’écrire
       car le médium livre ne peut pas répondre à des interrogations,
       valorisent la communication orale comme reflet d’une profonde
       humanité de par son imperfection et sa créativité. Les innovations
       technologiques doivent nous faire réfléchir sur les impacts des
       innovations constantes et la nature des acteurs qui régissent et
       alimentent ces systèmes. Aussi, elles doivent nous questionner sur
       la transformation des méthodes d’évaluation par les générateurs de
       textes et sur comment « créer des modes d’appréhension analytiques
       de ces sources ». De plus, les IA donnent une importance au
       collectif dans le savoir, par la pluralité des points de vues et la
       nécessité du débat.
     * L’IA n’a pas conscience du contexte et des sources utilisées, à
       l’inverse des humains qui ont la capacité de dialoguer selon un
       contexte précis et de débattre, de commenter, de citer, etc.
     * Les sources sont indispensables : la « métacognition » (capacité
       d’un individu à savoir comment il développe son savoir et d’ajuster
       ses manières d’apprentissage en fonction des contextes) est
       primordiale pour développer nos connaissances et argumenter.
       ChatGPT échappe à ça : il donc nécessaire de vérifier l’origine et
       la véracité des textes générés par ces algorithmes avant de les
       utiliser.
     * La solitude dans le numérique semble être accentuée par ces bots
       conversationnels : ne pas oublier leur artificialité.
         ______________________________________________________________

   Plutôt que de s’alarmer sur le remplacement des humains par les
   machines, l’article propose de questionner sur ce qui définit notre
   humanité afin de mieux saisir les enjeux pédagogiques avec les IA. Il
   s’agirait d’être attentif aux textes générés par les bots, dont les
   sources sont non traçables et non-vérifiables pour les individus. Les
   IA semblent aller à l’encontre du développement des savoirs et de la
   diversité des points de vue qui sont les valeurs du système pédagogique
   traditionnel.
     __________________________________________________________________

  Février 2023
     __________________________________________________________________

   Adrien Tallent, « Exploitation des données : un changement de contrat
   social à bas bruit », Descript, 28 février 2023
   (BUTTON) Idées clé

     * Les IA sont souvent entourées d’un champ lexical « magique »,
       engendrant ainsi chez certaines personnes une idéalisation de ces
       outils, qui seraient rationnels et responsables.
     * Or, la collecte des données et l’omniprésence de ces algorithmes
       rendent leurs présences dangereuses pour la démocratie et le
       contrat social. Le pouvoir de décision et les actions des individus
       sont donc délégués par un échange de service, la plupart du temps
       gratuit.
     * Terme de « rationalité algorithmique » énoncée par la chercheuse en
       sciences juridiques Antoinette Rouvroy pour parler des IA qui
       répondent avant tout à des choix politiques.
     * Analogie avec le contrat social de Rousseau développé au XVIIIe
       siècle, dans lequel l’échange de la liberté des individus contre un
       ordre politique est questionné. On y retrouve un système similaire
       ici, avec la récolte des données des utilisateurs par les IA, et
       qui en échange leur rendent un service.
     * Avènement d’une « gouvernementalité algorithmique » : décisions
       sont liées à des données et non plus à du droit ou des normes
       sociales. Il y a alors un véritable danger car les statistiques,
       censées être irréfutables, sont à la base de prises de décisions.
     * Nos préférences, nos envies et notre statut sont donc connus par
       ces entreprises : l’essayiste Eli Pariser nomme ce phénomène la
       « bulle de filtre ». Ainsi, la place à l’expérience et aux
       alternatives sont limitées.
     * Les individus deviennent ce que nomme Bernard Stiegler, des
       « fournisseurs de datas », des clients pour les grandes entreprises
       du numérique. Leurs données récoltées servent à les déposséder de
       leur volonté : le désir devient alors automatisé.
     *
     * Or, nous nous sommes habitués à ces systèmes qui nous assurent une
       rapidité et un divertissement en continue : cela accentue donc la
       généralisation et la pérennisation de ces techniques, impactant nos
       libertés et notre vie privée. Nous sommes donc soumis aux prises de
       décisions opaques et autocratiques des IA.
     * Les technologies s’imposent de force dans nos sociétés, malgré une
       conscience des risques liés à ces IA par les politiques. Des normes
       et lois sont rédigées, comme en Europe avec le RGPD (Règlement
       Général sur la Protection des Données). Cependant, les individus
       directement impactés semblent écartés de ces débats etréglements
       (discrimination, notation…). Il est donc plus que nécessaire de
       former les populations aux enjeux du numérique pour faire émerger
       des débats et permettre d’encadrer ces technologies ensemble.
         ______________________________________________________________

   Les enjeux gouvernementaux liés aux IA s’avèrent être plus complexes
   qu’il n’y paraît. Notre habitude face à ces technologies, nous rendant
   un service au détriment d’une récolte de nos données plus ou moins
   consenties, questionne donc nos droits et libertés. Les individus sont
   de véritables produits pour les entreprises. Il est donc primordial
   d’inclure dans les débats politiques les individus et de les former aux
   enjeux du numérique pour encadrer ensemble l’avenir de ces
   technologies.
     __________________________________________________________________

   Ashley Hamer, « How writers can use AI writing tools to be more
   creative (even if they’re scared) », Descript, 23 février 2023
   (BUTTON) Idées clé

     * Article sur l’utilisation de l’IA comme assistant d’écriture.
     * Leanne Leeds, écrivaine, nous dit que l’IA permet d’accélérer son
       travail et apparaît comme une version plus puissante de sa base de
       données.
     * Une ouverture du processus créatif à tout le monde est rendue
       possible par les IA (Jim Hull, animateur Disney et Dreamworks).
     * Exemple de Subtxt : assistant d’écriture associant cadre narratif
       prédictif et génération de texte L’IA comme un tremplin pour se
       lancer dans la création ?
     * Un ajustement est nécessaire avec l’IA pour garder l’intention de
       l’artiste originel. Exemple donné par Jim Hull lorsque l’IA
       participe au storytelling d’un dessin animé adapté d’un livre : le
       programme a tendance à s’écarter du ton de l’œuvre originale et de
       ne pas mettre les actions au bon endroit.
     * Plagiat et droit : penser aux problèmes pour les artistes de leurs
       œuvres reprises par IA et de savoir comment détecter si on s’est
       inspiré, sans le savoir, de créations générées par l’IA.
         ______________________________________________________________

   Des lignes directrices sur l’utilisation de l’IA ont été mises en place
   par l’Alliance of Independant Authors, afin de guider les auteurs dans
   l’utilisation des IA (le guide est accessible sur le site de l’AIA).
   L’article soutient l’idée que, au lieu de rejeter les IA, il faut
   plutôt apprendre à expérimenter avec. Si l’on est suffisamment bon dans
   un domaine l’IA mettra du temps à nous remplacer.
     __________________________________________________________________

   Marine Dessaux, « ChatGPT : de la pédagogie jusqu’à la recherche, des
   remous et des réponses », Campus Matin, 16 février 2023
   (BUTTON) Idées clé

     * Perturbation de la confiance entre enseignant et étudiant par
       ChatGPT. Comment s’assurer que les devoirs rendus n’ont pas été
       faits avec ChatGPT ? Comment adapter les systèmes de notation ? Il
       y a une peur de l’épuisement des enseignants à devoir faire un jeu
       de « cache-cache ».
     * Modalités d’évaluation à repenser : attribuer des coefficients plus
       faibles pour les devoirs maison par exemple.
     * L’interdiction par SciencesPo de ChatGPT est plus de l’ordre de
       l’obligation de la mention de l’usage des IA dans les travaux par
       les étudiants. Un professeur d’économie, Stéphane Justeau (Essca)
       répond qu’il est impossible d’interdire cet outil et qu’il faut
       réussir à évaluer les élèves sur des critères que les IA ne peuvent
       pas remplir. Même avis pour Jacques Fayolle, président de la Cdefi
       et directeur de Mines Saint-Étienne.
     * ChatGPT pourrait servir à « dégrossir des sujets » et « à faire
       gagner du temps dans la rédaction d’une note » selon Médéric
       Gasquet-Cyrus, linguiste, et Léo Garcia, doctorant à l’Université
       Toulouse 1 Capitole.
     * Une formation aux outils d’IA par un groupe de travail de la Neoma
       Business School a été testé. Le but était de faire comprendre aux
       étudiants la désinformation que peuvent engendrer les IA et donc,
       qu’ils ne doivent pas s’appuyer sur ces outils pour préparer leurs
       examens.
     * Andrew Davis, vice-président des communications mondiales
       d’Elsevier, demande l’obligation pour les auteurs de signaler
       l’usage des IA dans leurs textes. « Ces derniers peuvent les
       utiliser pour améliorer la lisibilité et le langage de l’article de
       recherche, mais pas pour remplacer les tâches clés telles que
       l’interprétation des données ou l’élaboration de conclusions
       scientifiques ».
     * Les chercheurs, malgré la pression de la production d’écrits, ne
       doivent pas s’appuyer sur ChatGPT car il ne se base que sur des
       travaux publiés jusqu’en 2021.
         ______________________________________________________________

   L’article traite de diverses réglementations de ChatGPT dans
   différentes institutions scolaires afin de repenser les systèmes
   d’enseignements et les modalités d’évaluation. Plusieurs hypothèses
   d’intégration de l’IA dans les processus d’enseignement sont
   présentées, comme le réajustement des coefficients de notation des
   devoirs maisons, des formations, ou des grilles d’évaluation pour noter
   des compétences que seuls les étudiants peuvent accomplir.
     __________________________________________________________________

   Entretien avec Laurence Devillers par Stéphane Barge, « Bloom,
   l’intelligence artificielle made in France n’a rien à envier à ChatGPT
   », Capital, 10 février 2023
   (BUTTON) Idées clé

     * Succès de ChatGPT est avant tout un coup de marketing selon
       Laurence Devillers car le système de ces IA n’a rien de nouveau :
       la différence se joue dans la puissance de leurs capacités
       d’analyse de données. ChatGPT en emmagasinant des milliards de
       données se montrent donc plus puissants mais n’est en rien
       intelligent : il ne fonctionne que par probabilité.
     * Une IA, nommée Bloom, conçu par trois français et des milliers de
       chercheurs du CNRS et soutenue par des entreprises (Orange, Thalès,
       Airbus…) a été développé en France depuis l’été 2021. Son objectif
       est d’être open-source et plus transparente que les IA des
       entreprises américaines et chinoises.
     * Le pouvoir de manipulation des IA est plus important dès lors
       qu’elles sont opaques car on ne peut avoir accès à leurs
       fonctionnements et aux données qu’elles utilisent (d’où le fait
       qu’il est indispensable d’instaurer des gardes-fous).
     * Les machines intelligentes ne sont pas que source d’angoisse selon
       Laurence Devillers : elles s’avèrent utiles dans le domaine médical
       (détection tumeurs par exemple) ou dans la lutte contre le
       réchauffement climatique (analyse d’images satellitaires pour
       évaluer le gaz à effet de serre).
     * Il faut définir un code d’utilisation des IA pour éviter les
       dérives et par exemple demander aux entreprises d’être
       transparentes sur leur IA. S’il y a un non respect, ces entreprises
       doivent être audités. De plus, il est important de sensibiliser les
       plus jeunes à ces technologies pour éviter des incompréhensions et
       une aliénation.
         ______________________________________________________________

   Laurence Devillers présente le projet Bloom, une IA développée par des
   chercheurs du CNRS et des programmeurs français, qui assure une
   meilleure transparence. Elle traite de l’importance de réglementer au
   plus vite ces technologies pour éviter une trop grande manipulation et
   des dérives. Elle propose de réfléchir aux oefficients de notation des
   devoirs maisons, à des formations ou à des grilles d’évaluation pour
   noter des compétences que seuls les étudiants peuvent accomplir.
     __________________________________________________________________

   Richard Menneveux, « Créatifs, développeurs, enseignants, ils ont fait
   de l’intelligence artificielle leur nouvel outil de travail », Le Club
   FrenchWeb, 10 février 2023
   (BUTTON) Idées clé

   1^er entretien : Etienne Mineur, à propos de workshops réalisés avec
       des générateurs d’images.
     * Il y a un vocabulaire des prompts à apprendre, mais aussi des
       contournements textuels à trouver face à la censure des IA qui
       permettent aux étudiants de faire appel à leur créativité.
     * Il y a un véritable travail à faire sur les formulations permettant
       de générer des images : décrire au maximum ce que l’on veut, mettre
       les mots dans le bon ordre, et ne pas seulement demander de générer
       une image « dans le style de… ». Dès lors que l’étudiant décrit, il
       y a un travail d’analyse sur les spécificités d’un mouvement, d’un
       style ou d’un artiste. L’étudiant apprend ainsi à développer sa
       capacité descriptive et analytique.
     * L’IA est un outil qui ne remplacera pas les humains car certains
       métiers demandent des compétences qu’une IA ne pourra pas
       développer : exemple du designer objet/industriel qui a des
       connaissances techniques (physiques, mécaniques, ergonomiques,
       etc.) et qui travaille en ayant en tête ces contraintes. L’IA n’a
       pas conscience des limites physiques.
     * Attention aux stéréotypes que les IA reproduisent : importance pour
       les étudIAnts de travailler avec et de comprendre les logiques des
       IA pour en tirer le meilleur, sans se faire avoir. 2^e entretien :
       Julien Barbier, fondateur école Holberton School
     * Le besoin permanent de vérification et de retour critique sur les
       sorties d’IA montre qu’elle ne sera jamais autonome et que l’humain
       devra toujours être à ses côtés.
     * Importance du contexte d’apprentissage : si l’étudiant utilise l’IA
       à outrance, il y a alors un véritable danger pour sa propre
       évolution et à l’inverse, interdire IA n’est pas la solution car
       l’étudiant l’utilisera sûrement mais ne saura pas comment
       travailler avec. Il faut donc établir des règles éthiques et les
       expliquer. 3^e entretien : André Manoukian et Philippe Guillaud, à
       propos de leur logiciel MatchTunes
     * La capacité créative des IA est puissante. L’espace-temps singulier
       d’Internet a permis une multitude de nouveaux styles musicaux car,
       peu importe d’où on vient, on peut apprendre des techniques et
       s’enrichir culturellement.
     * L’IA permet de mixer les univers et une expérimentation ouverte :
       MatchTunes est au service de « l’excellence » car il s’axe sur la
       qualité et non la quantité, à l’inverse d’une IA comme Midjourney
       où 5-6 images sont proposées.
     * L’IA peut désencombrer les humains de tâches contraignantes et
       servir la création.
         ______________________________________________________________

   Selon Etienne Mineur et Julien Barbier, les IA ne sont que des outils
   qui ne pourront pas remplacer les compétences humaines que certains
   métiers exigent. Il est nécessaire d’établir des règles d’utilisation
   des IA en milieu scolaire pour permettre une connaissance fine de ces
   derniers, et éviter un usage non réfléchi. Savoir utiliser les IA,
   c’est pouvoir travailler avec, les corriger, contourner leurs limites
   (censure) en faisant appel à des capacités rédactionnelles. L’étudiant
   développe des compétences analytiques et descriptives lorsqu’il rédige
   un prompt. Plus la demande sera fine et descriptive, plus l’IA sera
   apte à générer des réponses originales. Par exemple, au lieu de
   demander « un chat dans le style de Dali » au bot, une formulation plus
   travaillée (où l’étudiant fait appel à ses connaissances sur le style
   d’un artiste en les décrivant dans le prompt) pourrait permettre un
   usage plus conscientisé.
     __________________________________________________________________

   Dominique Boullier, « Sciences Po a eu raison d’interdire ChatGPT »,
   AOC, 7 février 2023
   (BUTTON) Idées clé

     * La notion de « passage à l’acte » est convoquée plusieurs fois pour
       parler de l’immiscion des IA dans les différentes sphères de notre
       société sans discussion préalable (outil anti-démocratique ?)
     * Les termes de « rough consensus » et de « running code » sont
       utilisés par John Perry Barlow dans sa « Déclaration d’indépendance
       du cyberespace » en 1996 dans laquelle il défend l’idée de ne pas
       transposer à ce nouvel espace immatériel les règles et normes de
       nos sociétés.
     * « Capitalisme financier numérique » : il faut inclure les
       innovations techniques dans les sujets de discussions car elles
       sont liées aux questions de droit et de politique.
     * OpenAI adopte la « stratégie du choc », expression reprise du titre
       de l’ouvrage éponyme de 2007 de Naomi Klein, et qui renvoie à la
       puissance qu’à l’ultralibéralisme de forcer les prises de pouvoir.
     * La « tyrannie du retard » créée des systèmes oppressifs entre
       différents pays : ceux qui adoptent en dernier, ou jamais, les
       innovations techniques, sont mis de côté.
     * Le texte établit une analogie du « passage à l’acte » avec d’autres
       techniques qui, comme le TGV (qui passerait derrière chez nous),
       l’usine chimique (et ses odeurs nocives) ou les éoliennes, se
       répandent sans débats ni accord préalable auprès des populations
       (ce qui soulève des problèmes moraux et politiques).
     * Dès lors qu’une technique altère notre mental, il faut se
       questionner sur son « habitèle », ou comment la rendre habitable.
     * Des dispositifs de validation et de contrôle sont indispensables
       pour utiliser l’IA en contexte scolaire.
         ______________________________________________________________

   Dominique Boullier soutient l’interdiction de ChatGPT à Sciences Po
   Paris, en examinant ses conséquences éthiques et politiques. Il traite
   du danger du « passage à l’acte » des entreprises néo-libérales, qui
   figent le débat sur la légitimité de l’usage et de l’intégration de ces
   techniques dans nos sociétés. Pour défendre son propos, il montre
   l’intérêt à ne pas séparer le droit et la politique des innovations
   techniques afin d’éviter une acceptation passive des IA. Selon lui, il
   est primordial de trouver des alternatives ouvertes et plus
   contrôlables, et d’animer des débats pour mieux cadrer l’intégration de
   ces IA en contexte scolaire et éveiller les consciences.
     __________________________________________________________________

   Colin de la Higuera, « L’intelligence artificielle au quotidien :
   quelle position pour l’enseignant.e ? », Chaire UNESCO RELIA, 6 février
   2023
   (BUTTON) Idées clé

     * Nous n’avons pas encore assez de recul sur les IA dans
       l’enseignement même si ce n’est pas une question nouvelle (Google
       Translate).
     * Un projet européen, l’AI4T, a été lancé pour former les enseignants
       de 5 pays européens (dont la France) à l’IA.
     * Il est important d’analyser les logiciels non éducatifs intégrant
       l’IA, qui peuvent être utilisés pour de l’apprentissage. Exemple de
       BLOOM, conçu par plusieurs pays, décrit comme une solution
       réellement ouverte et conçue contre GPT-3.
     * Il ne faut pas rejeter ChatGPT mais bien trouver les contournements
       pour le piéger, même s’il se développe constamment. Cela demandera
       de suivre ces évolutions.
     * La triche avec les machines n’est pas nouvelle : les calculatrices
       ou Wikipedia nous le prouvent mais ils ont aussi montré qu’ils
       étaient avant tout des outils pratiques.
     * ChatGPT sera inévitablement utilisé pour aider à faire le travail,
       comme par exemple pour réaliser des QCM ou des plans de cours, mais
       cela ne peut être considéré comme de la triche. La question est
       plus complexe lorsque ChatGPT est utilisé pour réussir un examen.
     * On assiste à deux débats. Le premier porte sur la nécessité de
       former l’élève au futur en lui enseignant les nouveaux outils. Le
       système scolaire doit s’adapter à ces évolutions et ne pas rester
       dans des systèmes académiques dépassés, qui ne sont plus adaptés
       aux contextes actuels. Le second débat interroge l’évaluation
       sommative et ses modes de notation, qui seraient à repenser avec
       l’émergence des IA. Ces deux débats réunis s’annulent car « “former
       à utiliser l’IA” devient “former des futurs tricheurs et
       tricheuses” » et, inversement, « “interdire l’usage de ces
       technologies” conduit à ne pas former au monde de demain ».
         ______________________________________________________________

   Le corps enseignant ne doit pas rejeter les IA mais plutôt les essayer
   et débattre avec les élèves sur les réponses générées. Pour intégrer au
   mieux ces technologies dans le milieu scolaire, il faudrait d’abord se
   questionner sur ce qui est de l’ordre de la tricherie ou non :
   « Vis-à-vis du système mis alors en défaut ? Vis-à-vis des autres
   élèves, car il n’y aurait pas une égalité ? Vis-à-vis de l’élève qui
   n’est pas en train d’apprendre tout en obtenant des résultats
   trompeurs ? ». Une fois ces questionnements éclaircis, l’IA dans
   l’enseignement pourrait aider l’étudiant, par exemple, à reformuler, à
   développer un sens critique, à lutter contre le syndrôme de la page
   blanche, et permettre de contrôler la véracité des informations.
     __________________________________________________________________

  Janvier 2023
     __________________________________________________________________

   Chris Ré, « AI’s Linux Moment: An Open-Source AI Model Love Note »,
   Hazy research, 30 janvier 2023
   (BUTTON) Idées clé

     * L’IA vit son moment Linux : débuts de l’IA open source.
     * L’open source a joué un rôle majeur dans l’informatique (cf. Whole
       Earth Catalog) : il a été initié par des communautés soucieuses
       d’améliorer le monde par des logiciels libres (Linux).
     * Repenser ce mouvement avec la démocratisation des IA : elles ont
       connu une forte renommée et évolution grâce à des communautés de
       gens passionnés qui ont conçu des IA en accès libre (via Discord,
       GitHub, etc.).
     * De nombreux exemples d’amélioration par la communauté : PuTorch
       (Meta), Keras (Google), Transformers (Hugging Face), etc.
     * On observe un fort enthousiasme chez les individus car les IA sont
       applicables au quotidien, accessibles, et permettent de créer des
       applications à partir de ces modèles.
     * Les entreprises pourront développer leurs propres outils grâce à la
       démocratisation de ces logiciels et partir de leurs propres données
       pour les adapter à chaque structure.
         ______________________________________________________________

   L’article soutient l’idée d’une évolution de l’IA, par l’open source et
   la communauté pour démocratiser les connaissances sur cette
   technologie. Avec l’open source, les connaissances sont diffusées,
   partagées, et permettraient aux entreprises de développer leurs propres
   IA, adaptées à leurs besoins.
     __________________________________________________________________

   Gleinn Kleiman, « Teaching Students to Write with AI : The SPACE
   Framework », Medium, 5 janvier 2023
   (BUTTON) Idées clé

     * L’article propose plusieurs étapes et instructions à suivre pour
       assurer une utilisation pertinente des IA pour le corps étudiant
       via l’acronyme « SPACE: a framework for writing with AI tools » :
     * Set directions qui définit le but, le contenu et la cible du texte
       écrit pour ajuster la place de l’IA dans le texte, c’est à-dire,
       lui donner un rôle plus ou moins important dans la rédaction. Par
       exemple, l’IA complète ou corrige les fautes mais l’humain rédige
       la majeure partie du texte.
     * Prompt, qui assigne à l’IA une tâche spécifique dans le texte, au
       lieu de le laisser tout rédiger.
     * Assess, qui évalue les résultats de l’IA pour les ajuster :
       exactitude, exhaustivité, partialité, etc.
     * Curate, pour conserver le texte généré par l’IA et ensuite
       sélectionner et organiser des extraits de textes d’IA ou d’humains
       pour rendre cohérent l’ensemble. L’intérêt est de travailler avec
       différentes versions de textes pour varier les propositions et
       choisir parmi plusieurs possibilités.
     * Edit, qui édite des contributions humaines et non humaines pour
       produire un texte bien organisé et formulé.
     * L’IA demande de nouvelles compétences à acquérir pour le corps
       étudiant, d’où la nécessité de le guider pour éviter des
       utilisations inefficaces.
     * Il est important de décrire les étapes du processus d’écriture pour
       permettre à l’étudiant un recul critique.
     * Quelques exemples montrent de quelles manières les humains peuvent
       être impliqués dans les étapes de la méthode SPACE : par exemple le
       texte « A robot wrote this entire article. Are you scared yet,
       human? », paru dans The Guardian en septembre 2020. Cet article a
       été composé à partir de huit textes générés par GPT-3. Les parties
       les plus intéressantes ont été conservées afin de proposer un texte
       avec une variété de styles. Par la suite, l’équipe a fait un
       travail de recomposition du texte en coupant des phrases et des
       paragraphes.
     * Il est essentiel de limiter l’usage des IA de génération de textes
       pour éviter au maximum des textes pauvres et indésirables. Les
       étudiants doivent apprendre à reconnaître et à résoudre les erreurs
       de textes pour appliquer au mieux les étapes SPACE. Quelques
       exemples de limites de l’IA
     * L’émotion et l’IA : il ne faut pas oublier que l’IA n’a pas
       conscience de la profondeur d’un texte, des tournures de phrases,
       du contexte, du sens, des intentions portées par un texte. Le texte
       peut vite devenir insipide. L’impact émotionnel du texte sur les
       humains ne doit pas être négligé
     * Les systèmes discriminatoires : IA peut générer du texte stéréotypé
       voire discriminatoire.
     * La temporalité et véracité : l’IA met du temps à produire du texte
       sur des phénomènes récents car il n’y a pas encore assez d’éléments
       lui permettant d’analyser l’événement. Cela peut amener à des
       textes incomplets, dépassés et hors sujet, ou à l’invention de
       références par l’IA.
     * L’éducation et les IA ont deux approches opposées : « Embrace AI
       Tools » et « Resist AI Tools » (interdisant strictement
       l’utilisation des IA à l’école). L’approche « Embrace AI Tools »
       reconnaît les forces et limites des IA, qui peuvent aider par
       exemple à corriger les erreurs d’écriture des élèves. Il faut faire
       une utilisation limitée, justifiée et équilibrée des IA
       (traduction, suggestion de phrases). Faut-il n’autoriser l’IA que
       dans l’enseignement supérieur ?
     * « Je ne sais pas ce que je pense jusqu’à ce que je l’écrive » (Joan
       Didion) et « J’écris car je ne sais pas ce que je pense jusqu’à ce
       que je lise ce que j’ai dit » (Flannery O’Connor).
     * La question centrale est de savoir ce qu’est l’expertise à l’ère de
       l’IA, et comment préparer au mieux les étudiants à utiliser ces
       technologies pour enrichir leurs vies.
         ______________________________________________________________

   Gleinn Kleimann propose de guider les étudiants sur l’utilisation des
   IA, avec un principe qu’il nomme « SPACE », dont chaque lettre est
   associée à une étape à suivre : Set directions, Prompt, Assess, Curate
   et Edit. L’idée est d’apprendre à travailler en collaboration avec ces
   IA pour les intégrer dans les processus de rédaction, sans pour autant
   leur laisser une place prédominante. Il est essentiel de rappeler
   l’absence d’émotion des IA, dont les textes générés peuvent impacter
   les individus. De plus, la temporalité des sources utilisées par ces
   technologies questionne la véracité et l’actualité des informations
   produites.
     __________________________________________________________________

  Décembre 2022
     __________________________________________________________________

   Fabian Suchanek, Gaël Varoquaux, « Beau parleur comme une IA », The
   Conversation, 26 décembre 2022
   (BUTTON) Idées clé

     * L’IA semble avoir du mal à comprendre le positionnement des mots
       dans les phrases et le sens attribué. Elle ne comprennent pas le
       sens et la fonction de chaque mot, et l’importance de leur
       positionnement dans la phrase. L’IA fait preuve de pur mimétisme de
       textes rédigés par des humains.
     * Les « transformeurs » (paradigme à la base du deep learning
       présenté en 2017 dans l’article « Attention is All You Need »,
       utilisé majoritairement dans le traitement du langage naturel) ne
       permettent qu’une compréhension de la demande dans l’ordre
       d’écriture, et ne distinguent pas les séquences paires et impaires.
       Exemple : « La lumière est éteinte. J’appuie sur l’interrupteur
       d’éclairage. Je mange une pizza. J’appuie sur l’interrupteur
       d’éclairage. La lumière est-elle allumée ? ».
     * Les modèles de langage ont des problèmes avec la négation et les
       raisonnements complexes car ils ne sont pas faits pour mémoriser et
       comprendre des données massives. Par exemple, à l’énigme de
       l’examen national des fonctionnaires chinois : « David connaît
       l’ami de M. Zhang, Jack, et Jack connaît l’amie de David, Mme Lin.
       Tous ceux qui connaissent Jack ont une maîtrise, et tous ceux qui
       connaissent Mme Lin sont de Shanghai. Qui est de Shanghai et a une
       maîtrise ? », seulement 45% des IA répondent juste alors que les
       humains répondent juste à 96%.
     * L’IA ne sait pas si elle est « juste » ou non, donc des réponses
       fausses et/ou inventées sont générées. Celles-ci prennent de
       « belles » formes textuelles et une apparence de vraisemblance, ce
       qui peut poser problème pour des individus qui accordent, dès lors,
       une confiance aveugle à l’IA.
     * Attention, il ne faut pas penser que ces modèles de langage sont
       incompétents : ils ne sont pas à l’heure actuelle assez performants
       pour faire des raisonnements et conserver des données fiables.
     * Les « représentations symboliques » restent le modèle utilisé par
       les IA actuelles, Elle fonctionnent en stockant les données comme
       des ensembles d’entités et font des relations entre elles. Pour
       assurer la véracité des relations établies, des règles et des
       contraintes sont appliquées.
     * Les modèles de langages sont un moyen pour construire ces
       «représentations symboliques», en construisant des bases de
       connaissance par l’analyse d’articles.
     * Les « modèles de langage », qui analysent et génèrent des textes en
       langage naturel sont donc hybridés avec les «représentations
       symboliques», qui assurent le stockage d’éléments et qui peuvent
       raisonner sur ces derniers.
     * Pour mieux comprendre ce fonctionnement, l’article fait une
       analogie avec le cerveau humain qui peut autant raisonner
       intuitivement et rapidement (lecture, additions simples) que
       réfléchir sur des points abstraits et complexes (mémoriser des
       numéros, comparer, etc.).
         ______________________________________________________________

   L’article analyse les forces et faiblesses de la compréhension des IA
   dans la structuration des phrases. D’une part, les IA ont encore du mal
   à comprendre l’ordre logique et le sens des mots et ne peuvent donc pas
   raisonner sur des textes trop complexes et utilisant beaucoup de
   négations. Or, d’un autre côté, ces IA se montrent très performantes
   dans le stockage de bases de connaissances et dans l’analyse de textes
   plus ou moins simples.
     __________________________________________________________________

   Mathilde Fontez « L’intelligence artificielle multiplie les exploits »,
   franceinfo, 18 décembre 2022
   (BUTTON) Idées clé

     * IA, AlphaCode, capable de créer des programmes informatiques. Elle
       a été développée par Alphabet, dont Google s’avère être une
       filiale.
     * Il y a une pénurie mondiale des programmeurs : ce genre d’IA
       pourrait donc s’avérer utile car elles peuvent manipuler les
       langages de programmations de manière quasi humaine.
     * AlphaCode a été entraîné avec une multitude de lignes de code et a
       appris de multiples langages.
     * Si l’IA peut coder, alors les IA pourront-elles se modifier toutes
       seules ? Elles pourront éventuellement corriger des erreurs dans
       leur code mais cela reste encore fictionnel. Il faut néanmoins dès
       à présent, établir des garde-fous et des règles pour éviter
       qu’elles n’échappent à l’Homme.
         ______________________________________________________________

   L’article présente AlphaCode, une IA génératrice de programmes,
   remettant donc en question les limites de cette technologie. Dès lors
   qu’une IA peut se programmer toute seule, elle pourrait donc échapper à
   la main de l’Homme. Ce scénario, qui reste encore fictionnel, doit
   cependant alerter et faire comprendre l’importance d’instaurer des
   garde-fous et règles.
     __________________________________________________________________

   Stephen Marche, « The College Essay Is Dead », The Atlantic, 6 décembre
   2022
   (BUTTON) Idées clé

     * Les « méthodes d’apprentissages » : exemple d’un essai écrit par un
       IA en mai 2022 et commenté par Mike Sharples, professeur au
       Royaume-Uni. Le but de Sharples était d’alerter les enseignants sur
       la nécessité de « repenser l’enseignement et l’évaluation » à l’ère
       de ces technologies.
     * Des étudiants s’aident de l’IA pour améliorer des compétences
       qu’ils ont du mal à faire progresser, comme la qualité
       rédactionnelle, car ChatGPT n’est pas « quelqu’un » mais un
       programme. Certains élèves considèrent donc que demander de l’aide
       à un IA n’a pas les mêmes conséquences que d’être aidé par un
       humain.
     * L’essai est historiquement le moyen d’enseigner aux élèves
       « comment rechercher, penser et écrire ». Or les IA se montrent
       meilleures que la plupart des élèves.
     * Publié en 1950, l’essai The Two Cultures rédigé par C. P. Snow
       traite de la perte de lien entre les humanités et les sciences. Les
       deux domaines font preuve d’incompréhensions mutuelles et de
       visions erronées. Dans un monde animé par les innovations
       technologiques, l’éducation aux humanités se perd.
     * Les médias sociaux jouent un rôle capital dans l’accentuation de
       l’ignorance des questions de société et d’histoire. Exemples de
       Mark Zuckerberg (ancien lecteur de César) ou d’Elon Musk (ingénieur
       talentueux) qui font pourtant preuve de bassesse d’esprit sur les
       médias sociaux.
     * L’échec provient d’un « esprit délibéré » et non mesquin ou avide.
       Les ingénieurs devraient considérer les questions humanistes dans
       leurs projets (comme « l’herméneutique, la contingence, la liberté
       d’expression ou la généalogie des morales ») qui sont étroitement
       liées aux IA. Il ne faut pas penser aux problèmes complexes comme
       s’ils étaient compréhensibles pour tout le monde.
     * Les humanistes ont intégré dans leurs réflexions les innovations
       technologiques, à l’inverse des ingénieurs. Or la compréhension des
       technologies est généralement relative par les humanistes en raison
       d’un manque d’actualisation de leurs approches. Il ne faut plus
       penser avec le paradigme de la « méta-narration », principe
       introduit en 1979 par le philosophe Jean-François Lyotard pour
       parler des récits expliquant la légitimité des systèmes
       institutionnels (savoir-vivre, justice, dialectique, etc.).
     * Il y a une importance des humanistes à expliquer l’importance de la
       langue (style, ton, éloquence, histoire, systèmes éthiques, etc.)
       dans un monde centré sur la technologie, et ce face à une baisse de
       diplômés en sciences humaines. « Plus la compréhension de
       l’expérience humaine est large, meilleure sera la conception »
       disait Steve Jobs à l’époque où il a abandonné ses études pour
       découvrir les arts (danse, calligraphie, théâtre).
     * Il y a autant d’étudiants inscrits en informatique que dans toutes
       les sciences humaines réunies. Avec GPT-3, le traitement du langage
       naturel est questionné. La base de l’évaluation des étudiants étant
       la dissertation (examens, thèse, doctorat, etc.), quels impacts les
       IA vont-elles avoir sur les modes d’apprentissages et
       d’évaluation ? Il faudra attendre 10 ans avant que ces IA soient
       réellement ancrés dans monde universitaire : « Deux ans pour que
       les étudiants comprennent la technologie, trois autres années pour
       que les professeurs reconnaissent que les étudiants utilisent la
       technologie, et enfin cinq ans pour que les administrateurs
       universitaires décident, le cas échéant, de ce qu’il faut faire à
       ce sujet. »
     * Les ingénieurs et humanistes vont se rencontrer avec IA car les
       informaticiens ont besoin d’avoir des bases sur le langage, la
       sociologie et l’éthique.
         ______________________________________________________________

   La baisse d’étudiants inscrits en sciences humaines (et leur
   augmentation en informatique) questionne la place des humanités. Le
   dialogue entre les humanités et les sciences est essentiel pour
   réfléchir de manière transdisciplinaire aux conséquences éthiques et
   morales des IA de traitement du langage naturel dans l’enseignement.
     __________________________________________________________________

  Décembre 2021
     __________________________________________________________________

   Meredith Whittaker, « The Steep Cost of Capture », SSRN,
   Novembre-Décembre 2021
   (BUTTON) Idées clé

     * Critique des entreprises privées qui se faufilent dans nos vies et
       institutions et qui façonnent insidieusement l’accès aux
       informations.
     * Critique des entreprises privées qui se faufilent dans nos vies et
       institutions et qui façonnent insidieusement l’accès aux
       informations.
     * Essor des IA depuis une dizaine d’années possibles par la
       concentration des données dans des entreprises privées : IA sont
       dépendantes de systèmes commerciaux.
     * Ces entreprises dirigent ce que nous devons savoir et ne pas savoir
       sur les IA et sur comment nous pouvons interagir avec.
     * Elle fait un lien sur le rapport des sciences avec le militaire,
       notamment pendant la Guerre Froide, avec l’influence de l’industrie
       technologique sur les IA. Elle rappelle aussi que l’armée
       américaine a grandement participé à contrôler la diffusion des
       connaissances et à punir les dissidents.
     * Industrie technologique actuelle est sous pression, à cause de
       réglementations, figeant les alternatives et aseptisant les débats
       donc il y a privation pour les communautés de premières lignes, les
       politiques de s’exprimer sur les coûts et les conséquences des IA
       et de dénoncer les responsables.
     * Question de comment et pourquoi l’IA s’est-elle développée depuis
       70 ans malgré plusieurs “hiver de l’IA” et pourquoi elle a pris de
       l’ampleur ces dernières décennies ?
     * Mutabilité du terme d’IA évidente face à ces questions et
       problématiques sur la centralité de la concentration de
       l’information.
     * AlexNet, algorithme créé à Toronto en 2012, qui a gagné le grand
       défi de reconnaître visuellement une multitude d’images sur
       Internet. Événement marquant, montrant la force des d’algorithmes
       supervisés mais aussi ce que peut faire l’IA avec des données
       informatique à grande échelle. AlexNet a donc ouvert la voie aux IA
       que nous connaissons actuellement, enrendant compte de la puissance
       et de leurs adaptations dans de nombreux domaines possibles.
     * Les recherches sur les IA ne sont pas toujours du ressort des
       chercheurs individuels car, elles dépendent de financements qui
       sont du côté des entreprises IA.
     * Modèle de fondation : terme inventé par Stanford pour le lancement
       du CRFM (Center for Research on Foundation Models) pour parler des
       IA entraînées sur de gros volume de données et qui peuvent être
       adaptées à plusieurs tâches. Ces modèles sont parmi les plus riches
       en termes de données et d’informations dans le domaine l’IA et donc
       les plus favorisés par les entreprises. (GPT-4, DALL.E etc.).
     * S’appuyer sur le monde universitaire et sa diversité de domaines
       pour élargir l’accès à la recherche sur l’IA.
     * Dès lors que les entreprises font parties des infrastructures de
       recherches et appellent à encadrer les recherches sur l’IA, alors
       il y a une extension de leur pouvoir et domination car elles ont la
       mainmise sur ces recherches.
     * Danger de laisser les entreprises et centre de recherches définir
       les conditions d’utilisation, leurs paramètres d’équité.
     * Question des limites des ressources informatiques en milieu
       universitaire qui empêchent de travailler avec de puissants outils
       informatiques. Ainsi, des écoles décident de s’allier avec des
       entreprises, malgré toutes les conditions tacites qui vont avec.
     * Liberté académique de dénoncer et demander des comptes aux grandes
       entreprises paraît presque impossible : les entreprises peuvent
       mettre tout en place pour renvoyer des individus et discréditer
       l’institution. La critique est donc neutralisée pour éviter les
       `opinions dissidentes allant à l’encontre, accentuant le pouvoir et
       la domination de ces entreprises. (cf. « Timnit Gebru Is Calling
       Attention to the Pitfalls of AI »
         ______________________________________________________________

   Ce texte accuse les géants de la tech des IA de neutraliser les
   critiques en dénigrant les approches dissidentes et en finançant les
   critiques les plus faibles. La mainmise par ces entreprises sur les IA
   rend donc les contestations et les modifications de ces systèmes plus
   compliquées.
     __________________________________________________________________

  Mars 2021
     __________________________________________________________________

   Emily M.Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret
   Shmitchell, « On the Dangers of Stochastic Parrots: Can Language Models
   Be Too Big ? », ACM Digital Library home, 1^er mars 2021
   (BUTTON) Idées clé

     * Article co-écrit par des chercheur.euses employés chez Google, mais
       considéré comme gênant par l’entreprise, il a été demandé d’enlever
       les noms des personnes ou de supprimer l’article. Opposée à cette
       demande, Timnit Gebru, une des chercheuses, a été renvoyée de
       Google.
     *
     * Il y a un développement fort des modèles des LM (Language Model)
       depuis les années 2018 et des concurrences aussi pour produire de
       plus en plus de puissants modèles. Le concept de LM remonte aux
       débuts de l’informatique : les premiers succès étaient la
       reconnaissance automatique de la parole (ASR) et la traduction
       automatique (MT) début des années 1980. La puissance de calculs
       évoluant au fil du temps a amené aux transformeurs (2017), que
       nous avons maintenant, qui permettent de mieux contextualiser les
       séquences d’entrée. Les LM actuels peuvent s’appliquer dans
       plusieurs domaines, s’adapter et contenir des millions
       de paramètres.
     * Dans l’article, les chercheurs.euses se demandent si on a
       suffisamment réfléchi aux risques potentiels de ces modèles ?
     *
     * Premier risque évoqué : celui écologique. Coûts et travaux liés à
       ces technologies à établir au préalable pour mieux préparer leur
       installation et minimiser les impacts. L’augmentation des impacts
       environnementaux et des coûts financiers accentue la fracture
       numérique et les discriminations f : les territoires marginalisés
       seront ceux qui seront le plus impactés par ces technologies. Il y
       a aussi un problème de marginalisation linguistique par l’usage
       de l’anglais quasi-systématiquef, qui accentue cette fracture. Il
       est moins chers et plus « facile » de faire appel à des pays en
       sous-développement pour la main d’œuvre, l’installation
       des serveurs de ces IA etc. (Cf. Antonio Casillif, « Malaise dans
       l’éthique de l’IA. Conflictualités sociales et environnementales
       autour de l’automation intelligente », conférence à l’université
       de Genève en compagnie de Nicolas Nova et de Claire Balleys, 19
       avril 2023, visible dans cet état de l’art). De plus, une étude de
       l’Université du Massachusetts a déterminé qu’un grand modèle d’AI
       émettra près de cinq fois les émissions à vie d’une voiture
       américaine classique.
     *
     * Autre risque étudié : financier. Il y aura une augmentation de 0,1%
       du score BLEU (mesure utilisées pour évaluer la traduction
       automatique)  qui coûtera environ 150 000$ en puissance de calcul.
     *
     * Utiliser les énergies renouvelables pourraient atténuer ces impacts
       mais leur utilisation est limitée et leurs coûts environnementaux
       sont néanmoins existants.
     *
     * Un potentiel problème supplémentaire est celui de l’incapacité à ne
       pas gérer les données d’entraînement . Les points de vue
       hégémoniques sont les plus présents dans ces algorithmes car ce
       sont ceux qui ont le plus accès au numérique. Par exemple, 67% des
       utilisateurs de Reddit sont des hommes américain et qui ont entre
       18 et 29 ans et sur Wikipédia, les éditeurs ne sont que 8,8 à 15% à
       être des femmes.
     *
     * Aussi, l’exclusion de contenus dits indésriables (pornographie,
       violents etc) est à la fois une bonne chose mais exclus des
       discussions où l’on parle positivement de ces sujets. Par
       exemple, des espaces en ligne LGBTQ où l’on parle positivement
       d’expériences vécues vont donc être masqués. Une fois de plus les
       identités marginalisées positives sont écartées et les points
       de vue hégémonique surreprésentés. Le défi est donc de créer des
       falgorithmes plus flexibles et dont les systèmes de modération et
       de contrôle intègrent plus de diversité.
     *
     * Or, une question se pose : se pourrait-il que ces technologies ne
       soient tout simplement pas à la hauteur d’un déploiement global et
       unique ? Une plus grande contribution locale,  en contextualisant
       et définissant mieux les données à récolter et à traiter,
       serviraient à rendre plus inclusives ce système. Les sources
       seraient donc plus vérifiables et les atténuations potentielles
       plus faciles à développer.
     *
     * Théorie linguistique moderne reposant sur «  l’activité conjointe
       », terme énoncé par Herbert H.Clark, qui signifie que nous parlons
       des activités communes avec les autres, au lieu de  décrire les
       actes de parole. Le langage n’est que secondaire : il n’est qu’un
       moyen d’atteindre une fin. Les interlocuteurs travaillent ensemble
       pour arriver à une compréhension commune. Le langage n’est qu’un
       indice parmi tant d’autres, communiquant l’intention. Comparons
       cela avec les LM, système pour assembler au hasard des formes
       linguistiques à partir de vastes données, sans aucun contexte.
       C’est là où l’expression « perroquet stochastique » intervient : un
       perroquet répète ce qu’il entend sans en comprendre le sens. Les LM
       créent simplement un modèle auquel l’humain applique une
       signification. C’est cela qui induit tous les problèmes
       discriminatoires, de désinformation etc.
     *
     * Comment atténuer ces dommages ? Tout d’abord, une meilleure
       planification et évaluation de ces modèles, comprenant le calcul de
       l’efficacité énergétique et de calcul et une meilleure sélection
       des données à traiter seraient nécessaires. Tout cela doit être
       aussi documenté : utilisateurs potentiels, motivations derrières
       ces IA, analyses « pré-mortem » (envisager les pires scénarios pour
       anticiper les risques)… Le développement de langages à faibles
       ressources est une piste intéressante  : moins coûteux, plus
       spécialisés, ces LM ont de nombreux avantages.
     *
         ______________________________________________________________

   Cet article rédigé par quatre chercheur·euses de Google, Emily Bender,
   Timnit Gebru, Angelina McMillan-Major et Margaret Mitchell, traite des
   différents problèmes liés au développement des LM (model language).
   Licencié.es à la suite de cet article par Google, iels alertent sur les
   enjeux écologiques, éthiques, linguistiques et discriminatoires que ces
   algorithmes engendrent. Elles utilisent l’expression de « perroquet
   stochastique » pour parler de ces technologies illusoires : elles se
   contentent de répéter sans saisir la sémantique de leurs propos. Une
   meilleure planification et évaluation en définissant les enjeux, les
   cibles et les données à sélectionner est nécessaire.

   ← Document précédent

   « Filtrer un CV scientifique ? »

Newsletter

   ____________________ S’abonner

Rechercher dans ce site

   ____________________ Rechercher

   Site propulsé par Kirby CMS
   Réalisation : Marine Illiet & Anthony Masure
   Polices : Skolar Sans Variable & Input Sans Narrow
