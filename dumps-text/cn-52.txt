 
Skip to content
[fli_logo_w]

  • Our mission
  • Cause areas
     
    Cause area overview
     
    Artificial Intelligence
     
    Biotechnology
     
    Nuclear Weapons
  • Our work
     
    Our work overview
     
    Policy
     
    Futures
     
    Outreach
     
    Grantmaking
   
    Featured projects

    UK AI Safety Summit
    Policy
    Strengthening the European AI Act
    Policy
    Imagine A World Podcast
    Futures
    Artificial Escalation
    Futures
     
    Our content
    Articles, Podcasts, Newsletters, Resources, and more.
  • About us
     
    About us overview
     
    Our people
     
    Careers
     
    Donate
     
    Finances
     
    FAQs
     
    Contact us
  • Take action

Search for: [                    ] [Search]
 
Take action

 1. Home »
 2. Benefits & Risks of Artificial Intelligence Chinese

Benefits & Risks of Artificial Intelligence Chinese

Published:
April 1, 2017
Author:
Revathi Kumar
[artificial]

Contents

人工智能的益处和风险


“我们对文明所爱的一切都是智慧的产物，所以用人工智能增强人类智能有促进文明走向前
所未有的兴盛的潜力。但，前提是能我们保持这项技术有利无弊。”

 未来生命研究所总裁马克斯·泰格马克

Click here to see this page in other languages: English US_Flag French ● German
● Japanese  ●  Korean  ●  Russian●

什么是人工智能？


从SIRI到自动驾驶汽车，人工智能（AI）正在迅速地发展。虽然科幻小说经常将人工智能
描绘成具有类人特性的机器人，但人工智能可以涵盖从谷歌（Google）的搜索算法，IBM沃
森 (人工智能程序)，到自动武器的任何科技。

当今的人工智能被正确地称为专用人工智能（或弱AI），因为它被设计来执行的任务范围
狭窄（例如，仅执行面部识别，或只进行互联网搜索，或仅驾驶汽车）。然而，许多研究
人员的长期目标是创建通用人工智能（AGI或强AI）。虽然专用人工智能能在其特定的任务
上超越人类，如下棋或解方程，AGI将在几乎所有认知任务上超越人类。

为何研究人工智能的安全性？


在短期内，保障AI对社会有益的科研范畴可涵盖诸多领域如经济学和法学，以及验证法、
有效性计算、安全性和控制论等技术层面课题。如果安全保障仅限于防止你的笔记本电脑
宕机或被黑客入侵，就算出了问题也只是给你添点小麻烦。但当AI系统控制汽车、飞机、
心脏起搏器、自动交易系统或电网时，我们必须保证该系统完全遵照我们的指令，否则后
果将不可设想。另一个短期挑战是如何防止自主武器的毁灭性军备竞赛。

从长远来看，一个重要的问题是，如果我们成功创造了能在所有认知功能上超过人类的通
用人工智能，将会发生些什么。正如约翰·古德（Irving John Good）在1965年所说的，设
计更有智慧的AI系统本身就是一个认知任务。这样的系统有潜力执行递归式的自我完善，
触发智能爆炸，并远远地超越人类智力。通过发明革命性的新技术，这样的超级智能可以
帮助我们消除战争，疾病和贫困。因此，创造强AI可能是人类历史上最重大的事件。然而
，一些专家表示担心，人类的历史也会随着此类的超强大AI的诞生戛然而止，除非我们学
会在AI成为超级智能之前即可使其与我们同心所向。

有些人质疑强AI是否会实现，也有些人坚信创建超级智能对人类必定是有益的。FLI承认这
两种情形皆有可能，但我们更意识到人工智能系统具有有意或无意造成巨大危害的潜力。
我们相信今日的研究意在防患于未然，使得未来人们受益于AI的同时，规避潜在风险隐患
。

人工智能如何可能制造危险？


大多数研究人员同意，一个超智能AI不可能产生人类的情感，如爱或恨。他们也认为人们
没有理由期望AI有意识地趋善向恶。当考虑AI如何成为风险时，专家认为有两种情况最有
可能发生：

 1. AI被设计执行毁灭性任务：自主武器（autonomous weapon）是为杀戮而生的人工智能
    系统。若落入恶人手中，这些武器很容易便可造成大量伤亡。此外，AI军备竞赛也可
    能无意中引发AI战争，导致大量伤亡。为了避免被敌对势力从中阻挠，这些武器的“关
    闭”程式将被设计得极为复杂，因而人们也可能对这种情况失去掌控。这种风险虽然也
    存在于专用人工智能（narrow AI）中，但随着AI智能和自驱动水平的提高，风险也会
    跟着增长。
 2. AI被开发进行有益的任务，但它执行的过程可能是具破坏性的：这可能发生在我们尚
    未达到人类和人工智能目标的一致性（fully align），而解决人类和人工智能目标一
    致性的问题并不是一件容易的事。试想，如果你召唤一辆智能车将你以最快速度送到
    机场，它可能不顾一切地严格遵从你的指令，即使以你并不希望的方式——你可能因超
    速而被直升机追逐或呕吐不止。如果一个超智能系统的任务是一个雄心勃勃的地球工
    程项目，副效果可能是生态系统的破坏，并把人类试图阻止它的行为视为一个必须解
    除的威胁。

以这些例子来看，我们对高级AI的担忧并非在于其可能产生恶意，而是它可能达到的能力
。超智能AI将非常善于完成其目标，如果这些目标不与我们的目标形成一致，我们麻烦就
大了。正比如，你可能并非对蚂蚁深恶痛绝，也没有踩死蚂蚁的恶意，但如果你负责水电
绿色能源项目，而项目开发的所在地区有一个蚁丘会因为大水而被淹没，那只能说是算蚂
蚁倒霉罢了。人工智能安全研究的一个关键目的就是要避免将人类置于和这些蚂蚁同样的
位置。

为何人们最近对AI安全感兴趣？


史蒂芬·霍金、伊隆·马斯克、史蒂夫·沃兹尼亚克、比尔·盖茨等许多科技大腕和许多领导
AI研究的科学家最近在媒体和公开信中对AI风险表示关心。为什么这个话题在近期会突然
出现在多则头条新闻中呢？

强AI的成功实现一直被认为是天方夜谭，即使得以达成也是最少是几千年以后的事情。然
而，由于最近的突破性研究和许多AI里程碑的创造，在仅仅五年前被专家认为是几十年后
才能达到的技术，现在已经实现。这让许多专家认真地考虑在我们整个人生中出现超级智
能的可能性。虽然一些专家仍然猜测类人AI在几个世纪以后才可能发生，大多数AI研究员
在2015年波多黎各会议中预测人类智能级别的人工智能在2060年之前便可能产生。因为AI
安全研究可能耗费十几年才能完成，所以我们应该乘现在便进行这些方面的研究。

由于AI有可能变得比人类更聪明，我们并没有确实的方式来预测它的行为将会是如何。对
于AI的发展可能衍生出什么后果，我们不能把过去的技术发展作为参考，因为我们从来没
有创造出任何能够在智慧上超越人类的科技。事实上，我们最能作为参考的依据便是人类
本身的进化史。人类之所以能在今天成为这个星球的万物之灵，并不是因为我们最强、最
快或体积最大，而是因为我们是最具智慧的。如果有一天，我们不再是世上最聪明的，我
们还能够自信对世界拥有控制力吗？

FLI的立场是，只要我们管理科技的智慧比技术不断增长的力量更加强劲，我们的文明就会
持续蓬勃发展。针对AI技术，FLI的立场是，我们应该在不阻碍AI发展的前提下，通过支持
人工智能安全研究来加速对AI科技的管理能力。

关于高级AI的误传


关于人工智能的未来以及它对人类意味着什么的对话已经正在进行。当下，AI技术有许多
连顶尖专家都无法达到共识的争议，例如：AI对就业市场未来的影响; 人类智能级别的人
工智能是否/何时会被开发;AI开发是否会导致智力爆炸;以及这是我们应该欢迎还是恐惧的
事情。但当下也存在着许多由误解或人们各说各话所引起的伪争论。为了帮助我们专注于
对有意义的争议和问题做出讨论，而不是分心在错误的信息上，让我们首先将一些最常见
的误传解除掉。


[AI-risk-66]

时间推断上的误区


一个常见的误区与AI的时间表有关：那就是人们认为他们能肯定地断定机器极大地取代人
类智能到底需要多长的时间。

许多人误认为我们肯定能在这个世纪内制造出超越人类的人工智能。事实上，历史充满了
人类将技术夸大的例子。比如说，从前人们认为一定会实现的聚变发电厂和飞行汽车，现
如今却从未出现过。 AI这类科技也在过去被人们，甚至被这个领域的先锋，一直重复地过
分夸大。比如说，约翰·麦卡锡（“人工智能”一词的创造人），马文·明斯基，纳撒尼尔·罗
切斯特和克劳德·香农曾写下个过于乐观的预测。单凭当年科技陈旧的电脑，他们提出：在
1956年夏天的2个月之内，他们要在达特茅斯学院进行一个10人人工智能的研究。他们企图
了解怎样能让机器使用语言，形式抽象和概念，来解决目前只有人类能应对的各种问题，
并进行AI自我提升。他们写道，只要有一组精心挑选的科学家们在一起工作一个夏天，就
可以在一个或多个问题上取得重大进展。但他们的预测，并没有成真。

在另一方面，人们之中也普遍流传着另一个极面的预测，那就是我们肯定知道超越人类的
人工智能不会在本世纪中出现。研究人员对我们与超越人类的人工智能的距离进行了广泛
的估计，但是我们亦不能断定本世纪出现超越人类的人工智能的概率为零，因为这种对科
技充满质疑的预测在过去很多时候都是错的。例如，伟大的核物理学家欧内斯特·卢瑟福（
Ernest Rutherford）曾在1933年，在西拉德发明核链反应前不到24小时之内说，核能只是
飘渺虚无的“月光”。皇家天文学家理查德·伍利也在1956年称行星际旅行不过是“一派胡言“
。他们负面的想法显然并不正确。对AI最为极端的预测是，超越人类的人工智能永远不会
到达，因为它在物理上是不可能实现的。然而，物理学家们都知道，大脑本身是由夸克和
电子排列起来的强大计算机，而世上并没有物理学的规律规定人类不可能建立更加智能的
夸克斑点。

目前，已经有一些调查问卷向AI研究人员询问他们对制造类人AI至少有50％概率的时间预
测。所有的调查都有相同的结论，那就是连世界上领先的专家都无法达到共识，所以不要
说我们，就更不能知道到底什么时候才会出现超越人类的人工智能。例如，在2015年波多
黎各AI会议的AI调查中，平均（中值）答案是在2045年，但当中也有些研究人员猜测超越
人类的人工智能只可能在数百年或更长的时间后发生。

还有一个相关的传言就是对AI有所担忧的人认为类人AI在几年内就会发生。事实上，大多
数担心超越人类的人工智能会带来隐忧的人，但凡在记录上曾发言过的，都预测超越人类
的人工智能至少需要几十年才会发生。但他们认为，只要我们不是100％确定超越人类的人
工智能不会在本世纪中发生，最聪明的做法就是乘现在便开始进行安全研究，以防范于未
来。许多与人类智能级别的人工智能相关的安全问题非常艰难，可能需要几十年才能解决
。所以，与其等到一群过度操劳的程序员不小心启动问题前才做出对策，现在就开始针对
这些安全问题进行研究是明智的。

误传的争议


另一个常见的误解是，那些对人工智能感到担忧和提倡人工智能安全研究的人事实上对AI
了解并不多。当标准AI教科书作者斯图尔特·罗素在波多黎各会谈中提到这一点时，他逗得
观众哄堂大笑。另一个相关的误解是，有些人认为支持人工智能安全研究是极具争议性的
。事实上，为人工智能安全研究能得到适当的投资，人们只需要理解到AI可能带来的安全
课题不可被忽视，但他们并不需要被说服认为其风险非常高。这就好比为房屋投保是一样
的道理：买保险并不是因为房屋极大的可能被烧毁，而是因为房屋被烧毁拥有不可忽略的
概率。为了安全起见，投入AI安全研究是非常合理的。

或许是媒体的夸大使人工智能的安全课题看起来比实际上更具争议。毕竟，在文章中使用
恐吓的语言，并利用超出语境的引言来宣扬即将来临的危机可以比发表具平衡的报告引来
更多的点击率。因此，若人们只是由媒体报道来了解彼此的立场，他们可能会认为他们在
立场上的分裂比事实上还要严重。比如说，一个对科技存在怀疑的读者只读了比尔·盖茨在
英国小报中所叙述的立场，便可能会错误地认为盖茨认为超级智能即将来临。同样的，参
与符合共同利益的人工智能运动的人或许在并不理解吴恩达博士的确实立场前，就因为他
对火星人口过多的发言而错误地认为他不在乎人工智能的安全。然而事实上，他对人工智
能安全具有一定的关心。他的发言的关键是因为吴恩达对时间线的估计更长，因此他自然
地倾向于优先考虑短期内AI所能带来的挑战，多于考虑长期中可出现的挑战。

关于超越人类的人工智能风险的误解


“斯蒂芬·霍金警告说，机器人的兴起可能对人类带来灾难。”许多AI研究人员在看到这个标
题时都感到不以为然。许多人甚至已经数不清他们看到了多少相似的文章。很多时候，这
些文章附着一个邪恶的机器人携带武器的图片，似乎意味着我们应该担心机器人会崛起并
屠杀人类，因为他们已经变得有意识和/或邪恶。但往另一个较为轻松的层面想，这样的文
章实际上是相当有意思的，因为他们简洁地总结了AI研究人员不担心的情况。这种情况结
合了多达三个单独的，有关于机器意识，邪恶本质和机器人制造的误区。

如果你沿着道路行驶，你具有对颜色、声音等的主观体验。但是自驾车会有主观体验吗？
为了成为一部自驾车，它需要对任何东西有所感受吗？虽然这种意识的奥秘本身是有趣的
，但它与AI的风险无关。如果你被无人驾驶的车撞到，它是否拥有主观感觉对你一点都不
重要。相同的，会影响我们的人类是超智能AI所做的事，而不是它的主观感觉。

对机器变成邪恶的恐惧是另一个分散注意力的话题。人类真正该担心的不是AI的恶意，而
是能力。根据定义，超级人工智能非常善于实现其目标，无论这些目标是什么，所以我们
需要确保其目标与我们的目标一致。人类通常不讨厌蚂蚁，但我们比他们更聪明，所以如
果我们想建一个水电站大坝而那里有一个蚁丘，蚂蚁就只能倒霉了。符合共同利益的人工
智能运动希望避免把人类放在和那些蚂蚁相同的处境上。

针对机器意识上的误解与机器不能拥有目标的误区有关。显然地，机器的目标在狭义上便
是做出能履行它的任务的行为：比如说，热寻求导弹的行为简单地解释就是遵循它击中标
靶的任务。如果你受到和你的目标不一致的机器的威胁，那么真正困扰你的是它被设置的
狭窄的任务，而不是机器本身是否有意识或是否有什么目的。如果一枚寻求热的导弹正追
逐着你，你大可能不会惊叫：“我不担心，因为机器本身没有目标！“吧。

我同情罗德尼·布鲁克斯和其他被小报不公平地妖魔的化机器人先锋。一些记者似乎痴迷地
对机器人穷追猛打，并在他们的文章将机器人刻画成邪恶的、拥有血红双眼的金属怪物。
事实上，符合共同利益的人工智能运动主要关注的不是机器人，而是智能本身：具体来说
，他们关心的是人工智能的目标与我们的目标不一致。这种不一致的超人类智能其实不需
要拥有一副机器人的身躯，而只需通过互联网连接就能给我们带来麻烦---如超控金融市场
，取代人类研究人员，操纵人类领导人，或开发我们甚至不能理解的武器。即使建筑机器
人在物理上是不可能达到的，一个超智慧和超富裕的人工智能可以很轻易地用金钱支付或
操纵许多不知情的人听从它的指示。

关于对机器人的误解与机器无法控制人类的想法有关。智慧产生控制力：人类之所以能够
控制老虎不是因为我们更强壮，而是因为我们更聪明。这也就意味着，如果我们放弃在地
球上属于万物之灵的地位，那我们也可能失去控制力。

有趣的争议


与其把时间浪费在以上所诉的误传中，我们不如将精力专注在那些真实存在，让专家无法
达成一致的有趣争议上。比如说，你想要什么样的未来？我们应该开发致命的自主武器吗
？您希望通过工作自动化实现什么？你会给现在的孩子们什么职业建议？你希望有一个新
的职业替代旧时代职业的社会，还是一个每个人都可以享有休闲的生活和由机器生产出的
财富的无业社会？往后，你希望我们创造出超智能的生命，并传播到我们的宇宙中吗？我
们是否会控制智能机器还是由它们控制我们？智能机器会取代我们，与我们共存还是与我
们合并？在人工智能的时代中，身为人类的意义是什么？你希望人类拥有什么样的意义，
而我们又如何导向那个未来呢？在此，我们恳请您参与对话与讨论！

推荐参考资源


视频

  • Stuart Russell – The Long-Term Future of (Artificial) Intelligence
  • Humans Need Not Apply
  • Nick Bostrom on Artificial Intelligence and Existential Risk
  • Stuart Russell Interview on the long-term future of AI
  • Value Alignment – Stuart Russell: Berkeley IdeasLab Debate Presentation at
    the World Economic Forum
  • Social Technology and AI: World Economic Forum Annual Meeting 2015
  • Stuart Russell, Eric Horvitz, Max Tegmark – The Future of Artificial
    Intelligence

媒体文章

  • Concerns of an Artificial Intelligence Pioneer
  • Transcending Complacency on Superintelligent Machines
  • Why We Should Think About the Threat of Artificial Intelligence
  • Stephen Hawking Is Worried About Artificial Intelligence Wiping Out
    Humanity
  • Artificial Intelligence could kill us all. Meet the man who takes that risk
    seriously
  • Artificial Intelligence Poses ‘Extinction Risk’ To Humanity Says Oxford
    University’s Stuart Armstrong
  • What Happens When Artificial Intelligence Turns On Us?
  • Can we build an artificial superintelligence that won’t kill us?
  • Artificial intelligence: Our final invention?
  • Artificial intelligence: Can we keep it in the box?
  • Science Friday: Christof Koch and Stuart Russell on Machine Intelligence
    (transcript)
  • Transcendence: An AI Researcher Enjoys Watching His Own Execution
  • Science Goes to the Movies: ‘Transcendence’
  • Our Fear of Artificial Intelligence

AI研究人员的论文

  • Stuart Russell: What do you Think About Machines that Think?
  • Stuart Russell: Of Myths and Moonshine
  • Jacob Steinhardt: Long-Term and Short-Term Challenges to Ensuring the
    Safety of AI Systems
  • Eliezer Yudkowsky: Why value-aligned AI is a hard engineering problem

相关文章

  • Intelligence Explosion: Evidence and Import (MIRI)
  • Intelligence Explosion and Machine Ethics (Luke Muehlhauser, MIRI)
  • Artificial Intelligence as a Positive and Negative Factor in Global Risk
    (MIRI)
  • Basic AI drives
  • Racing to the Precipice: a Model of Artificial Intelligence Development
  • The Ethics of Artificial Intelligence
  • The Superintelligent Will: Motivation and Instrumental Rationality in
    Advanced Artificial Agents
  • Wireheading in mortal universal agents

研究合集

  • Bruce Schneier – Resources on Existential Risk, p. 110
  • Aligning Superintelligence with Human Interests: A Technical Research
    Agenda (MIRI)
  • MIRI publications

案例分析

  • The Asilomar Conference: A Case Study in Risk Mitigation (Katja Grace,
    MIRI)
  • Pre-Competitive Collaboration in Pharma Industry (Eric Gastfriend and Bryan
    Lee, FLI): 工业竞争前对安全制度进行合作的案例研究

博客文章和会谈

  • AI control
  • AI Impacts
  • No time like the present for AI safety work
  • AI Risk and Opportunity: A Strategic Analysis
  • Where We’re At – Progress of AI and Related Technologies:
    对开发新AI技术的研究机构的进展做出介绍。
  • AI safety
  • Wait But Why on Artificial Intelligence
  • Response to Wait But Why by Luke Muehlhauser
  • Slate Star Codex on why AI-risk research is not that controversial
  • Less Wrong: A toy model of the AI control problem

书籍

  • Superintelligence: Paths, Dangers, Strategies
  • Our Final Invention: Artificial Intelligence and the End of the Human Era
  • Facing the Intelligence Explosion
  • E-book about the AI risk （包括比电影版本更真实的“终结者”场景）

组织

  • Machine Intelligence Research Institute机器智能研究所：一个非营利组织，其使
    命在于确保创造人类智能具有正面的影响。
  • Centre for the Study of Existential Risk (CSER)生存风险研究中心：一个多学科
    研究中心，致力于研究和减轻可能导致人类灭绝的风险。
  • Future of Humanity Institute人类未来研究所：一个多学科研究所，利用数学，哲
    学和科学，对人类及其前景的大观问题进行探讨。
  • Global Catastrophic Risk Institute 全球灾难风险研究所：一个领导全球灾难性风
    险研究、教育和专业网络的智囊团。
  • Organizations Focusing on Existential Risks关注生存风险的组织：对从事生存风
    险工作的一些组织的简要介绍。80,000 Hours: 80,000小时：AI安全研究人员的职业
    指南。


This content was first published at futureoflife.org on April 1, 2017.

About the Future of Life Institute

The Future of Life Institute (FLI) is a global non-profit with a team of 20+
full-time staff operating across the US and Europe. FLI has been working to
steer the development of transformative technologies towards benefitting life
and away from extreme large-scale risks since it's founding in 2014. Find out
more about our mission or explore our work.

Our content

Related content

Other posts about AI, Translation

If you enjoyed this content, you also might also be interested in:
[AI-Answers]

As Six-Month Pause Letter Expires, Experts Call for Regulation on Advanced AI
Development

This week will mark six months since the open letter calling for a six month
pause on giant AI experiments. Since then, a lot has happened. Our signatories
reflect on what needs to happen next.
September 21, 2023
[3-SB3-Arti]

Introductory Resources on AI Risks

Why are people so worried about AI?
September 18, 2023
[Podcast-th]

Robert Trager on International AI Governance and Cybersecurity at AI Companies

August 20, 2023
[US-senate-]

US Senate Hearing 'Oversight of AI: Principles for Regulation': Statement from
the Future of Life Institute

We implore Congress to immediately regulate these systems before they cause
irreparable damage, and provide five principles for effective oversight.
July 25, 2023
Our content

Sign up for the Future of Life Institute newsletter

Join 40,000+ others receiving periodic updates on our work and cause areas.
 
View previous editions
[FLi_Logo_C]
Steering transformative technology towards benefitting life and away from
extreme large-scale risks.

Cause areas

  • Artificial Intelligence
  • Biotechnology
  • Nuclear Weapons

Our work

  • Policy
  • Outreach
  • Grantmaking
  • Futures

Our content

  • Articles
  • Podcasts
  • Newsletters
  • Open letters

About us

  • Our people
  • Careers
  • Donate
  • Finances
  • FAQs
  • Contact us

  • Privacy Policy
  • Accessibility
  • Tax Forms
  • Report a broken link
  • Internal

  • Privacy Policy
  • Accessibility
  • Tax Forms
  • Report a broken link
  • Internal

© 2023 Future of Life Institute. All rights reserved.
    
