   IFRAME: https://www.googletagmanager.com/ns.html?id=GTM-59MW7L4

   (BUTTON)

   (BUTTON) Menu
   Rechercher ____________________ (BUTTON)
   Epaper/PDF Newsletters

   logo letemps S'abonner Mon compte
     * En continu
     * Monde
     * Suisse
     * Économie
     * Opinions
     * Culture
     * Société
     * Sciences
     * Sport
     * Data
     * Événements
     * Vidéos
     * Podcasts
     *

   ____________________ (BUTTON)
     * S'abonner
     * Accueil
     * En continu
     * Monde
     * Suisse
     * Économie
     * Opinions
     * Culture
     * Société
     * Sciences
     * Sport
     * Data
     * Événements
     * Hyperlien
     * Dossiers
     * Grands formats
     * En images
     * Vidéos
     * Podcasts
     *
     *
     * Epaper/PDF
     * Newsletters
     * Magazine T
     * Le Journal de l'Immobilier
     * Blogs
     * Archives
     * Archives historiques
     *
     * Abonnements
     * Services aux abonnés
     * Questions fréquentes
     *
     * Régie Publicitaire
     * Avis de décès

     * À propos
     * Impressum
     * Communication
     * Emplois

   Publicité

    1. Accueil
    2. Économie

OpenAI promet de suspendre des modèles dangereux d’intelligence artificielle

   Un mois après le feuilleton du départ puis du retour de Sam Altman, la
   firme annonce la mise sur pied d’un comité de surveillance. Une mesure
   vue comme un gage donné au conseil d'administration

   Sam Altman lors d’un sommet Asie-Pacifique de l’APEC, 16 novembre 2023,
   à San Francisco. — © Eric Risberg / keystone-sda.ch Sam Altman lors
   d’un sommet Asie-Pacifique de l’APEC, 16 novembre 2023, à San
   Francisco. — © Eric Risberg / keystone-sda.ch

   Le Temps avec l’AFP
   Le Temps avec l’AFP
   Publié le 19 décembre 2023 à 07:18. / Modifié le 19 décembre 2023 à
   09:57.
     *
     *
     *
     *
     *

   OpenAI a créé une équipe dédiée à l’identification et à la prévention
   des risques liés à l’intelligence artificielle (IA), qui pourra mener à
   suspendre le lancement d’un modèle d’IA s’il est considéré comme trop
   dangereux.

   Cette annonce intervient un mois seulement après le licenciement du
   patron du créateur de l’interface conversationnelle ChatGPT, Sam
   Altman, finalement réintégré au bout de quelques jours.

   Selon plusieurs médias américains, des membres du conseil
   d’administration lui reprochaient de privilégier le développement
   accéléré d’OpenAI quitte à éluder certaines interrogations sur les
   possibles dérives de l’IA.

   Notre éditorial du 18 décembre 2023: Sans garde-fou, l’IA menace nos
   sociétés

Une attention aux modèles «d’avant-garde»

   L’équipe d’alerte (preparedness team) sera pilotée par le chercheur en
   informatique Aleksander Madry, qui s’est mis en congé du Massachusetts
   Institute of Technology (MIT), où il est professeur, selon un message
   posté lundi par l’universitaire sur X (ex-Twitter).

   Elle s’appuiera sur un cadre de travail défini par un document publié
   lundi et qui établit son champ d’application et des procédures.

   Ce nouveau groupe va surtout s’intéresser aux modèles dits
   «d’avant-garde» (frontier models), en cours d’élaboration et dont les
   capacités sont supérieures aux logiciels les plus aboutis en matière
   d’IA.

   «Nous pensons que l’étude scientifique des risques de catastrophe
   découlant de l’IA n’est pas du tout à la hauteur», expliquent les
   responsables d’OpenAI dans le document. La création de cadre doit
   «aider à combler ce décalage», selon eux.

   Retrouvez nos articles sur l'intelligence artificielle.

Quatre classes de risque

   L’équipe évaluera chaque nouveau modèle et lui assignera un niveau de
   risque dans quatre catégories principales.

   La première concerne la cybersécurité et la capacité du modèle à
   procéder à des attaques informatiques d’ampleur. La deuxième mesurera
   la propension du logiciel à aider à la création d’un mélange chimique,
   d’un organisme (tel un virus) ou d’une arme nucléaire, tous éléments
   susceptibles d’être nocifs pour l’humain. La troisième catégorie touche
   à la capacité de persuasion du modèle, à savoir dans quelle mesure il
   peut influencer des comportements humains. La dernière classe de risque
   concerne l’autonomie potentielle du modèle, c’est-à-dire déterminer
   notamment s’il peut s’exfiltrer, ce qui revient à échapper au contrôle
   des programmeurs qui l’ont créé.

   Une fois les risques identifiés, ils seront soumis au Conseil de
   sécurité (Safety Advisory Group ou SAG), nouvelle entité qui effectuera
   des recommandations à Sam Altman ou à une personne désignée par lui.

   Le patron d’OpenAI statuera alors sur les possibles modifications à
   apporter à un modèle pour diminuer les risques associés. Le conseil
   d’administration sera tenu informé et pourra invalider une décision de
   la direction. C’est un gage donné aux administrateurs, après la saga du
   départ temporaire de Sam Altman qui avait valu au conseil des critiques
   sur sa légitimité.

   A ce propos: OpenAI, numéro un mondial de l’intelligence artificielle,
   plonge dans le chaos et son existence est menacée
     __________________________________________________________________

   Découvrez notre nouvelle chronique en vidéo, Emploi du Temps:

   IFRAME: https://www.youtube.com/embed/k62Necq6z6Y?rel=0&enablejsapi=1

   Intelligence artificielle
     *
     *
     *
     *
     *

le Temps

     * Impressum
     * À propos
     * Communication
     * Régie Publicitaire
     * Avis de décès
     * Événements
     * Emplois

Abonnements et Services

     * Abonnements
     * Services aux abonnés
     * Epaper/PDF
     * Newsletters
     * Magazine T
     * Journal de l'immobilier
     * Questions fréquentes
     * Archives
     * Archives historiques

Documents de références

     * Conditions générales d'utilisation
     * Conditions générales de vente
     * Politique de confidentialité
     * Gestion des cookies
     * Charte rédactionnelle
     * Charte des partenariats

Suivez Le Temps

     * Facebook
     * Ex-Twitter
     * Linkedin
     * Instagram
     * Youtube
     * Tiktok

     * Avenue du Bouchet 2, 1209 Genève | Service Clients: +41 22 539 10
       75 | Contactez Le Temps
