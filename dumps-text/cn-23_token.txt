稀土 掘金 稀土 掘金

• 首 页
□ 首 页
□ 沸点
□ 课程
□ 直播
□ 活动
□ 竞赛 商城 APP 插件
□
☆ [ ]
搜索 历史 清空
☆ 创作者 中心
○
写 文章
○
发沸点
○
写 笔记
○
写 代码
○
草稿 箱
创作 灵感 查看 更 多
□ vip
会员
□ 登录
注册

CV大 模型 系列 之 ： 全面 解读 VIT ， 它 到底 给 植树 人 挖 了 多少 坑

猛 猿
2023 - 07 - 11 2 , 446


⚠️ ⚠️ ⚠️ 本文 为 稀土 掘金 技术 社区 首发 签约 文章 ， 30 天 内 禁止 转载 ， 30 天 后 未 获 授权 禁
止 转载 ， 侵权 必究 ！

大 家 好 ， 最近 越 演 越 热 的 AIGC浪潮 ， 将 Transformer 这个 模型 带 进 了 大 家 的 视野 。 如果 你 从
事NLP相关 的 职业 ， 你 一定 知道 Bert （ 来 自Transformer 的 Encoder ） 在 5 年 前 卷 起 的 NLP 预训
练 的 热潮 。 而 在 当时 因为 受到 训练 数据 和 算力 的 限制 ， 以 GPT （ 来 自Transformer 的 Decoder
） 为 代表 的 生成式 自然 语言 模型 ， 还 没有 展现 出 惊人 的 涌现 能力 ， 但 它 却 启发 了 人们 对 “ 自
监督 训练 ” 的 不懈 研究 ， 以此 来 解决 “ 自然 语言 训练 中 标注 数据 不 足 ” 的 问题 。 时间 来 到 2020
年 ， 此时 还是 Bert独占鳌头 ， 在 Transformer Encoder 架构 的 启发 下 ， CV 算法 工程师 们 开始
思考 一个 问题 ： ” 当 CNN 的 架 构快 做到 极致 时 ， 我们 能 否 换 一个 新 方向 ？ ” 于是 这 一 年 ，
Google 推出 了 VIT （ Vision Transformer ） ： 一个 和 Bert 几乎 一致 ， 同时 不 添加 任何 卷 积结
构 的 图像 分类 模型 。 VIT 在 Transformer 上 的 成功 ， 证明 了 可以 用 统一 的 模型 ， 来 处理 不 同
领域 （ 语言 / 图像 / 视频 ） 的 任务 ， 进而 开启 了 多 模态 模型 研究 的 新篇章 。 VIT 作为 众多 大模
型 的 backbone （ 骨架 结构 ） ， 是 我们 在 研究 AIGC 时 绕 不 过 的 话题 。

今天 这 篇 文章 ， 就 和 大 家 一起 来 全面 解读 VIT 。 如果 大 家 对 Transformer 和 Bert 不 了解 ， 强
烈 建议 大 家 在 阅读 本文 前速 读 以下 2 篇 文章 ：

• 深入浅出 Transformer 系列 之 二 ： Self - attention （ attention 是 Transformer 的 核心 模
块 ）
• 深入浅出 Bert 系列 之一 ： 模型 原理 篇

CV大 模型 系列 文章 导航 （ 持续 更新 中 ） ：
🌸C V大 模型 系列 之 ： 扩散 模型 基石 DDPM （ 模型 架 构篇 ） 🌸
🌸C V大 模型 系列 之 ： 扩散 模型 基石 DDPM （ 人人 都 能 看 懂 的 数学 原理 篇 ） 🌸
🌸C V大 模型 系列 之 ： 扩散 模型 基石 DDPM （ 源码 解读 与 实操 篇 ） 🌸
🌸C V大 模型 系列 之 ： 全面 解读 VIT ， 它 到底 给 植树 人 挖 了 多少 坑 🌸
🌸C V大 模型 系列 之 ： 多 模态 经典 之 作 CLIP ， 探索 图文 结合 的 奥秘 🌸
🌸C V大 模型 系列 之 ： MAE ， 实现 像 素级 图像 重建 🌸
🌸C V大 模型 系列 之 ： MoC o v 1 ， 利用 对比 学习 在 CV 任务 上 做 无 监督 训练 🌸
🌸C V大 模型 系列 之 ： DALLE2 ， OpenA I文生图 代表作 解读 🌸

一 、 模型 架构

提起 一个 新 模型 ， 我 想 大 家 最 关心 的 事 就是 ： 它 到底 长什 么 样 ？ 输入 输出 是 什 么 ？ 我 要 怎
么 用 ？

所以 ， 我们 先 来 看 模型 架构 。

1 . 1 Bert 架构

前面 说 过 ， VIT 几乎 和 Bert 一致 ， 我们 来 速扫 一下 Bert 模型 ：

[ c 37c1 df00 f ]

• input ： 输入 是 一 条 文本 。 文本 中 的 每个 词 （ token ） 我们 都 通过 embedding 把 它 表示 成
了 向量 的 形式 。 、

• 训练 任务 ： 在 Bert 中 ， 我们 同时 做 2 个 训练 任务 ：

□ Next Sentence Prediction Model （ 下 一 句 预测 ） ： input 中 会 包含 两 个 句子 ， 这
两 个 句子 有 50 % 的 概率 是 真实 相 连 的 句子 ， 50 % 的 概率 是 随机 组装 在 一起 的 句子 。
我们 在 每个 input 前面 增加 特殊 符 < cls > ， 这个 位置 所在 的 token 将 会 在 训练 里 不 断
学习 整条 文本 蕴含 的 信息 。 最后 它 将 作为 “ 下 一 句 预测 ” 任务 的 输入 向量 ， 该 任务
是 一个 二 分类 模型 ， 输出 结果 表示 两 个 句子 是 否 真实 相 连 。
□ Masked Language Model （ 遮蔽词 猜测 ） ： 在 input 中 ， 我们 会 以 一定 概率 随机 遮
盖 掉 一些 token （ < mask > ) ， 以此 来 强迫 模型 通过 Bert 中 的 attention 结构 更 好 抽取
上下文 信息 ， 然后 在 “ 遮蔽词 猜测 ” 任务 重 ， 准确 地 将 被 覆盖 的 词猜 测出 来 。
• Bert 模型 ： Transformer 的 Encoder层 。

关于 Bert 更 详细 的 介绍 ， 可以 参考 文章 开头 给 出 的 链接 文章 。

1 . 2 VIT 模型 架构

[ fb 49 b 6 e 44 c ]

我们 先 来 看 左侧 部分 。

• Patch ： 对于 输入 图片 ， 首先 将 它 分成 几 个 patch （ 例如 图 中 分为 9 个 patch ） ， 每个
patch 就 类似 于 NLP 中 的 一个 token （ 具体 如何 将 patch 转变 为 token 向量 ， 在 下 文会 细说
） 。
• Position Embedding ： 每个 patch 的 位置 向量 ， 用于 指示 对应 patch 在 原始 图片 中 的 位
置 。 和 Bert 一样 ， 这个 位置 向量 是 learnable 的 ， 而 并非 原始 Transformer 中 的 函数式
位置 向量 。 同样 ， 我们 会 在 下 文详 细 讲解 这 一 块 。
• Input : 最终 传入 模型 的 Input = patching _ emebdding + position embedding ， 同样
， 在 输入 最 开始 ， 我们 也 加 一个 分类符 < cls > ， 在 bert 中 ， 这个 分类符 是 作为 “ 下 一 句
预测 ” 中 的 输入 ， 来 判断 两 个 句子 是 否 真实 相 连 。 在 VIT 中 ， 这个 分类符 作为 分类 任务
的 输入 ， 来 判断 原始 图片 中 物体 的 类别 。

右侧 部分 则 详细 刻画 了 Transformer Encoder层 的 架构 ， 它 由 L块 这样 的 架构 组成 。 图片 已
刻画 得 很 详细 ， 这里 不 再 赘述 。

总结 起 来 ， VIT 的 训练 其实 就 在 做 一 件 事 ： 把 图片 打成patch ， 送入 Transformer Encoder ，
然后 拿 < cls > 对应 位置 的 向量 ， 过 一个 简单 的 softmax 多 分类 模型 ， 去 预测 原始 图片 中 描绘
的 物体 类别 即可 。

你 可能 会 想 ： “ 这个 分类 任务 只 用 一个 简单 的 softmax ， 真 得 能 分准 吗 ？ ” 其实 ， 这 就是 VIT
的 精华 所 在 了 ： VIT 的 目 的 不 是 让 这个 softmax 分类 模型 强大 ， 而是 让 这个 分类 模型 的 输入
强大 。 这个 输入 就 是 Transformer Encoder 提炼 出 来 的 特征 。 分类 模型 越 简单 ， 对 特征 的 要
求 就 越 高 。

所以 为什 么 说 Transformer 开启 了 大 一统模型 的 预训练 大门 呢 ？ 主要 原因 就 在于 它 对 特征 的
提炼 能力 —— 这样 我们 就 可以 拿 这个 特征 去 做 更 多 有趣 的 任务 了 。 这 也 是 VIT 能 成为 后续 多 模
态backbone 的 主要 原因 。

二 、 从 patch 到 token

讲 完 了 基本 框架 ， 我们 现在 来 看 细节 。 首先 我们 来 看看 ， 图片 的 patch 是 怎 么 变成 token
embedding 的 。

2 . 1 patch变token 的 过程

[ 6304 e 32 b 15 ]

如图 ， 假设 原始 图片 尺寸 大小 为 ： 224 * 224 * 3 ( H * W * C ) 。

现在 我们 要 把 它 切 成 小patch ， 每个 patch 的 尺寸 设 为 16 （ P = 16 ） ， 则 每个 patch 下 图片 的 大
小 为 16 * 16 * 3 。

则 容易 计算 出 共有 224 ∗ 22416 ∗ 16   =   196 \ frac { 224 * 224 } { 16 * 16 }   =   19616 ∗ 16224 ∗ 224 ​   =
  196 个 patch 。

不 难 看出 每个 patch 对应 着 一个 token ， 将 每个 patch展平 ， 则 得到 输入矩 阵X ， 其 大小 为
( 196 , 768 ) ， 也 就是 每个 token 是 768 维 。

通过 这样 的 方式 ， 我们 成功 将 图像 数据 处理 成 自然 语言 的 向量 表达 方式 。

好 ， 那 么 现在 问题 来 了 ， 对于 图 中 每 一个 16 * 16 * 3 的 小 方块 ， 我 要 怎 么 把 它 拉平 成 1 * 768
维度 的 向量 呢 ？

比如说 ， 我 先 把 第一 个 channel 拉 成 一个 向量 ， 然后 再 往 后 依次 接 上 第二 个 channel 、 第三
个 channel 拉平 的 向量 。 但 这种 办法 下 ， 同 一个 pixel本 来 是 三 个 channel 的 值 共同 表达 的 ，
现在 变成 竖直 的 向量 之后 ， 这 三 个 值 的 距离 反而 远 了 。 基于 这个 原因 ， 你 可能 会 想 一些 别
的 拉平 方式 ， 但 归根究底 它们 都 有 一个 共同 的 问题 ： 太规 则 化 ， 太 主观 。

所以 ， 有 办法 利用 模型 来 做 更 好 的 特征 提取 吗 ？ 当然 没 问题 。 VIT 中 最终 采用 CNN 进行 特征
提取 ， 具体 方案 如下 ：

[ 3008ea3 b 4 d ]

采用 768 个 16 * 16 * 3 尺 寸 的 卷积核 ， stride = 16 ， padding = 0 。 这样 我们 就 能 得到 14 * 14 * 768 大
小 的 特征 图 。 如同 所示 ， 特征 图 中 每 一个 1 * 1 * 768 大小 的 子特 征图 ， 都 是 由 卷 积核 对 第一 块
patch 做 处理 而 来 ， 因此 它 就 能 表示 第一 块 patch 的 token 向量 。

【 备注 】 ：

你 可能 会 问 ， 前面 不 是 说VIT 已经 摆脱 CNN 了 吗 ？ 这里 怎 么 又 用 卷积 了 ？ 由于 这 一 步 只是 输
入 预处理 阶段 ， 和 主体 模型 没有 关系 ， 只要 将 其 试 为 一致 特征 提取 方法 即可 ， 并 不 影响 我
们 之前 的 结论 。

2 . 2 为什 么 要 处理 成 patch

你 可能 想 问 ， 为什 么 一定 要 先 分patch ， 再 从 patch转token 呢 ？

第一 个 原因 ， 是 为 了 减少 模型 计算量 。

在 Transformer 中 ， 假设 输入 的 序列 长度 为 N ， 那 么 经过 attention 时 ， 计算复 杂度 就 为 O
( N2 ) O ( N ^{ 2 } ) O ( N2 ) ， 因为 注意力 机制 下 ， 每个 token 都 要 和 包括 自己 在内 的 所有 token 做 一
次attention score 计算 。

在 VIT 中 ， N = H∗ WP2 N = \ frac { H * W }{P ^{ 2 } }N = P2 H ∗ W​ ， 当 patch尺寸 P越 小时 ， N 越 大 ， 此时 模
型 的 计算量 也 就 越 大 。 因此 ， 我们 需要 找到 一个 合适 的 P值 ， 来 减少 计算 压力 。

第二 个 原因 ， 是 图像 数据 带有 较 多 的 冗余 信息 。

和 语言 数据 中 蕴含 的 丰富语 义 不 同 ， 像 素 本身 含有 大量 的 冗余 信息 。 比如 ， 相邻 的 两 个 像
素格子 间 的 取值 往往 是 相似 的 。 因此 我们 并 不 需要 特别 精准 的 计算 粒度 （ 比如 把 P设 为 1 ）
。 这个 特性 也 是 之后 MAE ， MoCo之类 的 像 素级 预测 模型 能够 成功 的 原因 之一 。

三 、 Emebdding

如下 图 ， 我们 知道 在 Bert （ 及 其它 NLP 任务 中 ） ：

输入 = token _ embedding ( 将 单个词 转变 为 词向量 ) + position _ embedding ( 位置 编码 ， 用
于 表示 token 在 输入 序列 中 的 位置 ) + segment _ emebdding ( 非 必须 ， 在 bert 中 用于 表示 每
个 词属 于 哪 个 句子 ) 。

在 VIT 中 ， 同样 存在 token _ embedding 和 postion _ emebedding 。

[ 0ca 39caa2 d ]

3 . 1 Token Emebdding

我们 记token emebdding 为 ， 则 ， 则 ， 则 是 一个 形状 为 ( 768 , 768 ) 的 矩阵 。

由 前 文知 经过 patch 处理 后 输入 $$$$的 形状 为 ( 196 , 768 ) ， 则 输入 X过toke _ embedding 后 的
结果 为 ：

XTE = X∗E = ( 196 , 768 ) ∗ ( 768 ∗ 768 ) = ( 196 , 768 ) X _ {TE } = X * E = ( 196 , 768 ) * ( 768 * 768 )
= ( 196 , 768 ) XTE​ = X∗E = ( 196 , 768 ) ∗ ( 768 ∗ 768 ) = ( 196 , 768 )

你 可能 想 问 ， 输入 X本 来 就 是 一个 ( 196 ， 768 ) 的 矩阵 啊 ， 我 为什 么 还 要 过 一 次 embedding
呢 ？ 这个 问题 的 关键 不 在于 数据 的 维度 ， 而 在于 embedding 的 含义 。 原始 的 X仅 是 由 数据 预
处理 而 来 ， 和 主体 模型 毫无关系 。 而 token _ embedding 却 参与 了 主体 模型 训练 中 的 梯度 更新
， 在 使用 它 之后 ， 能 更 好 地 表示 出 token 向量 。 更进一步 ， E 的 维度 可以 表示 成 ( 768 , x ) 的
形式 ， 也 就 是 第二 维 不 一定 要是 768 ， 你 可以 自由 设定 词向量 的 维度 。

3 . 2 Position Embedding （ 位置 向量 ）

在 NLP 任务 中 ， 位置 向 量 的 目 的 是 让 模型 学 得 token 的 位置 信息 。 在 VIT 中 也 是 同理 ， 我们 需
要 让 模型 知道 每个 patch 的 位置 信息 （ 参见 1 . 2 中 架 构图 ） 。

我们 记位 置向量 为 EposE _{pos } Epos​ ， 则 它 是 一个 形状 为 ( 196 ， 768 ) 的 矩阵 ， 表示 196 个 维
度 为 768 的 向量 ， 每个 向量 表示 对应 token 的 位置 信息 。

构造 位置 向 量 的 方法 有 很多 种 ， 在 VIT 中 ， 作者 做 了 不同 的 消融 实验 ， 来 验证 不 同 方案 的 效
果 （ 论文 附录 D . 4 ） 部分 ， 我们 来 详细 看看 ， 作者 都 曾 尝试 过 哪 些 方案 。

方案 一 ： 不 添加 任何 位置 信息

将 输入 视为 一 堆 无序 的 patch ， 不 往 其中 添加 任何 位置 向量 。

方案 二 ： 使用 1 - D 绝对 位置 编码

也 就 是 我们 在 上文 介绍 的 方案 ， 这 也 是 VIT 最终 选定 的 方案 。

1 - D 绝对 位置 编码 又 分为 函数式 （ Transformer 的 三角函数 编码 ， 详情 可 参见 这 篇 文章 ） 和
可 学习式 （ Bert 采用 编码 方式 ） ， VIT 采用 的 是 后者 。 之所以 被 称为 “ 绝对 位置 编码 ” ， 是 因
为 位置 向量 代表 的 是 token 的 绝对 位置 信息 （ 例如 第1 个 token ， 第2 个 token 之类 ） 。

方案 三 ： 使用 2 - D 绝对 位置 编码

[ f 2590652d0 ]

如 图 所 示 ， 因为 图像 数据 的 特殊性 ， 在 2 - D 位置 编码 中 ， 认为 按 全局 绝对 位置 信息 来 表示 一
个 patch 是 不 足够 的 （ 如 左侧 所示 ） ， 一个 patch 在 x轴 和 y轴 上 具有 不 同 含义 的 位置 信息 （
如 右侧 所 示 ） 。 因此 ， 2 - D 位置 编码 将 原 来 的 PE向量 拆 成 两 部分 来 分别 训练 。

方案 四 ： 相对 位置 编码 （ relative positional embeddings ）

[ 0eee 86026 a ]

相对 位置 编码 （ RPE ） 的 设计 思想 是 ： 我们 不 应该 只 关注 patch 的 绝对 位置 信息 ， 更 应该 关
注patch 间 的 相对 位置 信息 。 如 图 所示 ， 对于 token4 ， 它 和 其余 每 一个 token间 都 存在 相对
位置 关系 ， 我们 分别 用 w−3 , w−2 , ...w 1 w _{-3 } , w _{-2 } , ... w _{ 1 } w − 3 ​ , w−2 ​ , ...w1 ​ 这 5 个
向量 来 表示 这种 位置 关系 。 那 么 接 下 来 ， 只要 在 正常 计算attention 的 过程 中 ， 将 这 5 个 向
量 当作 bias 添加 到 计算 过程 中 （ 如 图 公式 所 示 ） ， 我们 就 可以 正常 训练 这些 相对 位置 向 量
了 。 为 了 减少 训练 时 的 参数量 ， 我们 还 可以 做 clip 操作 ， 在 制定 clip 的 步数 k 之后 ， 在 k范
围 之外 的 w 我们 都 用 固定 的 w 表示 。 例如 图 中 当k = 2时 ， 向 token4 的 前方 找 ， 我们 发现 w− 3 w _
{ -3 } w − 3 ​ 已经 在 k = 2步 之外 了 ， 因此 就 可以 用 w− 2 w _{-2 } w−2 ​ 来 替代 w − 3 w _{-3 } w − 3 ​ ， 如果
token1 之前 还有 token ， 那 么 它们 的 w 都 可 用 w− 2 w _{-2 } w−2 ​ 替代 。 向 token4 的 后方 找 ， 发现
大 家 都 在 k = 2步 之内 ， 因此 无 需 做 任何 替换 操作 。

关于 相对 位置 编码 的 更 多 信息 ， 可以 阅读 原始 论文 arxiv . org / pdf / 1803 . 02 …

实验 结果

这 四 种 位置 编码 方案 的 实验 结果 如下 ：

[ 04 d 317 d 554 ]

可以 发现 除 了 “ 不 加 任何 位置 编码 ” 的 效果 显著 低 之外 ， 其余 三 种 方案 的 结果 都 差 不 多 。 所
以 作者 们 当然 选择 最 快捷 省力 的 1 - D 位置 编码 方案 啦 。 当 你 在 阅读 VIT 的 论文 中 ， 会 发现 大
量 的 消融 实验 细节 （ 例如 分类 头 < cls > 要 怎 么 加 ） ， 作者 这样 做 的 目 的 也 很 明确 ： “ 我们 的
方案 是 在 诸多 可行 的 方法 中 ， 逐一 做 实验 比 对 出 来 的 ， 是 全面 考虑 后 的 结果 。 ” 这 也 是 我
一直 觉得 这 篇 论文 在 技术 之外 值得 借鉴 和 反复 读 的 地方 。

四 、 模型 架构 的 数学 表达

到 这 一 步 位置 ， 我们 已 基本 将 VIT 的 模型 架构 部分 讲 完 了 。 结合 1 . 2 中 的 模型 架 构图 ， 我们
来 用 数学 语言 简练 写 一下 训练 中 的 计算 过程 ：

[ 91e5a04120 ]

( 1 ） 即 是 我们 说 的 图像 预处理 过程 :

• xpix _{p }^{i } xpi​ ： 第i 块 patch

• E ,  EposE ,  E _{pos } E ,  Epos​ ： Token Embedding ， 1 - D Positional Embedding

• xclassx _{class } xclass​ ： 和 Bert 类似 ， 是 额外 加 的 一个 分类头

• z0 z _{0 } z0 ​ ： 最终 VIT 的 输入

（ 2 ） 即 是 计算multi - head attention 的 过程 ， （ 3 ） 是 计算 MLP 的 过程 。

（ 4 ） 是 最终 分类 任务 ， LN 表示 是 一个 简单 的 线性 分类 模型 ， zL0 z _{L }^{0 } zL0 ​ 则 是 < cls >
对应 的 向量 。

五 、 微调 （ fine - t une ）

目前 为止 ， 按照 一 至 五 部分 所 说 的 内容 ， 通过 让 模型 做 分类 预测 ， 我们 可以 预训练 （
pretrain ） 好 一个 VIT 了 。

前面 说 过 ， 预训 练 好 的 VIT 模型 是 个 有力 的 特征 提取器 ， 我们 可以 用 它 输出 的 特征 ， 去 做 更
多 有趣 的 下游 任务 （ downstream task ) 。 例如 拿 它 去 做 类型 更 丰富 的 分类 ， 目标 检测 等 事
情 。 在 做 这些 任务 时 ， 我们 会 喂 给 预训练 模型 一 堆 新 的 数据 ， 同时 尽量 保证 模型 的 主体 架
构 不 变 （ 例如 VIT 整体 参数 不 动 ， 只 在 输出层 后 接 一个 新 模型 ， 再次 训练 时 只 对 新 模型 做 参
数 更新 之类 ） 。 这种 既 利用 了 已 有 模型 的 特征 提取 能力 ， 又 能 让 模型 更 好 适应 不 同 任务 的
操作 ， 称为 微调 （ fine - t une ） 。

在 fine - t une 的 时候 ， 我们 用 的 图像 大小 可能 和 预训练 时 的 并 不 一致 ， 比如 ：

• 预训 练时 用 224 * 224 * 3 大小 的 图片 ， fine - t une 时 为 了 效果 更 好 ， 一 般 选择 分辨率 更 高
的 图片 ， 例如 1024 * 1024 * 3

• 假设 保持 patch尺 寸P = 16 不 变 ， 则 预训练 时 产生 的 patch数 有 196 个 ， fine - t une 时 产生
的 patch数 有 4096 个 ( H∗WP2 \ frac { H * W }{P ^{ 2 } }P2 H ∗ W​ )

• 我们 知道 ， Transformer 主体 架构 理论 上 是 可以 处理 任意 长度 的 输入 序列 的 （ 相关 分析
参见 这 篇 文章 ） 。 但是 可 学习 的 （ learnable ） 位置 编码 不 是 ， 由于 一个 位置 对应 一 条
位置 编码 ， 它 和 输入 序列 长度 密切 相关 。

那 么 多 出 来 的 patch ， 在 fine - t une 时 要 怎 么 给 它们 位置 编码 呢 ？ 如果 统一 都 赋成0 向 量 ， 然
后 在 fine - t une 的 时候 再 去 训练 这些 向量 ， 看 起 来 可以 ， 但 这样 粗暴 的 赋值 不 仅 增加 了 计算
量 ， 也 浪费 了 已 有 的 信息 （ 例如 ， 是 否 能 从 已 有 的 位置 编码 粗略 地 初始化 一些 新 的 位置 编
码出 来 ？ ） 考虑 到 这 一 点 ， VIT 在 fine - t une 时 ， 对 预训练 阶段 的 位置 编码 做 了 2D 插值 处理
。

5 . 1 VIT fine - t une : 2 D插值 位置 编码

[ 179638ae1 a ]

如 图 绿色 部分 所示 ， 在 fine - t une 阶段 要 处理 的 patch / token数 sfinetunes _{finetune } sfin
etune​ 可能 比 预训练 阶段 要 处理 的 spretrains _{pretrain } spretrain​ 要 多 。

图 中 红色 部分 演示 了 如何 通过 插值 方法 将 spretrains _{pretrain } spretrain​ 扩展 至
sfinetunes _{finetune } sfinetune​ 。 其中 interpolate部分 就 是 2D插值 ， 这 部分 是 重点 ，
我们 直接 看 下 代码 中 的 操作 ：

new _ pos _ embedding _ img = nn . functiona l . interpolate (
pos _ embedding _ img ,
size = new _ seq _ leng th _ 1 d ,
mode = interpolation _ mode ,
align _ corners = True ,
)

可以 发现 这里 用 了 pytorch 内置 的 interpolate函数 ， mode 表示 具体 的 插值 方法 ， 在 VIT 中 采
用 的 是 bicubic 。 align _ corners = True 的 意思 是 在 固定 原 矩阵 四 角 的 情况 下 按 mode 进行 插
值 ， 可以 参加 图 中 ， 白色 圆圈 表示 原始 的 矩阵 ， 蓝色 点 表示 做 完 插值 后 的 矩阵 。 插值 后 矩
阵 的 四 角 保持 不 变 ， 中间 则 按 设置 的 方法 做 插值 。 关于 插值 位置 编码 更 详细 的 讲解 ， 可以
参考 这 篇 文章 。

六 、 VIT 效果

到 目前 为止 ， 我们 已 讲 完 了 预训练 和 微调 的 内容 。 接 下 来 ， 我们 来 看VIT 的 效果 ， 及 一些 有
趣 的 实验 结果 。

6 . 1 不 同VIT 模型 的 表示 符号

[ f 277adbef 3 ]

VIT预 训练 了 三 种 不 同 参数 规模 的 模型 ， 分别 是 VIT - Base ， VIT - Large 和 VIT - Huge 。 其 规模
可 具体 见 上图 。

在 论文 及 实际 使用 中 ， 我们 常用 VIT - s ize / patch _ size 的 形式 来 表示 该 模型 是 在 “ 什 么 规模 ”
及 “ 多 大 的 patch尺寸 ” 上 预训 练出 来 的 。 例如 VIT - H / 14 就 表示 该 模型 是 在 Huge 规模 上 ， 用
patch尺寸 为 14 的 数据 做 预训练 的 。

6 . 2 VIT VS 卷 积神经 网络

既然 VIT 的 目 的 是 替换 卷积神经 网络 ， 那 么 当然 要 比较 一下 它 和 目前 SOTA 的 卷积 网络 间 的 性
能 了 。

作者 选取 了 ResNet 和 Noisy Student 这 两 种 经典 高性能 的 卷积神经 网络 与 VIT 进行 比较 ， 比
较 内容 为 “ 预测 图片 类别 的 准确性 ” 与 “ 训练 时 长 ” ， 结果 如下 ：

[ 76655dab73 ]

前 三 列 Ours - JFT ( VIT - H / 14 ) ， Ours - JFT ( VIT - L / 16 ) ， Ours - I 12 K ( VIT - L / 16 ) 表示 三 个 VIT预
训练 模型 ， 它们 分别 在 不 同 规模 和 不 同 数据集 （ JFT , I 12K ） 上 预训 练 而 来 。 后 两 列 表示 两
个 卷积神经 网络 模型 。

纵向 的 ImageNet ， ImageNet Real 等 表示 不 同 的 图像 数据集 ， 当 我们 的 VIT 模型 和 卷积 模型
预训 练 好 后 ， 我们 就 可以 借助 这些 pretrain 模型 ， 在 图像 数据 集 上 做fine - t une ， 而 表格 里
给 出 的 就 是 fine - t une 后 的 准确率 。

观察 表格 ， 我们 发现 一个 有趣 的 现象 ： VIT 和 卷积 神经 网络 相比 ， 表现 基本 一致 。 关于 这 一
点 ， 我们 会 在 下 文详 细 分析 。

虽然 准确率 没有 突出 表现 ， 但是 训练 时间 上 VIT 的 还是 有 亮点 的 ， 表格 最后 一行 表示 ， 假设
用 单块 TPU 训练 模型 ， 所 需要 的 天数 。 我们 发现 VIT 最高 也 只 需要 2500 核 - 天 （ 当然 其实 这个
值 也 不 小 啦 ） ， 卷积 网络 要 花 至 9900 核 - 天 以上 。 所以 VIT 的 一个 优势 在于 ， 训练 没 那 么 贵
了 。 关于 这 点 ， 我 的 猜想 是 基于 Transformer 架构 的 VIT ， 和 卷积 神经 网络 相比 ， 更 适合 做
切分 均匀 的 矩阵 计算 ， 这样 我们 就 能 把 参数 均 匀切 到 不 同 卡 上 做 分布式 训练 ， 更 好 利用 GPU
算力 ， 平衡 整个 训练 系统 了 。

现在 ， 我们 回到 刚才 的 问题 ， 为什 么 VIT 相比 卷积 网络 ， 在 准确率 上 没有 突出 优势 ？ 为 了 解
答 这个 问题 ， 我们 先 来 看 卷 积神经 网络 的 归纳 偏置 （ inductive biases ）

6 . 2 . 1 卷 积神经 网络 的 归纳 偏置

归纳 偏置 用 大白话 来 说 ， 就 是 一 种 假设 ， 或者 说 一 种 先验 知识 。 有 了 这种 先验 ， 我们 就 能
知道 哪 一 种 方法 更 适合 解决 哪 一 类 任务 。 所以 归纳 偏置 是 一 种 统称 ， 不 同 的 任务 其 归纳 偏
置 下 包含 的 具体 内容 不 一样 。

对 图像 任务 来 说 ， 它 的 归纳 偏置 有 以下 两 点 ：

• 空间 局部性 （ locality ） ： 假设 一 张 图片 中 ， 相邻 的 区域 是 有 相关 特征 的 。 比如 太阳
和 天空 就 经常 一起 出现 。
• 平移 等 边性 （ translation equivariance ） ： f ( g ( x ) ) = g ( f ( x ) ) , f = 卷积 , g = 平f ( g ( x ) )
= g ( f ( x ) ) , f = 卷积 , g = 平f ( g ( x ) ) = g ( f ( x ) ) , f = 卷积 , g = 平 。 假设 一 张 图 中 ， 左上角 有
一个 太阳 ， 你 对 这 张 图 正常 做 卷积 得到 特征图 ， 则 左上角 的 卷积 可 表示 为 f ( xf ( xf ( x ，
做 完 卷积 后 ， 你 想 把 左上角 的 特征图 移动 到 右上角 去 ， 则 你 这 一 顿 操作 可以 用g ( f ( x ) g
( f ( x ) g ( f ( x ) 来 表示 。 这 一 系列 操作 等同 于 ， 你 先 把 左上角 的 太阳 移动 到 右上角 去 ( g
( xg ( xg ( x ) ， 然后 再 做 卷 积f ( g ( x ) f ( g ( x ) f ( g ( x ) ， 这 就是 图像 的 平移 等 边性 。 不 论 物体
移动 到 哪 里 ， 只要 给 卷 积核 的 输入 不 变 ， 那 么 输出 也 是 一致 的 。

在 这 两 种 先验 假设 下 ， CNN 成为 了 图像 任务 最佳 的 方案 之一 。 卷 积 核能 最 大 程度 保持 空间局
部性 （ 保存 相关 物体 的 位置 信息 ） 和 平移 等 边性 ， 使得 在 训练 过程 中 ， 最 大 限度 学习 和 保
留 原始 图片 信息 。

好 ， 那 么 现在 ， 如果 说 VIT 相比 于 卷积 ， 在 图像 任务 上 没有 显著 优势 ， 那 大概率 VIT 对 这 两
种 先验 的 维护 没有 CNN 做 的 好 ， 具体 来 看 ：

[ 4820 c 35273 ]

图 中 箭头 所 指 的 两 部分 都 属于 同一栋 建筑 。 在 卷积 中 ， 我们 可以 用 大小 适当 的 卷积核 将 它
们 圈 在 一起 。 但是 在 VIT 中 ， 它们 之间 的 位置 却 拉远 了 ， 如果 我 把 patch 再 切 分细 一些 ， 它
们 的 距离 就 更 远 了 。 虽然 attention 可以 学习 到 向量 间 的 想 关系 ， 但是 VIT 在 空间 局部性 的
维护 上 ， 确实 没有 卷积 做 的 好 。 而 在 平移 等 边性 上 ， 由于 VIT 需要 对 patch 的 位置 进行 学习
， 所以 对于 一个 patch ， 当 它 位置 变幻 时 ， 它 的 输出 结果 也 是 不 一样 的 。 所以 ， VIT 的 架构
没有 很 好 维护 图像 问题 中 的 归纳 偏置 假设 。

但是 ， 这 就 意味着 VIT 没有 翻盘 的 一 天 了 吗 ？ 当然 不 是 ， 不 要 忘 了 ， Transformer 架构 的 模
型 都 有 一个 广为人知 的 特性 ： 大力 出 奇迹 。 只要 它 见 过 的 数据 够 多 ， 它 就 能 更 好 地 学习 像
素块 之间 的 关联性 ， 当然 也 能 抹 去 归纳 偏置 的 问题 。

6 . 2 . 2 VIT ： 大力 出 奇迹

作者 当然 也 考虑 到 了 这 点 ， 所以 采用 了 不同 数量 的 数据集 ， 对 VIT 进行 训练 ， 效果 如下 ：

[ c 631948acb ]

如图 ， 横轴 表示 不 同 量级 的 数据 集 （ 越 往 右 数据 集 越 大 ） ， 纵轴 表示 准确率 。 图 中 灰色 阴
影部分 表示 在 相应 数据 集下 ， 不 同 架构 的 卷积神经 网络 的 准确率 范围 。 可以 发现 ， 当 数据
集 较 小时 ， VIT 表现 明显 弱 于 卷积 网络 。 但 当 数据量 级 大于 21 k 时 ， VIT 的 能力 就 上 来 了 。

6 . 3 VIT 的 Attention 到底 看到 了 什 么

讲 完 了 VIT 的 整体 效果 ， 我们 来 探究 下VIT 具体 学 到 了 什 么 ， 才 能 帮助 它 达到 这样 的 效果 。
我们 首先 来 看 attention层 。

[ 475e90e0 a 6 ]

这 张 实验图 刻画 了 VIT 的 16 个 multi - head attention 学 到 的 像素 距离 信息 。 横轴 表示 网络 的
深度 ， 纵轴 表示 “ 平均 注意力 距离 ” ， 我们 设 第i 个 和 第j 个 像素 的 平均 注意力 距离 为 dijd _
{ ij } dij​ ， 真实 像 素距离 为 dij ′d _{ij }^{\prime } dij ′​ ， 这 两 个 像素 所在patch 某 一个 head
上 的 attention score为aija _{ij } aij​ ， 则 有 ： dij = aij∗dij ′d _{ij } = a _{ij } * d _{ij }^
{\prime } dij​ = aij ​∗dij ′​ 。 当 dijd _{ij } dij ​ 越 大 时 ， 说明 VIT 的 attention 机制 能 让 它 关
注 到 距离 较 远 的 两 个 像素 ， 类似 于 CNN 中 的 “ 扩大 感受 野 ” 。

图 中 每 一 列 上 ， 都 有 16 个 彩色 原点 ， 它们 分别 表示 16 个 head 观测 到 的 平均 像 素距离 。 由 图
可知 ， 在 浅层 网络 中 ， VIT 还 只能 关注 到 距离 较 近 的 像 素点 ， 随着 网络 加深 ， VIT 逐渐 学会
去 更 远 的 像 素点 中 寻找 相关 信息 了 。 这个 过程 就 和 用 在 CNN 中 用 卷 积逐层 去 扩大 感受 野 非常
相似 。

下 图 的 左侧 表示 原始 的 输入 图片 ， 右侧 表示 VIT 最后 一 层 看到 的 图片 信息 ， 可以 清楚 看见 ，
VIT 在 最后 一 层 已经 学 到 了 将 注意力 放 到 关键 的 物体 上 了 ， 这 是 非常 有趣 的 结论 ：

[ 0c 57036 c 35 ]

6 . 4 VIT 的 位置 编码 学 到 了 什 么

我们 在 上文 讨论 过 图像 的 空间 局部性 （ locality ） ， 即 有 相关性 的 物体 （ 例如 太阳 和 天空
） 经常 一起 出现 。 CNN 采用 卷积 框取 特征 的 方式 ， 极大 程度 上 维护 了 这种 特性 。 其实 ， VIT
也 有 维护 这种 特性 的 方法 ， 上面 所 说 的 attention 是 一 种 ， 位置 编码 也 是 一 种 。

我们 来 看看 VIT 的 位置 编码 学 到 了 什 么 信息 ：

[ 2dce 17 b 395 ]

上图 是 VIT - L / 32 模型 下 的 位置 编码 信息 ， 图 中 每 一个 方框 表示 一个 patch ， 图 中共 有 7 * 7 个
patch 。 而 每个 方框 内 ， 也 有 一个 7 * 7 的 矩阵 ， 这个 矩阵 中 的 每 一个 值 ， 表示 当前 patch 的
position embedding 和 其余 对应 位置 的 position embedding 的 余弦 相似 度 。 颜色 越 黄 ， 表
示越 相似 ， 也 即 patch 和 对应 位置 间 的 patch 密切 相关 。

注意 到 每个 方框 中 ， 最 黄 的 点 总是 当前 patch 所在 位置 ， 这个 不 难 理解 ， 因为 自己 和 自己 肯
定 是 最 相似 的 。 除此以外 颜色 较 黄 的 部分 都 是 当前 patch 所属 的 行和列 ， 以及 以 当前 patch
为 中心 往 外 扩散 的 一 小 圈 。 这 就 说明 VIT 通过 位置 编码 ， 已经 学 到 了 一定 的 空间 局部性 。

七 、 总结 ： VIT 的 意义 何在

到 此 为止 ， 关于 VIT 模型 ， 我们 就 介绍 完毕 了 。 一 顿 读 下 来 ， 你 可能 有 个 印象 ： 如果 训练 数
据量 不 够 多 的 话 ， 看起 来 VIT 也 没 比 CNN 好 多少 呀 ， VIT 的 意义 是 什 么 呢 ？

这 是 个 很 好 的 问题 ， 因为 在 工业界 ， 人们 的 标注 数据量 和 算力 都 是 有限 的 ， 因此 CNN 可能 还
是 首要 选择 。

但是 ， VIT 的 出现 ， 不 仅 是 用 模型 效果 来 考量 这 么 简单 ， 今天 再 来 看 这个 模型 ， 发现 它 的 作
用 ， 就 像 是 给 后续 的 植树 人 挖 好 了 坑 ， 等 人 在 上面 播种 耕植 ， 具体 表现 在 ：

• 证明 了 一个 统一 框架 在 不 同 模态 任务 上 的 表现 能力 。 在 VIT 之前 ， NLP 的 SOTA范式 被 认
为 是 Transformer ， 而 图像 的 SOTA范式 依然 是 CNN 。 VIT 出现 后 ， 证明 了 用 NLP 领域 的
SOTA 模型 一样 能 解 图像 领域 的 问题 ， 同时 在 论文 中 通过 丰富 的 实验 ， 证明 了 VIT 对 CNN
的 替代 能力 ， 同时 也 论证 了 大规模 + 大 模型 在 图像 领域 的 涌现 能力 （ 论文 中 没有 明确 指
出 这 是 涌现 能力 ， 但 通过 实验 展现 了 这种 趋势 ） 。 这 也 为 后续 两 年 多 模态 任务 的 发展
奠定 了 基石 。

• 虽然 VIT 只 是 一个 分类 任务 ， 但 在 它 提出 的 几 个 月 之后 ， 立刻 就 有 了 用 Transformer架
构做 检测 （ detection ） 和 分割 （ segment ation ） 的 模型 。 而 不 久 之后 ， GPT式 的 无监
督 学习 ， 也 在 CV届 开始 火热 起 来 。

• 工业界 上 ， 对 大部分 企业 来 说 ， 受到 训练 数据 和 算力 的 影响 ， 预训练 和 微调 一个 VIT 都
是 困难 的 ， 但是 这 不 妨碍 直接 拿 大 厂训 好 的 VIT 特征 做 下游 任务 。 同时 ， 低 成本 的 微调
方案 研究 ， 在 今天 也 层出不穷 。 长远 来 看 ， 2 年 前 的 这个 “ 庞然大物 ” ， 已经 在 逐步 走 进
千家万户 。

八 、 参考

1 、 arxiv . org / pdf / 2010 . 11 …

2 、 w w w . bilibil i . com / video / BV15 P …

3 、 arxiv . org / pdf / 1803 . 02 …

4 、 blog . csd n . net / qq _ 44166630 …


avatar
猛猿 创作 等级L V . 3
🏆掘金 签约 作者 ｜ 人工智能 方向

14
文章

22 k
阅读

89
粉丝
目录
收起

• 一 、 模型 架构
□ 1 . 1 Bert 架构
□ 1 . 2 VIT 模型 架构
• 二 、 从 patch 到 token
□ 2 . 1 patch变token 的 过程
□ 2 . 2 为什 么 要 处理 成 patch
• 三 、 Emebdding
□ 3 . 1 Token Emebdding
□ 3 . 2 Position Embedding （ 位置 向量 ）
☆ 方案 一 ： 不 添加 任何 位置 信息
☆ 方案 二 ： 使用 1 - D 绝对 位置 编码
☆ 方案 三 ： 使用 2 - D 绝对 位置 编码
☆ 方案 四 ： 相对 位置 编码 （ relative positional embeddings ）
☆ 实验 结果
• 四 、 模型 架构 的 数学 表达
• 五 、 微调 （ fine - t une ）
□ 5 . 1 VIT fine - t une : 2 D插值 位置 编码
• 六 、 VIT 效果
□ 6 . 1 不 同VIT 模型 的 表示 符号
□ 6 . 2 VIT VS 卷 积神经 网络
☆ 6 . 2 . 1 卷 积神经 网络 的 归纳 偏置
☆ 6 . 2 . 2 VIT ： 大力 出 奇迹
□ 6 . 3 VIT 的 Attention 到底 看到 了 什 么
□ 6 . 4 VIT 的 位置 编码 学 到 了 什 么
• 七 、 总结 ： VIT 的 意义 何在
• 八 、 参考

相关 推荐

CV大 模型 系列 之 ： MAE ， 实现 像 素级 图像 重建
828 阅读
  ·  
11点 赞

图解 Transformer 系列 一 ： Positional Encoding （ 位置 编码 ）
968 阅读
  ·  
6 点 赞

图解 Transformer 系列 二 ： Self - Attention （ 自 注意力 机制 ）
918 阅读
  ·  
3 点 赞

Diffusion Model 原理 详解 及 源码 解析
2 . 1k 阅读
  ·  
15 点 赞

CV大 模型 系列 之 ： 打败VIT ？ Swin Transformer 是 怎 么 做到 的
996 阅读
  ·  
3 点 赞

友情 链接 ：

• js js p session

